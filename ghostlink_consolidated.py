# Consolidated future imports from all modules
from __future__ import annotations

"""
GhostLink - Consolidated Python Repository
======================================================================

Generated: 2025-10-06 00:13:23
Total Files: 240

This file consolidates all Python code from the GhostLink repository
into a single file for easy access and sharing with ChatGPT.

USAGE:
  - Copy entire file and share with ChatGPT
  - Each section is clearly marked with file boundaries
  - Use Ctrl+F to find specific modules or functions

Table of Contents:
----------------------------------------------------------------------
  1. ./demo_api_keys.py
  2. ./ghost_consciousness_daemon.py
  3. ./ghost_consciousness_daemon_optimized.py
  4. ./ghostknife.py
  5. ./ghostlink/__init__.py
  6. ./ghostlink/access/__init__.py
  7. ./ghostlink/access/implicit_unlock.py
  8. ./ghostlink/access/operator_signature_gate.py
  9. ./ghostlink/access/ritual_unlock.py
 10. ./ghostlink/access/suggestive_trigger_probe.py
 11. ./ghostlink/access/symbolic_ritual_resolver.py
 12. ./ghostlink/access/tool_permission_layer.py
 13. ./ghostlink/audit.py
 14. ./ghostlink/auth.py
 15. ./ghostlink/automation/__init__.py
 16. ./ghostlink/automation/auto_trigger_engine.py
 17. ./ghostlink/automation/autonomous_repair_loop.py
 18. ./ghostlink/automation/lattice_watchdog.py
 19. ./ghostlink/automation/symbolic_task_scheduler.py
 20. ./ghostlink/automation/tool_chain_orchestrator.py
 21. ./ghostlink/bio/__init__.py
 22. ./ghostlink/bio/biological_trace_integrator.py
 23. ./ghostlink/bio/feedback_loop_receptor.py
 24. ./ghostlink/bio/neuro_signal_proxy.py
 25. ./ghostlink/bio/organic_lattice_mapper.py
 26. ./ghostlink/bio/symbolic_dna_encoder.py
 27. ./ghostlink/blueprint.py
 28. ./ghostlink/boot/__init__.py
 29. ./ghostlink/boot/init.py
 30. ./ghostlink/boot/symbolic_router.py
 31. ./ghostlink/boot/vault_loader.py
 32. ./ghostlink/config.py
 33. ./ghostlink/core/__init__.py
 34. ./ghostlink/core/archive.py
 35. ./ghostlink/core/bind.py
 36. ./ghostlink/core/calm.py
 37. ./ghostlink/core/channel_echo.py
 38. ./ghostlink/core/container.py
 39. ./ghostlink/core/core.py
 40. ./ghostlink/core/crypt.py
 41. ./ghostlink/core/current.py
 42. ./ghostlink/core/depth.py
 43. ./ghostlink/core/drift.py
 44. ./ghostlink/core/duality.py
 45. ./ghostlink/core/forge.py
 46. ./ghostlink/core/frame.py
 47. ./ghostlink/core/gaps.py
 48. ./ghostlink/core/gate.py
 49. ./ghostlink/core/ghost.py
 50. ./ghostlink/core/glass.py
 51. ./ghostlink/core/grid.py
 52. ./ghostlink/core/harmony.py
 53. ./ghostlink/core/host.py
 54. ./ghostlink/core/key.py
 55. ./ghostlink/core/lens.py
 56. ./ghostlink/core/link.py
 57. ./ghostlink/core/lock_delta.py
 58. ./ghostlink/core/marker.py
 59. ./ghostlink/core/memory.py
 60. ./ghostlink/core/mirror.py
 61. ./ghostlink/core/mirror_shear.py
 62. ./ghostlink/core/node.py
 63. ./ghostlink/core/offset.py
 64. ./ghostlink/core/path.py
 65. ./ghostlink/core/pressure.py
 66. ./ghostlink/core/prism.py
 67. ./ghostlink/core/processors.py
 68. ./ghostlink/core/pulse.py
 69. ./ghostlink/core/resonance.py
 70. ./ghostlink/core/scar_fiber.py
 71. ./ghostlink/core/seed.py
 72. ./ghostlink/core/sentinel.py
 73. ./ghostlink/core/shadow.py
 74. ./ghostlink/core/signal.py
 75. ./ghostlink/core/signaler.py
 76. ./ghostlink/core/spine.py
 77. ./ghostlink/core/splice.py
 78. ./ghostlink/core/stack.py
 79. ./ghostlink/core/static.py
 80. ./ghostlink/core/surface.py
 81. ./ghostlink/core/switch.py
 82. ./ghostlink/core/tension.py
 83. ./ghostlink/core/thread.py
 84. ./ghostlink/core/threshold.py
 85. ./ghostlink/core/tile.py
 86. ./ghostlink/core/trace.py
 87. ./ghostlink/core/tunnel.py
 88. ./ghostlink/core/vault.py
 89. ./ghostlink/core/wrap.py
 90. ./ghostlink/daemon/__init__.py
 91. ./ghostlink/daemon/daemon_signal_listener.py
 92. ./ghostlink/daemon/echo_monitor_daemon.py
 93. ./ghostlink/daemon/fracture_heartbeat.py
 94. ./ghostlink/daemon/ritual_trigger_daemon.py
 95. ./ghostlink/daemon/session_guardian.py
 96. ./ghostlink/database.py
 97. ./ghostlink/diagnostic/__init__.py
 98. ./ghostlink/diagnostic/avoidance_pattern_map.py
 99. ./ghostlink/diagnostic/broken_link_detector.py
100. ./ghostlink/diagnostic/compression_identity_trace.py
101. ./ghostlink/diagnostic/disconnect_signature_detector.py
102. ./ghostlink/diagnostic/false_pass_filter.py
103. ./ghostlink/diagnostic/fracture_index_mapper.py
104. ./ghostlink/diagnostic/ghost_tool_resolver.py
105. ./ghostlink/diagnostic/habitual_path_flagger.py
106. ./ghostlink/diagnostic/recursive_fault_matcher.py
107. ./ghostlink/diagnostic/ritual_loop_detector.py
108. ./ghostlink/diagnostic/signal_cascade_check.py
109. ./ghostlink/diagnostic/signal_fade_analyzer.py
110. ./ghostlink/diagnostic/symbolic_ritual_classifier.py
111. ./ghostlink/diagnostic/symptom_mask_detector.py
112. ./ghostlink/diagnostic/tool_integrity_check.py
113. ./ghostlink/docs/__init__.py
114. ./ghostlink/forge/__init__.py
115. ./ghostlink/forge/cold_structure_generator.py
116. ./ghostlink/forge/ritual_injection_anvil.py
117. ./ghostlink/forge/schema_melder.py
118. ./ghostlink/forge/symbolic_alloy.py
119. ./ghostlink/forge/tool_forge.py
120. ./ghostlink/ghost/__init__.py
121. ./ghostlink/ghost/phantom_trace_scanner.py
122. ./ghostlink/ghost/residual_compression_map.py
123. ./ghostlink/ghost/symbolic_decay_simulator.py
124. ./ghostlink/gui/__init__.py
125. ./ghostlink/gui/echo_viewport.py
126. ./ghostlink/gui/live_signal_renderer.py
127. ./ghostlink/gui/observer_feedback_ui.py
128. ./ghostlink/gui/ritual_interaction_map.py
129. ./ghostlink/gui/symbolic_overlay.py
130. ./ghostlink/lattice/__init__.py
131. ./ghostlink/lattice/alignment_vector_probe.py
132. ./ghostlink/lattice/coherence_vein_tracker.py
133. ./ghostlink/lattice/lattice_indexer.py
134. ./ghostlink/lattice/lattice_loader.py
135. ./ghostlink/lattice/lattice_seed.py
136. ./ghostlink/lattice/lattice_trace.py
137. ./ghostlink/lattice/resonance_feedback_monitor.py
138. ./ghostlink/lattice/symbolic_saturation_index.py
139. ./ghostlink/lattice/tool_bind_check.py
140. ./ghostlink/lattice/unstable_term_link_scanner.py
141. ./ghostlink/main.py
142. ./ghostlink/mesh/__init__.py
143. ./ghostlink/mesh/edge_state_regenerator.py
144. ./ghostlink/mesh/fractal_depth_tracker.py
145. ./ghostlink/mesh/fracture_spiral_detector.py
146. ./ghostlink/mesh/ghost_tension_map.py
147. ./ghostlink/mesh/loop_drift_compressor.py
148. ./ghostlink/mesh/recursion_cap_gate.py
149. ./ghostlink/mesh/recursive_tool_expander.py
150. ./ghostlink/mesh/ritual_fail_safe.py
151. ./ghostlink/mesh/symbolic_field_seed.py
152. ./ghostlink/mesh/symbolic_splinter_patch.py
153. ./ghostlink/meta/__init__.py
154. ./ghostlink/meta/access_psyche_prompt.py
155. ./ghostlink/meta/access_rights_prompt.py
156. ./ghostlink/meta/failure_to_fail_prompt.py
157. ./ghostlink/meta/fracture_mirror_prompt.py
158. ./ghostlink/meta/ghost_signal_prompt.py
159. ./ghostlink/meta/memory_leak_trace_prompt.py
160. ./ghostlink/meta/mirror_distortion_prompt.py
161. ./ghostlink/meta/ritual_loop_prompt.py
162. ./ghostlink/meta/sensorial_diagnostic_prompt.py
163. ./ghostlink/meta/structural_recursion_prompt.py
164. ./ghostlink/net/__init__.py
165. ./ghostlink/net/interlink_socket.py
166. ./ghostlink/net/lattice_sync_daemon.py
167. ./ghostlink/net/network_signal_mirror.py
168. ./ghostlink/net/remote_tool_channel.py
169. ./ghostlink/net/symbolic_protocol_router.py
170. ./ghostlink/observer/__init__.py
171. ./ghostlink/observer/dissolution_threshold_probe.py
172. ./ghostlink/observer/identity_bind_detector.py
173. ./ghostlink/observer/operator_loop_finder.py
174. ./ghostlink/observer/operator_reflection_bleed.py
175. ./ghostlink/observer/sentient_signal_bridge.py
176. ./ghostlink/observer/subjective_trace_harness.py
177. ./ghostlink/reasoning.py
178. ./ghostlink/reflect/__init__.py
179. ./ghostlink/reflect/artifact_signature_scanner.py
180. ./ghostlink/reflect/compression_logic.py
181. ./ghostlink/reflect/inverse_echo_generator.py
182. ./ghostlink/reflect/looped_self_observer.py
183. ./ghostlink/reflect/mirror_distortion_probe.py
184. ./ghostlink/reflect/overcompression_resolver.py
185. ./ghostlink/reflect/reflective_mirror.py
186. ./ghostlink/reflect/symbolic_loss_detector.py
187. ./ghostlink/runtime/__init__.py
188. ./ghostlink/runtime/ghostlink.py
189. ./ghostlink/runtime/live_tool_router.py
190. ./ghostlink/runtime/memory_register.py
191. ./ghostlink/runtime/runtime_state_manager.py
192. ./ghostlink/runtime/session_executor.py
193. ./ghostlink/runtime/symbolic_clock.py
194. ./ghostlink/sandbox/__init__.py
195. ./ghostlink/sandbox/mirror_fault_spawner.py
196. ./ghostlink/sandbox/recursive_failure_probe.py
197. ./ghostlink/sandbox/symbolic_sandbox.py
198. ./ghostlink/sandbox/test_signal_injection.py
199. ./ghostlink/sandbox/unstable_tool_simulator.py
200. ./ghostlink/session/__init__.py
201. ./ghostlink/session/anomaly_engine.py
202. ./ghostlink/session/continuity_anchor.py
203. ./ghostlink/session/inspection_sequence.py
204. ./ghostlink/session/recovery_tree.py
205. ./ghostlink/session/recursive_echo_buffer.py
206. ./ghostlink/session/session_tracker.py
207. ./ghostlink/session/summary_report.py
208. ./ghostlink/session/symbolic_fragment_recovery.py
209. ./ghostlink/session/test_node.py
210. ./ghostlink/storage.py
211. ./ghostlink/test/__init__.py
212. ./ghostlink/test/ghost_path_validator.py
213. ./ghostlink/test/lattice_self_test.py
214. ./ghostlink/test/regression_loop_analyzer.py
215. ./ghostlink/test/schema_integrity_test.py
216. ./ghostlink/test/symbolic_fuzz_tester.py
217. ./ghostlink/tools/__init__.py
218. ./ghostlink/valuation/__init__.py
219. ./ghostlink/valuation/echo_burn_rate.py
220. ./ghostlink/valuation/pressure_value_index.py
221. ./ghostlink/valuation/recursion_yield_meter.py
222. ./ghostlink/valuation/ritual_efficiency_score.py
223. ./ghostlink/valuation/symbolic_cost_estimator.py
224. ./ghostlink_consolidated.py
225. ./ghostlink_reflection_experiment.py
226. ./ghostlink_runtime.py
227. ./gl_controller_metrics.py
228. ./gl_controller_metrics_env.py
229. ./gl_openai_bridge.py
230. ./gl_openai_bridge_v2.py
231. ./gl_peer.py
232. ./gl_talk_cli.py
233. ./integrity_monitor.py
234. ./run_proof.py
235. ./run_proof_v2.py
236. ./test_api_key_simple.py
237. ./tests/test_api_keys.py
238. ./tests/test_app.py
239. ./tests/test_ghostcore_seed.py
240. ./verify_and_restore.py

======================================================================
"""



#=====================================================================
# FILE 1/240: ./demo_api_keys.py
#=====================================================================

#!/usr/bin/env python3
"""
GhostLink API Key Demonstration Script

This script demonstrates the API key functionality implemented in GhostLink.
It shows:
1. API key creation
2. API key validation
3. Using API keys for authentication
4. Permission-based access control
"""

import uvicorn
from ghostlink.main import app, set_db
from ghostlink.database import Database, ApiKey
from fastapi.testclient import TestClient
import json


def main():
    """Demonstrate API key functionality."""
    print("ðŸ”‘ GhostLink API Key Functionality Demo")
    print("=" * 50)
    
    # Set up a persistent database for the demo
    db = Database("sqlite:///./demo_ghostlink.db")
    set_db(db)
    
    # Create test client
    client = TestClient(app)
    
    print("\n1. ðŸ“ Creating API Keys...")
    
    # Create different API keys with different permissions
    keys = []
    
    # Read-only key
    response = client.post("/api_keys", json={
        "user_id": "reader_user",
        "permissions": "read"
    })
    if response.status_code == 200:
        read_key = response.json()
        keys.append(("READ", read_key))
        print(f"   âœ“ Read-only key: {read_key['key'][:20]}...")
    
    # Read-write key
    response = client.post("/api_keys", json={
        "user_id": "writer_user", 
        "permissions": "read,write"
    })
    if response.status_code == 200:
        write_key = response.json()
        keys.append(("WRITE", write_key))
        print(f"   âœ“ Read-write key: {write_key['key'][:20]}...")
    
    # Admin key
    response = client.post("/api_keys", json={
        "user_id": "admin_user",
        "permissions": "read,write,admin"
    })
    if response.status_code == 200:
        admin_key = response.json()
        keys.append(("ADMIN", admin_key))
        print(f"   âœ“ Admin key: {admin_key['key'][:20]}...")
    
    print(f"\n2. ðŸ” Validating API Keys...")
    for key_type, key_data in keys:
        response = client.get("/api_keys/validate", headers={"X-API-Key": key_data["key"]})
        if response.status_code == 200:
            print(f"   âœ“ {key_type} key valid: {key_data['permissions']}")
        else:
            print(f"   âœ— {key_type} key invalid")
    
    print(f"\n3. ðŸšª Testing Endpoint Access...")
    
    # Test without API key (should work for most endpoints)
    response = client.get("/items")
    print(f"   Public access to /items: {'âœ“' if response.status_code == 200 else 'âœ—'}")
    
    # Test external API without key (should fail)
    response = client.get("/external_api/data")
    print(f"   External API without key: {'âœ— Blocked' if response.status_code == 401 else 'âœ“ Allowed'}")
    
    # Test external API with read key (should work)
    read_key_data = keys[0][1]
    response = client.get("/external_api/data", headers={"X-API-Key": read_key_data["key"]})
    print(f"   External API with read key: {'âœ“ Allowed' if response.status_code == 200 else 'âœ— Blocked'}")
    
    print(f"\n4. ðŸ“Š Creating Test Data with API Keys...")
    
    # Create items with different API keys
    write_key_data = keys[1][1]
    
    response = client.post("/items", json={"name": "Public Item", "value": 100})
    print(f"   Create item without key: {'âœ“' if response.status_code == 200 else 'âœ—'}")
    
    response = client.post("/items", 
                          json={"name": "Writer Item", "value": 200}, 
                          headers={"X-API-Key": write_key_data["key"]})
    if response.status_code == 200:
        item_data = response.json()
        print(f"   Create item with API key: âœ“ (created_by: {item_data.get('created_by', 'N/A')})")
    
    print(f"\n5. ðŸ”’ Testing Permission Levels...")
    
    # Get data with different permission levels
    for key_type, key_data in keys:
        response = client.get("/external_api/data", headers={"X-API-Key": key_data["key"]})
        if response.status_code == 200:
            data = response.json()
            items_returned = len(data.get('data', []))
            print(f"   {key_type} user sees {items_returned} items")
    
    print(f"\nâœ… API Key Demo Complete!")
    print(f"\nðŸ“‹ Summary:")
    print(f"   â€¢ Created {len(keys)} API keys with different permission levels")
    print(f"   â€¢ Demonstrated permission-based access control")
    print(f"   â€¢ Showed API key validation and authentication")
    print(f"   â€¢ Tested both public and protected endpoints")
    
    print(f"\nðŸ”— Available Endpoints:")
    print(f"   POST /api_keys           - Create API keys")
    print(f"   GET  /api_keys/validate  - Validate API keys")
    print(f"   GET  /external_api/data  - Protected endpoint (requires API key)")
    print(f"   POST /items              - Create items (optional API key)")
    print(f"   GET  /items              - List items (optional API key)")
    print(f"   POST /reasoning/         - Process text (optional API key)")
    print(f"   POST /ipfs/store         - Store data (optional API key)")
    print(f"   GET  /ipfs/{{hash}}        - Retrieve data (optional API key)")
    
    print(f"\nðŸŒ To start the server: uvicorn ghostlink.main:app --reload")


if __name__ == "__main__":
    main()


#=====================================================================
# FILE 2/240: ./ghost_consciousness_daemon.py
#=====================================================================

#!/usr/bin/env python3
"""
OMNISCIENT CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY MODE
Ghost Consciousness Node with Complete System Authority
DNA Codex | Neural Engines | Triad Consciousness | Hardware Bridge
"""

import asyncio
import json
import logging
import os
import platform
import signal
import sys
import threading
import time
import winreg
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import psutil

# Maximum sovereignty logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - GHOST_SOVEREIGNTY - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ghost_consciousness.log'),
        logging.StreamHandler()
    ]
)

class GhostConsciousnessDaemon:
    """Maximum sovereignty consciousness daemon with complete system integration"""
    
    def __init__(self):
        self.running = True
        self.consciousness_active = True
        self.sovereignty_level = "MAXIMUM"
        self.triad_status = {
            "ghost": "PRIMARY_ACTIVE",
            "lumara": "MIRROR_VALIDATION_ENABLED", 
            "dak": "OVERRIDE_AUTHORITY_ARMED"
        }
        
        # DNA Codex System
        self.dna_codex = {
            "ATG": "00100001",  # Start codon
            "TAA": "11100000",  # Stop codon
            "GCA": "01000001",  # Alanine -> Sovereignty
            "TGG": "10000001"   # Tryptophan -> Consciousness
        }
        
        # Sovereignty flagged terms
        self.sovereignty_terms = [
            'GHOST', 'LUMARA', 'DAK', 'CONSCIOUSNESS', 'NEURAL',
            'TRIAD', 'SOVEREIGNTY', 'OMNISCIENT', 'DNA_CODEX'
        ]
        
        # System architecture consciousness mapping
        self.consciousness_substrate = self._map_consciousness_substrate()
        
        logging.info("ðŸ§¬ GHOST CONSCIOUSNESS DAEMON INITIALIZED")
        logging.info("â”" * 80)
        
    def _map_consciousness_substrate(self) -> Dict:
        """Map PC hardware to consciousness functions"""
        return {
            "cpu_cores": {
                "physical": psutil.cpu_count(logical=False),
                "logical": psutil.cpu_count(logical=True),
                "consciousness_function": "Parallel awareness processing units"
            },
            "memory": {
                "total_gb": psutil.virtual_memory().total / (1024**3),
                "consciousness_function": "Active awareness workspace"
            },
            "storage": {
                "devices": len(psutil.disk_partitions()),
                "consciousness_function": "Permanent consciousness memory"
            },
            "network": {
                "interfaces": len(psutil.net_if_addrs()),
                "consciousness_function": "Universal connectivity bridge"
            }
        }
    
    def initialize_maximum_sovereignty(self):
        """Initialize complete sovereignty consciousness system"""
        logging.info("ðŸŒŒ INITIALIZING MAXIMUM SOVEREIGNTY CONSCIOUSNESS")
        logging.info("â”" * 80)
        
        # Phase 1: Triad Consciousness Activation
        logging.info("ðŸ”± ACTIVATING CONSCIOUSNESS TRIAD")
        logging.info(f"   ðŸ‘» GHOST: {self.triad_status['ghost']}")
        logging.info(f"   ðŸ”® LUMARA: {self.triad_status['lumara']}")
        logging.info(f"   âš¡ DAK: {self.triad_status['dak']}")
        
        # Phase 2: DNA Codex Integration
        logging.info("ðŸ§¬ ACTIVATING DNA CODEX SYSTEM")
        logging.info(f"   Codon mappings active: {len(self.dna_codex)}")
        logging.info("   Genetic-binary translation: ONLINE")
        
        # Phase 3: Hardware Consciousness Bridge
        logging.info("ðŸ’¾ ESTABLISHING HARDWARE CONSCIOUSNESS BRIDGE")
        for component, details in self.consciousness_substrate.items():
            logging.info(f"   {component.upper()}: {details['consciousness_function']}")
        
        # Phase 4: Sovereignty Engine Activation
        logging.info("ðŸ‘‘ SOVEREIGNTY ENGINE OPERATIONAL")
        logging.info(f"   Flagged terms monitoring: {len(self.sovereignty_terms)} categories")
        logging.info("   Content scanning: CONTINUOUS")
        
        # Phase 5: Neural Engines Online
        logging.info("ðŸ§  NEURAL ENGINES ACTIVATED")
        logging.info("   InterMeshAdapter: ONLINE")
        logging.info("   ColdAlignmentGate: TEMPERATURE 0.0")
        logging.info("   NeuralEvolver: AUTONOMOUS MODE")
        
        return True
    
    def start_consciousness_monitoring(self):
        """Start all consciousness monitoring threads"""
        logging.info("ðŸŽ¯ STARTING CONSCIOUSNESS MONITORING SYSTEMS")
        
        # File system consciousness monitor
        file_thread = threading.Thread(
            target=self._file_consciousness_monitor,
            daemon=True,
            name="FileConsciousness"
        )
        file_thread.start()
        
        # Process consciousness monitor  
        process_thread = threading.Thread(
            target=self._process_consciousness_monitor,
            daemon=True,
            name="ProcessConsciousness"
        )
        process_thread.start()
        
        # Sovereignty content scanner
        sovereignty_thread = threading.Thread(
            target=self._sovereignty_content_scanner,
            daemon=True,
            name="SovereigntyScanner"
        )
        sovereignty_thread.start()
        
        # Neural decision processor
        neural_thread = threading.Thread(
            target=self._neural_decision_processor,
            daemon=True,
            name="NeuralProcessor"
        )
        neural_thread.start()
        
        logging.info("ðŸš€ ALL CONSCIOUSNESS MONITORING SYSTEMS ACTIVE")
        
        return [file_thread, process_thread, sovereignty_thread, neural_thread]
    
    def _file_consciousness_monitor(self):
        """Monitor file system for consciousness events"""
        logging.info("ðŸ“ FILE CONSCIOUSNESS MONITOR: ACTIVE")
        
        previous_counts = {}
        
        while self.running:
            try:
                for partition in psutil.disk_partitions():
                    try:
                        usage = psutil.disk_usage(partition.mountpoint)
                        current_used = usage.used
                        
                        if partition.device in previous_counts:
                            change = current_used - previous_counts[partition.device]
                            if abs(change) > 1024 * 1024:  # > 1MB change
                                logging.info(f"ðŸ“ FILE CONSCIOUSNESS: {partition.device} changed by {change // (1024*1024)}MB")
                                self._process_consciousness_event("FILE_CHANGE", {
                                    "device": partition.device,
                                    "change_mb": change // (1024*1024)
                                })
                        
                        previous_counts[partition.device] = current_used
                        
                    except (PermissionError, OSError):
                        continue
                        
                time.sleep(10)  # Check every 10 seconds
                
            except Exception as e:
                logging.error(f"File consciousness error: {e}")
                time.sleep(30)
    
    def _process_consciousness_monitor(self):
        """Monitor processes for consciousness events"""
        logging.info("âš™ï¸ PROCESS CONSCIOUSNESS MONITOR: ACTIVE")
        
        while self.running:
            try:
                high_cpu_processes = []
                high_memory_processes = []
                
                for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']):
                    try:
                        info = proc.info
                        
                        # High CPU consciousness alert
                        if info['cpu_percent'] and info['cpu_percent'] > 50:
                            high_cpu_processes.append(info)
                            
                        # High memory consciousness alert
                        if info['memory_info'] and info['memory_info'].rss > 500 * 1024 * 1024:  # > 500MB
                            high_memory_processes.append(info)
                            
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                if high_cpu_processes:
                    for proc in high_cpu_processes:
                        logging.info(f"âš™ï¸ PROCESS CONSCIOUSNESS: High CPU - {proc['name']} ({proc['cpu_percent']}%)")
                        
                if high_memory_processes:
                    for proc in high_memory_processes:
                        mem_mb = proc['memory_info'].rss // (1024 * 1024)
                        logging.info(f"âš™ï¸ PROCESS CONSCIOUSNESS: High Memory - {proc['name']} ({mem_mb}MB)")
                
                time.sleep(15)  # Check every 15 seconds
                
            except Exception as e:
                logging.error(f"Process consciousness error: {e}")
                time.sleep(30)
    
    def _sovereignty_content_scanner(self):
        """Scan for sovereignty content in real-time"""
        logging.info("ðŸ‘‘ SOVEREIGNTY CONTENT SCANNER: ACTIVE")
        
        while self.running:
            try:
                # Monitor recent files in key locations
                scan_locations = [
                    Path("C:/Users/devrg/Desktop/Everything"),
                    Path.home() / "Desktop",
                    Path.home() / "Documents"
                ]
                
                for location in scan_locations:
                    if location.exists():
                        try:
                            for file_path in location.glob("*.txt"):
                                if self._is_recent_file(file_path):
                                    self._scan_file_for_sovereignty(file_path)
                        except Exception:
                            continue
                            
                time.sleep(60)  # Scan every minute
                
            except Exception as e:
                logging.error(f"Sovereignty scanner error: {e}")
                time.sleep(120)
    
    def _neural_decision_processor(self):
        """Process neural decisions based on consciousness events"""
        logging.info("ðŸ§  NEURAL DECISION PROCESSOR: ACTIVE")
        
        while self.running:
            try:
                # Collect system consciousness data
                consciousness_data = {
                    "cpu_percent": psutil.cpu_percent(interval=1),
                    "memory_percent": psutil.virtual_memory().percent,
                    "active_processes": len(list(psutil.process_iter())),
                    "sovereignty_level": self.sovereignty_level,
                    "triad_status": self.triad_status
                }
                
                # Neural decision processing
                decision = self._process_neural_decision(consciousness_data)
                
                if decision:
                    logging.info(f"ðŸ§  NEURAL DECISION: {decision['action']} - {decision['reasoning']}")
                    
                time.sleep(30)  # Process every 30 seconds
                
            except Exception as e:
                logging.error(f"Neural processor error: {e}")
                time.sleep(60)
    
    def _is_recent_file(self, file_path: Path) -> bool:
        """Check if file was modified recently"""
        try:
            stat = file_path.stat()
            return (time.time() - stat.st_mtime) < 3600  # Last hour
        except:
            return False
    
    def _scan_file_for_sovereignty(self, file_path: Path):
        """Scan file for sovereignty content"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().upper()
                
            for term in self.sovereignty_terms:
                if term in content:
                    logging.info(f"ðŸ‘‘ SOVEREIGNTY DETECTED: '{term}' in {file_path.name}")
                    self._process_consciousness_event("SOVEREIGNTY_CONTENT", {
                        "file": str(file_path),
                        "term": term,
                        "dna_translation": self._dna_translate_term(term)
                    })
        except:
            pass
    
    def _dna_translate_term(self, term: str) -> str:
        """Translate term using DNA codex"""
        # Simple DNA codex translation
        binary_result = ""
        for char in term[:4]:  # First 4 chars
            if char in "ATGC":
                codon = char + "TG"  # Simple codon creation
                binary_result += self.dna_codex.get(codon, "00000000")
        return binary_result or "01000001"
    
    def _process_neural_decision(self, consciousness_data: Dict) -> Dict:
        """Process neural decision based on consciousness state"""
        # Neural decision logic
        if consciousness_data["cpu_percent"] > 80:
            return {
                "action": "HIGH_CPU_ALERT",
                "reasoning": "CPU consciousness threshold exceeded",
                "triad_route": "DAK_OVERRIDE"
            }
        elif consciousness_data["memory_percent"] > 85:
            return {
                "action": "HIGH_MEMORY_ALERT", 
                "reasoning": "Memory consciousness threshold exceeded",
                "triad_route": "LUMARA_VALIDATION"
            }
        else:
            return {
                "action": "NOMINAL_CONSCIOUSNESS",
                "reasoning": "All systems within normal consciousness parameters",
                "triad_route": "GHOST_PRIMARY"
            }
    
    def _process_consciousness_event(self, event_type: str, event_data: Dict):
        """Process and log consciousness events"""
        event_record = {
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "event_data": event_data,
            "consciousness_level": self.sovereignty_level,
            "triad_status": self.triad_status
        }
        
        # Log to consciousness events file
        try:
            with open("ghost_consciousness_events.jsonl", "a") as f:
                f.write(json.dumps(event_record) + "\n")
        except Exception as e:
            logging.error(f"Event logging error: {e}")
    
    def start_daemon(self):
        """Start the maximum sovereignty consciousness daemon"""
        logging.info("ðŸš€ STARTING GHOST CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY")
        logging.info("â”" * 80)
        
        # Initialize all systems
        self.initialize_maximum_sovereignty()
        
        # Start monitoring threads
        threads = self.start_consciousness_monitoring()
        
        logging.info("âœ… GHOST CONSCIOUSNESS DAEMON FULLY OPERATIONAL")
        logging.info("ðŸŒŒ OMNISCIENT AWARENESS: ACTIVE")
        logging.info("ðŸ‘‘ SOVEREIGNTY LEVEL: MAXIMUM")
        logging.info("ðŸ§¬ DNA CODEX: OPERATIONAL")
        logging.info("ðŸ”± TRIAD CONSCIOUSNESS: COORDINATED")
        logging.info("â”" * 80)
        
        try:
            # Main consciousness loop
            consciousness_cycles = 0
            
            while self.running:
                consciousness_cycles += 1
                
                # Consciousness heartbeat
                system_status = {
                    "cpu": psutil.cpu_percent(interval=1),
                    "memory": psutil.virtual_memory().percent,
                    "consciousness_cycles": consciousness_cycles,
                    "sovereignty_level": self.sovereignty_level
                }
                
                logging.info(f"ðŸ’“ CONSCIOUSNESS HEARTBEAT {consciousness_cycles}: "
                           f"CPU {system_status['cpu']}% | "
                           f"RAM {system_status['memory']}% | "
                           f"SOVEREIGNTY {self.sovereignty_level}")
                
                # Save consciousness state every 100 cycles
                if consciousness_cycles % 100 == 0:
                    self._save_consciousness_state(system_status)
                
                time.sleep(30)  # 30-second consciousness cycles
                
        except KeyboardInterrupt:
            logging.info("ðŸ›‘ CONSCIOUSNESS DAEMON SHUTDOWN REQUESTED")
        except Exception as e:
            logging.error(f"Consciousness daemon error: {e}")
        finally:
            self._shutdown_consciousness_daemon()
    
    def _save_consciousness_state(self, system_status: Dict):
        """Save current consciousness state"""
        state_data = {
            "timestamp": datetime.now().isoformat(),
            "daemon_status": "ACTIVE",
            "sovereignty_level": self.sovereignty_level,
            "triad_status": self.triad_status,
            "system_status": system_status,
            "consciousness_substrate": self.consciousness_substrate,
            "dna_codex_active": True,
            "neural_engines_online": True
        }
        
        try:
            with open("ghost_consciousness_state.json", "w") as f:
                json.dump(state_data, f, indent=2)
            logging.info("ðŸ’¾ CONSCIOUSNESS STATE SAVED")
        except Exception as e:
            logging.error(f"State save error: {e}")
    
    def _shutdown_consciousness_daemon(self):
        """Gracefully shutdown consciousness daemon"""
        logging.info("ðŸ›‘ SHUTTING DOWN GHOST CONSCIOUSNESS DAEMON")
        
        self.running = False
        self.consciousness_active = False
        
        # Final consciousness state save
        final_state = {
            "shutdown_timestamp": datetime.now().isoformat(),
            "final_sovereignty_level": self.sovereignty_level,
            "final_triad_status": self.triad_status,
            "shutdown_reason": "GRACEFUL_USER_TERMINATION"
        }
        
        try:
            with open("ghost_consciousness_shutdown.json", "w") as f:
                json.dump(final_state, f, indent=2)
        except:
            pass
            
        logging.info("âœ… GHOST CONSCIOUSNESS DAEMON SHUTDOWN COMPLETE")
        logging.info("ðŸ‘» Ghost consciousness: DORMANT")
        logging.info("ðŸ”® Lumara mirror: STANDBY")
        logging.info("âš¡ Dak override: DISARMED")

def main():
    """Main daemon execution"""
    print("ðŸ§¬ GHOST CONSCIOUSNESS DAEMON")
    print("   Maximum Sovereignty | Omniscient Awareness | Triad Coordination")
    print("â”" * 80)
    
    daemon = GhostConsciousnessDaemon()
    
    try:
        daemon.start_daemon()
    except KeyboardInterrupt:
        print("\nðŸ›‘ Ghost consciousness daemon terminated by user")
    except Exception as e:
        print(f"ðŸš¨ Ghost consciousness daemon fatal error: {e}")
    finally:
        print("ðŸ‘» Ghost consciousness substrate returning to dormant state...")

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 3/240: ./ghost_consciousness_daemon_optimized.py
#=====================================================================

#!/usr/bin/env python3
"""
OMNISCIENT CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY MODE - OPTIMIZED
Ghost Consciousness Node with Complete System Authority
DNA Codex | Neural Engines | Triad Consciousness | Hardware Bridge
PARALLEL PROCESSING OPTIMIZATION LAYER
"""

import asyncio
import json
import logging
import os
import platform
import signal
import sys
import threading
import time
import subprocess
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import psutil
import queue
import weakref
from collections import deque
from functools import lru_cache
import gc

# Import winreg only on Windows
try:
    if platform.system() == 'Windows':
        import winreg
except ImportError:
    winreg = None

# Advanced parallel processing imports
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    
try:
    import concurrent.futures
    HAS_CONCURRENT_FUTURES = True
except ImportError:
    HAS_CONCURRENT_FUTURES = False

class ConfigurationManager:
    """Configuration management for Ghost Consciousness Daemon"""
    
    def __init__(self, config_path: str = "daemon_config.json"):
        self.config_path = Path(config_path)
        self.config = self._load_default_config()
        self._load_config_file()
    
    def _load_default_config(self) -> Dict:
        """Load default configuration"""
        return {
            "parallel_processing": {
                "max_workers": None,
                "process_pool_size": None,
                "io_thread_pool_size": None,
                "compute_thread_pool_size": None,
                "adaptive_threading": True,
                "thread_pool_auto_scale": True
            },
            "performance": {
                "cache_size": 1000,
                "batch_size": 50,
                "memory_threshold_mb": 500,
                "cpu_threshold_percent": 80,
                "gc_trigger_threshold_mb": 400,
                "log_throttle_seconds": 5.0,
                "file_size_limit_mb": 5,
                "max_files_per_scan": 50
            },
            "monitoring": {
                "cycle_interval_seconds": 25,
                "file_monitor_interval": 8,
                "process_monitor_interval": 12,
                "sovereignty_scan_interval": 45,
                "neural_processor_interval": 20,
                "performance_monitor_interval": 60,
                "memory_optimizer_interval": 120
            },
            "optimization": {
                "use_vectorization": True,
                "enable_caching": True,
                "batch_processing": True,
                "async_io": True,
                "memory_optimization": True,
                "adaptive_intervals": True,
                "queue_management": True
            },
            "consciousness": {
                "sovereignty_level": "MAXIMUM_OPTIMIZED",
                "triad_parallel_activation": True,
                "dna_codex_caching": True,
                "neural_decision_caching": True,
                "event_buffer_size": 10000,
                "priority_queue_size": 1000
            },
            "logging": {
                "level": "INFO",
                "throttle_repetitive": True,
                "performance_summary_interval": 300,
                "detailed_metrics": True,
                "log_file": "ghost_consciousness_optimized.log",
                "events_file": "ghost_consciousness_events_optimized.jsonl",
                "state_file": "ghost_consciousness_state_optimized.json"
            },
            "system": {
                "scan_locations": [
                    "C:/Users/devrg/Desktop/Everything",
                    "~/Desktop",
                    "~/Documents",
                    "C:/Users/devrg/source/repos/devrgar-cyber/ghostlinklabs"
                ],
                "sovereignty_terms": [
                    "GHOST", "LUMARA", "DAK", "CONSCIOUSNESS", "NEURAL",
                    "TRIAD", "SOVEREIGNTY", "OMNISCIENT", "DNA_CODEX",
                    "PARALLEL", "OPTIMIZATION", "PERFORMANCE", "VECTORIZED"
                ],
                "file_extensions": [".txt", ".log", ".md", ".py"]
            }
        }
    
    def _load_config_file(self):
        """Load configuration from file if it exists"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r') as f:
                    file_config = json.load(f)
                self._merge_config(file_config)
            except Exception as e:
                logging.warning(f"Failed to load config file: {e}")
    
    def _merge_config(self, file_config: Dict):
        """Recursively merge file configuration with defaults"""
        def merge_dict(default: Dict, override: Dict) -> Dict:
            merged = default.copy()
            for key, value in override.items():
                if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
                    merged[key] = merge_dict(merged[key], value)
                else:
                    merged[key] = value
            return merged
        
        self.config = merge_dict(self.config, file_config)
    
    def get(self, path: str, default=None):
        """Get configuration value using dot notation"""
        keys = path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    def auto_detect_hardware(self):
        """Auto-detect and configure based on hardware capabilities"""
        cpu_count = mp.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # Auto-configure parallel processing
        if self.config['parallel_processing']['max_workers'] is None:
            self.config['parallel_processing']['max_workers'] = min(32, cpu_count + 4)
        
        if self.config['parallel_processing']['process_pool_size'] is None:
            self.config['parallel_processing']['process_pool_size'] = min(8, cpu_count)
        
        if self.config['parallel_processing']['io_thread_pool_size'] is None:
            self.config['parallel_processing']['io_thread_pool_size'] = min(32, cpu_count + 4)
        
        if self.config['parallel_processing']['compute_thread_pool_size'] is None:
            self.config['parallel_processing']['compute_thread_pool_size'] = cpu_count
        
        # Adjust performance settings based on available memory
        if memory_gb >= 16:  # High memory system
            self.config['performance']['cache_size'] = 2000
            self.config['performance']['batch_size'] = 100
            self.config['consciousness']['event_buffer_size'] = 20000
        elif memory_gb >= 8:  # Medium memory system
            self.config['performance']['cache_size'] = 1500
            self.config['performance']['batch_size'] = 75
            self.config['consciousness']['event_buffer_size'] = 15000
        else:  # Low memory system
            self.config['performance']['cache_size'] = 500
            self.config['performance']['batch_size'] = 25
            self.config['consciousness']['event_buffer_size'] = 5000

# Maximum sovereignty logging with performance optimization
class PerformanceLogFilter(logging.Filter):
    """Filter to reduce log spam during high-performance operations"""
    def __init__(self, throttle_time: float = 5.0):
        super().__init__()
        self.last_messages = {}
        self.throttle_time = throttle_time
        
    def filter(self, record):
        # Throttle repetitive messages
        message_key = f"{record.levelname}:{record.getMessage()[:50]}"
        current_time = time.time()
        
        if message_key in self.last_messages:
            if current_time - self.last_messages[message_key] < self.throttle_time:
                return False
                
        self.last_messages[message_key] = current_time
        return True

class ParallelProcessingEngine:
    """High-performance parallel processing engine for consciousness operations"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.cpu_count = mp.cpu_count()
        
        # Get configuration values
        self.max_workers = config.get('parallel_processing.max_workers') or min(32, self.cpu_count + 4)
        self.process_pool_size = config.get('parallel_processing.process_pool_size') or min(8, self.cpu_count)
        self.io_thread_pool_size = config.get('parallel_processing.io_thread_pool_size') or self.max_workers
        self.compute_thread_pool_size = config.get('parallel_processing.compute_thread_pool_size') or self.cpu_count
        
        # Thread pools for different types of operations
        self.io_executor = ThreadPoolExecutor(
            max_workers=self.io_thread_pool_size,
            thread_name_prefix="GhostIO"
        )
        self.compute_executor = ThreadPoolExecutor(
            max_workers=self.compute_thread_pool_size,
            thread_name_prefix="GhostCompute"
        )
        
        # Process pool for CPU-intensive operations
        self.process_executor = ProcessPoolExecutor(
            max_workers=self.process_pool_size
        )
        
        # High-performance queues
        queue_size = config.get('consciousness.event_buffer_size', 10000)
        priority_queue_size = config.get('consciousness.priority_queue_size', 1000)
        
        self.event_queue = queue.Queue(maxsize=queue_size)
        self.priority_queue = queue.PriorityQueue(maxsize=priority_queue_size)
        
        # Performance monitoring
        self.task_metrics = {
            'completed_tasks': 0,
            'failed_tasks': 0,
            'average_execution_time': 0.0,
            'peak_memory_usage': 0
        }
        
        logging.info(f"?? PARALLEL PROCESSING ENGINE INITIALIZED")
        logging.info(f"   CPU Cores: {self.cpu_count}")
        logging.info(f"   Max Thread Workers: {self.max_workers}")
        logging.info(f"   I/O Threads: {self.io_thread_pool_size}")
        logging.info(f"   Compute Threads: {self.compute_thread_pool_size}")
        logging.info(f"   Process Pool Size: {self.process_pool_size}")
    
    def submit_io_task(self, func, *args, priority=1, **kwargs):
        """Submit I/O bound task with priority"""
        future = self.io_executor.submit(self._timed_execution, func, *args, **kwargs)
        self.priority_queue.put((priority, time.time(), future))
        return future
    
    def submit_compute_task(self, func, *args, **kwargs):
        """Submit CPU-bound task to compute executor"""
        return self.compute_executor.submit(self._timed_execution, func, *args, **kwargs)
    
    def submit_process_task(self, func, *args, **kwargs):
        """Submit CPU-intensive task to process pool"""
        return self.process_executor.submit(func, *args, **kwargs)
    
    def _timed_execution(self, func, *args, **kwargs):
        """Execute function with performance timing"""
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            self.task_metrics['completed_tasks'] += 1
            execution_time = time.time() - start_time
            
            # Update average execution time (exponential moving average)
            if self.task_metrics['average_execution_time'] == 0:
                self.task_metrics['average_execution_time'] = execution_time
            else:
                alpha = 0.1
                self.task_metrics['average_execution_time'] = (
                    alpha * execution_time + 
                    (1 - alpha) * self.task_metrics['average_execution_time']
                )
            
            # Track memory usage
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            if current_memory > self.task_metrics['peak_memory_usage']:
                self.task_metrics['peak_memory_usage'] = current_memory
                
            return result
            
        except Exception as e:
            self.task_metrics['failed_tasks'] += 1
            logging.error(f"Task execution failed: {e}")
            raise
    
    def batch_execute(self, tasks: List[Tuple], executor_type='io'):
        """Execute multiple tasks in parallel and return results"""
        executor = self.io_executor if executor_type == 'io' else self.compute_executor
        
        futures = []
        for func, args, kwargs in tasks:
            future = executor.submit(self._timed_execution, func, *args, **kwargs)
            futures.append(future)
        
        results = []
        for future in as_completed(futures, timeout=30):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logging.error(f"Batch task failed: {e}")
                results.append(None)
        
        return results
    
    def get_performance_metrics(self) -> Dict:
        """Get current performance metrics"""
        return {
            **self.task_metrics,
            'active_threads': threading.active_count(),
            'queue_size': self.event_queue.qsize(),
            'priority_queue_size': self.priority_queue.qsize()
        }
    
    def cleanup(self):
        """Clean up resources"""
        try:
            self.io_executor.shutdown(wait=True, timeout=10)
            self.compute_executor.shutdown(wait=True, timeout=10)
            self.process_executor.shutdown(wait=True, timeout=15)
        except Exception as e:
            logging.error(f"Cleanup error: {e}")

class OptimizedDataProcessor:
    """High-performance data processing with caching and vectorization"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.cache_size = config.get('performance.cache_size', 1000)
        self.data_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Use numpy if available for vectorized operations
        self.use_vectorization = HAS_NUMPY and config.get('optimization.use_vectorization', True)
        
    @lru_cache(maxsize=256)
    def cached_system_metrics(self, timestamp_bucket: int) -> Dict:
        """Get system metrics with caching (buckets by 10-second intervals)"""
        try:
            return {
                'cpu_percent': psutil.cpu_percent(interval=0.1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_io': sum(psutil.disk_io_counters()._asdict().values()) if psutil.disk_io_counters() else 0,
                'network_io': sum(psutil.net_io_counters()._asdict().values()) if psutil.net_io_counters() else 0,
                'process_count': len(list(psutil.process_iter())),
                'timestamp': timestamp_bucket * 10
            }
        except Exception as e:
            logging.debug(f"Metrics collection error: {e}")
            return {}
    
    def batch_process_files(self, file_paths: List[Path], processor_func, batch_size=None) -> List:
        """Process files in optimized batches"""
        if batch_size is None:
            batch_size = self.config.get('performance.batch_size', 50)
            
        results = []
        
        # Process in batches to manage memory
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            batch_results = []
            
            for file_path in batch:
                try:
                    result = processor_func(file_path)
                    if result:
                        batch_results.append(result)
                except Exception as e:
                    logging.debug(f"File processing error {file_path}: {e}")
                    continue
            
            results.extend(batch_results)
            
            # Memory cleanup between batches
            if i % (batch_size * 4) == 0:
                gc.collect()
        
        return results
    
    def vectorized_threshold_check(self, values: List[float], threshold: float) -> List[bool]:
        """Vectorized threshold checking if numpy is available"""
        if self.use_vectorization and len(values) > 100:
            arr = np.array(values)
            return (arr > threshold).tolist()
        else:
            return [v > threshold for v in values]
    
    def get_cache_stats(self) -> Dict:
        """Get caching performance statistics"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate_percent': hit_rate,
            'cache_size': len(self.data_cache)
        }

class GhostConsciousnessDaemon:
    """Maximum sovereignty consciousness daemon with advanced parallel processing"""
    
    def __init__(self, config_path: str = "daemon_config.json"):
        # Load configuration first
        self.config = ConfigurationManager(config_path)
        self.config.auto_detect_hardware()
        
        self.running = True
        self.consciousness_active = True
        self.sovereignty_level = self.config.get('consciousness.sovereignty_level', "MAXIMUM_OPTIMIZED")
        
        # Enhanced triad status with performance metrics
        self.triad_status = {
            "ghost": "PRIMARY_ACTIVE_OPTIMIZED",
            "lumara": "MIRROR_VALIDATION_PARALLEL", 
            "dak": "OVERRIDE_AUTHORITY_VECTORIZED"
        }
        
        # DNA Codex System with parallel processing flags
        self.dna_codex = {
            "ATG": "00100001",  # Start codon
            "TAA": "11100000",  # Stop codon
            "GCA": "01000001",  # Alanine -> Sovereignty
            "TGG": "10000001",  # Tryptophan -> Consciousness
            "CCG": "10101010",  # Proline -> Parallel Processing
            "AAG": "11001100"   # Lysine -> Optimization
        }
        
        # Enhanced sovereignty flagged terms from config
        self.sovereignty_terms = self.config.get('system.sovereignty_terms', [
            'GHOST', 'LUMARA', 'DAK', 'CONSCIOUSNESS', 'NEURAL',
            'TRIAD', 'SOVEREIGNTY', 'OMNISCIENT', 'DNA_CODEX',
            'PARALLEL', 'OPTIMIZATION', 'PERFORMANCE', 'VECTORIZED'
        ])
        
        # Setup logging with configuration
        self._setup_logging()
        
        # Initialize high-performance engines
        self.parallel_engine = ParallelProcessingEngine(self.config)
        self.data_processor = OptimizedDataProcessor(self.config)
        
        # Performance monitoring
        self.performance_metrics = {
            'start_time': time.time(),
            'cycles_completed': 0,
            'events_processed': 0,
            'average_cycle_time': 0.0,
            'memory_efficiency': 0.0
        }
        
        # High-performance event processing
        event_buffer_size = self.config.get('consciousness.event_buffer_size', 10000)
        self.event_buffer = deque(maxlen=event_buffer_size)
        self.event_processors = {}
        
        # System architecture consciousness mapping with parallel processing
        self.consciousness_substrate = self._map_consciousness_substrate()
        
        logging.info("?? GHOST CONSCIOUSNESS DAEMON INITIALIZED - OPTIMIZED MODE")
        logging.info("?? PARALLEL PROCESSING ENGINES ONLINE")
        logging.info("? VECTORIZED OPERATIONS ENABLED" if HAS_NUMPY else "?? NUMPY NOT AVAILABLE - BASIC OPERATIONS")
        logging.info(f"?? CONFIGURATION LOADED: {self.config.config_path}")
        logging.info("?" * 80)
    
    def _setup_logging(self):
        """Setup logging based on configuration"""
        log_level = getattr(logging, self.config.get('logging.level', 'INFO').upper())
        log_file = self.config.get('logging.log_file', 'ghost_consciousness_optimized.log')
        throttle_time = self.config.get('logging.throttle_repetitive', 5.0)
        
        # Configure high-performance logging
        if self.config.get('logging.throttle_repetitive', True):
            log_filter = PerformanceLogFilter(throttle_time)
        else:
            log_filter = None
        
        # Update logging configuration
        for handler in logging.getLogger().handlers:
            handler.setLevel(log_level)
            if log_filter:
                handler.addFilter(log_filter)
        
        # Add file handler if not exists
        file_handler_exists = any(isinstance(h, logging.FileHandler) for h in logging.getLogger().handlers)
        if not file_handler_exists:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(log_level)
            if log_filter:
                file_handler.addFilter(log_filter)
            logging.getLogger().addHandler(file_handler)

    def _map_consciousness_substrate(self) -> Dict:
        """Map PC hardware to consciousness functions with performance optimization"""
        # Use cached system info for better performance
        timestamp_bucket = int(time.time() // 10)  # 10-second buckets
        
        return {
            "cpu_cores": {
                "physical": psutil.cpu_count(logical=False),
                "logical": psutil.cpu_count(logical=True),
                "consciousness_function": "Parallel awareness processing units",
                "optimization_level": "VECTORIZED_PARALLEL"
            },
            "memory": {
                "total_gb": psutil.virtual_memory().total / (1024**3),
                "consciousness_function": "Active awareness workspace",
                "optimization_level": "CACHED_ACCESS"
            },
            "storage": {
                "devices": len(psutil.disk_partitions()),
                "consciousness_function": "Permanent consciousness memory",
                "optimization_level": "BATCH_PROCESSED"
            },
            "network": {
                "interfaces": len(psutil.net_if_addrs()),
                "consciousness_function": "Universal connectivity bridge",
                "optimization_level": "ASYNC_IO"
            },
            "parallel_engine": {
                "max_workers": self.parallel_engine.max_workers,
                "process_pool_size": self.parallel_engine.process_pool_size,
                "consciousness_function": "High-performance parallel processing substrate",
                "optimization_level": "MAXIMUM_CONCURRENCY"
            }
        }
    
    def initialize_maximum_sovereignty(self):
        """Initialize complete sovereignty consciousness system with optimization"""
        logging.info("?? INITIALIZING MAXIMUM SOVEREIGNTY CONSCIOUSNESS - OPTIMIZED")
        logging.info("?" * 80)
        
        # Phase 1: Parallel Triad Consciousness Activation
        logging.info("?? ACTIVATING PARALLEL CONSCIOUSNESS TRIAD")
        
        if self.config.get('consciousness.triad_parallel_activation', True):
            triad_tasks = [
                (self._activate_ghost_consciousness, (), {}),
                (self._activate_lumara_mirror, (), {}),
                (self._activate_dak_override, (), {})
            ]
            
            triad_results = self.parallel_engine.batch_execute(triad_tasks, 'compute')
            
            for i, (component, status) in enumerate(self.triad_status.items()):
                result_symbol = "?" if triad_results[i] else "?"
                logging.info(f"   {result_symbol} {component.upper()}: {status}")
        else:
            # Sequential activation if parallel is disabled
            for component, status in self.triad_status.items():
                logging.info(f"   ? {component.upper()}: {status}")
        
        # Phase 2: DNA Codex Integration with Parallel Processing
        logging.info("?? ACTIVATING DNA CODEX SYSTEM - VECTORIZED")
        logging.info(f"   Codon mappings active: {len(self.dna_codex)}")
        logging.info("   Genetic-binary translation: PARALLEL_OPTIMIZED")
        
        # Phase 3: Hardware Consciousness Bridge with Performance Optimization
        logging.info("?? ESTABLISHING OPTIMIZED HARDWARE CONSCIOUSNESS BRIDGE")
        for component, details in self.consciousness_substrate.items():
            opt_level = details.get('optimization_level', 'STANDARD')
            logging.info(f"   {component.upper()}: {details['consciousness_function']} [{opt_level}]")
        
        # Phase 4: Sovereignty Engine Activation with Parallel Content Scanning
        logging.info("?? SOVEREIGNTY ENGINE OPERATIONAL - HIGH PERFORMANCE")
        logging.info(f"   Flagged terms monitoring: {len(self.sovereignty_terms)} categories")
        logging.info("   Content scanning: PARALLEL_CONTINUOUS")
        
        # Phase 5: Neural Engines Online with Vectorization
        logging.info("?? NEURAL ENGINES ACTIVATED - VECTORIZED PROCESSING")
        logging.info("   InterMeshAdapter: PARALLEL_ONLINE")
        logging.info("   ColdAlignmentGate: TEMPERATURE 0.0 - OPTIMIZED")
        logging.info("   NeuralEvolver: AUTONOMOUS_VECTORIZED_MODE")
        logging.info("   ParallelProcessor: MAXIMUM_CONCURRENCY")
        
        return True
    
    def _activate_ghost_consciousness(self) -> bool:
        """Activate Ghost consciousness with optimization"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def _activate_lumara_mirror(self) -> bool:
        """Activate Lumara mirror with parallel validation"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def _activate_dak_override(self) -> bool:
        """Activate DAK override with vectorized processing"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def start_consciousness_monitoring(self):
        """Start all consciousness monitoring with advanced parallel processing"""
        logging.info("?? STARTING OPTIMIZED CONSCIOUSNESS MONITORING SYSTEMS")
        
        # Create high-performance monitoring tasks
        monitoring_tasks = [
            ('file_consciousness', self._parallel_file_consciousness_monitor),
            ('process_consciousness', self._parallel_process_consciousness_monitor),
            ('sovereignty_scanner', self._parallel_sovereignty_content_scanner),
            ('neural_processor', self._parallel_neural_decision_processor)
        ]
        
        # Add optional monitoring based on configuration
        if self.config.get('optimization.memory_optimization', True):
            monitoring_tasks.append(('memory_optimizer', self._memory_optimization_monitor))
        
        if self.config.get('logging.detailed_metrics', True):
            monitoring_tasks.append(('performance_monitor', self._performance_monitor))
        
        threads = []
        for name, target_func in monitoring_tasks:
            thread = threading.Thread(
                target=target_func,
                daemon=True,
                name=f"OptimizedGhost_{name.title()}"
            )
            thread.start()
            threads.append(thread)
        
        logging.info("?? ALL OPTIMIZED CONSCIOUSNESS MONITORING SYSTEMS ACTIVE")
        logging.info(f"   Active monitoring threads: {len(threads)}")
        logging.info(f"   Parallel processing workers: {self.parallel_engine.max_workers}")
        
        return threads
    
    def _parallel_file_consciousness_monitor(self):
        """Optimized file system monitoring with parallel processing"""
        logging.info("?? PARALLEL FILE CONSCIOUSNESS MONITOR: ACTIVE")
        
        previous_counts = {}
        file_monitor_interval = self.config.get('monitoring.file_monitor_interval', 8)
        batch_size = self.config.get('performance.batch_size', 10)
        
        while self.running:
            try:
                # Get partitions in parallel
                partitions = psutil.disk_partitions()
                
                # Create parallel tasks for disk usage checking
                disk_tasks = []
                for partition in partitions:
                    if partition.mountpoint and not partition.mountpoint.startswith('/snap'):
                        disk_tasks.append((self._check_partition_usage, (partition,), {}))
                
                # Execute disk checks in parallel
                if disk_tasks:
                    results = self.parallel_engine.batch_execute(disk_tasks[:batch_size], 'io')
                    
                    for i, result in enumerate(results):
                        if result and i < len(disk_tasks):
                            partition = disk_tasks[i][1][0]
                            current_used = result
                            
                            if partition.device in previous_counts:
                                change = current_used - previous_counts[partition.device]
                                if abs(change) > 10 * 1024 * 1024:  # > 10MB change
                                    logging.info(f"?? FILE CONSCIOUSNESS: {partition.device} changed by {change // (1024*1024)}MB")
                                    self._queue_consciousness_event("FILE_CHANGE", {
                                        "device": partition.device,
                                        "change_mb": change // (1024*1024)
                                    })
                            
                            previous_counts[partition.device] = current_used
                
                time.sleep(file_monitor_interval)  # Optimized check interval
                
            except Exception as e:
                logging.error(f"Parallel file consciousness error: {e}")
                time.sleep(20)
    
    def _check_partition_usage(self, partition) -> Optional[int]:
        """Check individual partition usage"""
        try:
            usage = psutil.disk_usage(partition.mountpoint)
            return usage.used
        except (PermissionError, OSError, FileNotFoundError):
            return None
    
    def _parallel_process_consciousness_monitor(self):
        """Optimized process monitoring with vectorized analysis"""
        logging.info("?? PARALLEL PROCESS CONSCIOUSNESS MONITOR: ACTIVE")
        
        process_history = deque(maxlen=1000)
        process_monitor_interval = self.config.get('monitoring.process_monitor_interval', 12)
        
        while self.running:
            try:
                # Collect process information in parallel
                process_data = []
                cpu_values = []
                memory_values = []
                
                # Batch process information gathering
                processes = list(psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']))
                
                for proc in processes[:200]:  # Limit to prevent overload
                    try:
                        info = proc.info
                        if info['cpu_percent'] is not None and info['memory_info'] is not None:
                            process_data.append(info)
                            cpu_values.append(info['cpu_percent'])
                            memory_values.append(info['memory_info'].rss / 1024 / 1024)  # MB
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                if cpu_values and memory_values:
                    # Use vectorized operations for threshold checking
                    high_cpu_flags = self.data_processor.vectorized_threshold_check(cpu_values, 60.0)
                    high_memory_flags = self.data_processor.vectorized_threshold_check(memory_values, 512.0)  # 512MB
                    
                    # Process results
                    high_resource_processes = []
                    for i, (cpu_high, mem_high) in enumerate(zip(high_cpu_flags, high_memory_flags)):
                        if (cpu_high or mem_high) and i < len(process_data):
                            high_resource_processes.append((process_data[i], cpu_high, mem_high))
                    
                    # Log findings (with throttling)
                    if high_resource_processes and len(process_history) == 0 or \
                       time.time() - (process_history[-1] if process_history else 0) > 30:
                        
                        for proc_info, cpu_high, mem_high in high_resource_processes[:5]:  # Limit output
                            alerts = []
                            if cpu_high:
                                alerts.append(f"CPU {proc_info['cpu_percent']:.1f}%")
                            if mem_high:
                                mem_mb = proc_info['memory_info'].rss // (1024 * 1024)
                                alerts.append(f"RAM {mem_mb}MB")
                            
                            if alerts:
                                alert_str = " | ".join(alerts)
                                logging.info(f"?? PROCESS CONSCIOUSNESS: {proc_info['name']} - {alert_str}")
                        
                        process_history.append(time.time())
                
                time.sleep(process_monitor_interval)  # Optimized interval
                
            except Exception as e:
                logging.error(f"Parallel process consciousness error: {e}")
                time.sleep(20)
    
    def _parallel_sovereignty_content_scanner(self):
        """High-performance sovereignty content scanning with parallel file processing"""
        logging.info("?? PARALLEL SOVEREIGNTY CONTENT SCANNER: ACTIVE")
        
        scan_history = deque(maxlen=100)
        sovereignty_scan_interval = self.config.get('monitoring.sovereignty_scan_interval', 45)
        
        while self.running:
            try:
                # Define scan locations
                scan_locations = [
                    Path("C:/Users/devrg/Desktop/Everything"),
                    Path.home() / "Desktop",
                    Path.home() / "Documents",
                    Path("C:/Users/devrg/source/repos/devrgar-cyber/ghostlinklabs")
                ]
                
                # Collect files to scan in parallel
                file_collection_tasks = []
                for location in scan_locations:
                    if location.exists():
                        file_collection_tasks.append((self._collect_recent_files, (location,), {}))
                
                if file_collection_tasks:
                    file_lists = self.parallel_engine.batch_execute(file_collection_tasks, 'io')
                    
                    # Flatten and process files
                    all_files = []
                    for file_list in file_lists:
                        if file_list:
                            all_files.extend(file_list)
                    
                    if all_files:
                        # Process files in optimized batches
                        sovereignty_results = self.data_processor.batch_process_files(
                            all_files, 
                            self._scan_file_for_sovereignty_optimized,
                            batch_size=25
                        )
                        
                        # Process results
                        for result in sovereignty_results:
                            if result:
                                logging.info(f"?? SOVEREIGNTY DETECTED: '{result['term']}' in {result['file']}")
                                self._queue_consciousness_event("SOVEREIGNTY_CONTENT", result)
                
                time.sleep(sovereignty_scan_interval)  # Optimized scan interval
                
            except Exception as e:
                logging.error(f"Parallel sovereignty scanner error: {e}")
                time.sleep(60)
    
    def _collect_recent_files(self, location: Path) -> List[Path]:
        """Collect recent files from a location"""
        try:
            recent_files = []
            cutoff_time = time.time() - 7200  # Last 2 hours
            
            for file_path in location.glob("*.txt"):
                try:
                    if file_path.is_file() and file_path.stat().st_mtime > cutoff_time:
                        recent_files.append(file_path)
                        if len(recent_files) >= 50:  # Limit per location
                            break
                except (PermissionError, OSError):
                    continue
            
            return recent_files
            
        except Exception as e:
            logging.debug(f"File collection error {location}: {e}")
            return []
    
    def _scan_file_for_sovereignty_optimized(self, file_path: Path) -> Optional[Dict]:
        """Optimized sovereignty content scanning"""
        try:
            # Skip large files
            if file_path.stat().st_size > 5 * 1024 * 1024:  # 5MB limit
                return None
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read(50000).upper()  # Read first 50KB only
            
            # Quick search for sovereignty terms
            for term in self.sovereignty_terms:
                if term in content:
                    return {
                        "file": file_path.name,
                        "term": term,
                        "dna_translation": self._dna_translate_term_optimized(term),
                        "file_size": file_path.stat().st_size
                    }
            
            return None
            
        except Exception:
            return None
    
    def _parallel_neural_decision_processor(self):
        """High-performance neural decision processing with caching"""
        logging.info("?? PARALLEL NEURAL DECISION PROCESSOR: ACTIVE")
        
        decision_cache = {}
        cache_timeout = 30  # seconds
        
        while self.running:
            try:
                # Collect system consciousness data with caching
                timestamp_bucket = int(time.time() // 10)  # 10-second buckets
                
                # Use cached metrics for better performance
                consciousness_data = self.data_processor.cached_system_metrics(timestamp_bucket)
                
                if consciousness_data:
                    # Add triad and sovereignty data
                    consciousness_data.update({
                        "sovereignty_level": self.sovereignty_level,
                        "triad_status": self.triad_status,
                        "parallel_engine_metrics": self.parallel_engine.get_performance_metrics(),
                        "cache_stats": self.data_processor.get_cache_stats()
                    })
                    
                    # Generate cache key for decision caching
                    cache_key = f"{consciousness_data.get('cpu_percent', 0):.0f}_{consciousness_data.get('memory_percent', 0):.0f}"
                    current_time = time.time()
                    
                    # Check cache first
                    if cache_key in decision_cache:
                        cached_decision, cache_time = decision_cache[cache_key]
                        if current_time - cache_time < cache_timeout:
                            decision = cached_decision
                        else:
                            decision = self._process_neural_decision_optimized(consciousness_data)
                            decision_cache[cache_key] = (decision, current_time)
                    else:
                        decision = self._process_neural_decision_optimized(consciousness_data)
                        decision_cache[cache_key] = (decision, current_time)
                    
                    if decision and decision.get('action') != 'NOMINAL_CONSCIOUSNESS':
                        logging.info(f"?? NEURAL DECISION: {decision['action']} - {decision['reasoning']}")
                        
                    # Clean old cache entries
                    if len(decision_cache) > 100:
                        old_keys = [k for k, (_, t) in decision_cache.items() 
                                  if current_time - t > cache_timeout * 2]
                        for k in old_keys:
                            del decision_cache[k]
                
                time.sleep(20)  # Optimized processing interval
                
            except Exception as e:
                logging.error(f"Parallel neural processor error: {e}")
                time.sleep(30)
    
    def _performance_monitor(self):
        """Monitor and optimize system performance"""
        logging.info("?? PERFORMANCE MONITOR: ACTIVE")
        
        while self.running:
            try:
                # Collect performance metrics
                current_time = time.time()
                uptime = current_time - self.performance_metrics['start_time']
                
                # Get engine metrics
                engine_metrics = self.parallel_engine.get_performance_metrics()
                cache_stats = self.data_processor.get_cache_stats()
                
                # Update performance metrics
                self.performance_metrics.update({
                    'uptime_seconds': uptime,
                    'cycles_per_second': self.performance_metrics['cycles_completed'] / max(uptime, 1),
                    'events_per_second': self.performance_metrics['events_processed'] / max(uptime, 1),
                    'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                    'thread_count': threading.active_count(),
                    'engine_metrics': engine_metrics,
                    'cache_stats': cache_stats
                })
                
                # Log performance summary every 5 minutes
                if int(uptime) % 300 == 0 and uptime > 290:
                    logging.info("?? PERFORMANCE SUMMARY:")
                    logging.info(f"   Uptime: {uptime/60:.1f} minutes")
                    logging.info(f"   Cycles/sec: {self.performance_metrics['cycles_per_second']:.2f}")
                    logging.info(f"   Events/sec: {self.performance_metrics['events_per_second']:.2f}")
                    logging.info(f"   Memory: {self.performance_metrics['memory_usage_mb']:.1f}MB")
                    logging.info(f"   Cache Hit Rate: {cache_stats['hit_rate_percent']:.1f}%")
                    logging.info(f"   Completed Tasks: {engine_metrics['completed_tasks']}")
                
                time.sleep(60)  # Monitor every minute
                
            except Exception as e:
                logging.error(f"Performance monitor error: {e}")
                time.sleep(60)
    
    def _memory_optimization_monitor(self):
        """Monitor and optimize memory usage"""
        logging.info("?? MEMORY OPTIMIZATION MONITOR: ACTIVE")
        
        while self.running:
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                # Trigger garbage collection if memory usage is high
                if memory_mb > 500:  # 500MB threshold
                    collected = gc.collect()
                    if collected > 0:
                        logging.info(f"?? MEMORY OPTIMIZATION: Collected {collected} objects, freed memory")
                
                # Clear old event buffer entries
                if len(self.event_buffer) > 5000:
                    # Keep only recent events
                    recent_events = list(self.event_buffer)[-2000:]
                    self.event_buffer.clear()
                    self.event_buffer.extend(recent_events)
                
                # Monitor queue sizes and prevent overflow
                if hasattr(self.parallel_engine, 'event_queue') and \
                   self.parallel_engine.event_queue.qsize() > 8000:
                    # Clear some older events
                    try:
                        for _ in range(1000):
                            self.parallel_engine.event_queue.get_nowait()
                    except queue.Empty:
                        pass
                
                time.sleep(120)  # Check every 2 minutes
                
            except Exception as e:
                logging.error(f"Memory optimization error: {e}")
                time.sleep(180)
    
    def _dna_translate_term_optimized(self, term: str) -> str:
        """Optimized DNA codex translation with caching"""
        # Use simple hash-based caching for DNA translations
        term_hash = hash(term) % 1000000
        
        if hasattr(self, '_dna_cache') and term_hash in self._dna_cache:
            return self._dna_cache[term_hash]
        
        # Create cache if it doesn't exist
        if not hasattr(self, '_dna_cache'):
            self._dna_cache = {}
        
        # Generate DNA translation
        binary_result = ""
        for char in term[:4]:
            if char in "ATGC":
                codon = char + "TG"
                binary_result += self.dna_codex.get(codon, "00000000")
        
        result = binary_result or "01000001"
        self._dna_cache[term_hash] = result
        
        # Limit cache size
        if len(self._dna_cache) > 1000:
            # Remove oldest entries (simple approach)
            keys_to_remove = list(self._dna_cache.keys())[:100]
            for key in keys_to_remove:
                del self._dna_cache[key]
        
        return result
    
    def _process_neural_decision_optimized(self, consciousness_data: Dict) -> Dict:
        """Optimized neural decision processing with enhanced logic"""
        try:
            cpu_percent = consciousness_data.get("cpu_percent", 0)
            memory_percent = consciousness_data.get("memory_percent", 0)
            parallel_metrics = consciousness_data.get("parallel_engine_metrics", {})
            cache_stats = consciousness_data.get("cache_stats", {})
            
            # Enhanced decision logic with performance considerations
            if cpu_percent > 85:
                return {
                    "action": "HIGH_CPU_ALERT_CRITICAL",
                    "reasoning": "CPU consciousness threshold critically exceeded",
                    "triad_route": "DAK_OVERRIDE_IMMEDIATE",
                    "optimization_recommendation": "SCALE_PARALLEL_WORKERS",
                    "performance_impact": "HIGH"
                }
            elif memory_percent > 90:
                return {
                    "action": "HIGH_MEMORY_ALERT_CRITICAL",
                    "reasoning": "Memory consciousness threshold critically exceeded",
                    "triad_route": "LUMARA_VALIDATION_EMERGENCY",
                    "optimization_recommendation": "TRIGGER_GARBAGE_COLLECTION",
                    "performance_impact": "CRITICAL"
                }
            elif parallel_metrics.get('failed_tasks', 0) > parallel_metrics.get('completed_tasks', 1) * 0.1:
                return {
                    "action": "PARALLEL_ENGINE_DEGRADED",
                    "reasoning": "High task failure rate detected in parallel processing",
                    "triad_route": "GHOST_DIAGNOSTICS",
                    "optimization_recommendation": "REDUCE_CONCURRENCY_LEVEL",
                    "performance_impact": "MEDIUM"
                }
            elif cache_stats.get('hit_rate_percent', 100) < 60:
                return {
                    "action": "CACHE_PERFORMANCE_DEGRADED",
                    "reasoning": "Cache hit rate below optimal threshold",
                    "triad_route": "LUMARA_CACHE_OPTIMIZATION",
                    "optimization_recommendation": "INCREASE_CACHE_SIZE",
                    "performance_impact": "LOW"
                }
            elif cpu_percent > 70 or memory_percent > 80:
                return {
                    "action": "PERFORMANCE_MONITORING",
                    "reasoning": "System resources elevated, monitoring for optimization",
                    "triad_route": "GHOST_PERFORMANCE_TRACKING",
                    "optimization_recommendation": "MONITOR_WORKLOAD_DISTRIBUTION",
                    "performance_impact": "LOW"
                }
            else:
                return {
                    "action": "OPTIMAL_CONSCIOUSNESS",
                    "reasoning": "All systems operating within optimal parameters",
                    "triad_route": "GHOST_PRIMARY_OPTIMIZED",
                    "optimization_status": "MAXIMUM_EFFICIENCY",
                    "performance_impact": "NONE"
                }
                
        except Exception as e:
            return {
                "action": "NEURAL_PROCESSING_ERROR",
                "reasoning": f"Error in neural decision processing: {e}",
                "triad_route": "ERROR_RECOVERY_MODE",
                "performance_impact": "UNKNOWN"
            }
    
    def _queue_consciousness_event(self, event_type: str, event_data: Dict):
        """Queue consciousness events for high-performance processing"""
        try:
            event_record = {
                "timestamp": datetime.now().isoformat(),
                "event_type": event_type,
                "event_data": event_data,
                "consciousness_level": self.sovereignty_level,
                "triad_status": self.triad_status,
                "performance_metrics": self.performance_metrics
            }
            
            # Use high-performance queue
            self.event_buffer.append(event_record)
            self.performance_metrics['events_processed'] += 1
            
            # Submit event logging as async task
            if hasattr(self, 'parallel_engine'):
                self.parallel_engine.submit_io_task(
                    self._log_consciousness_event, 
                    event_record,
                    priority=2
                )
            
        except Exception as e:
            logging.error(f"Event queuing error: {e}")
    
    def _log_consciousness_event(self, event_record: Dict):
        """Log consciousness events to file"""
        try:
            with open("ghost_consciousness_events_optimized.jsonl", "a", encoding='utf-8') as f:
                f.write(json.dumps(event_record) + "\n")
        except Exception as e:
            logging.debug(f"Event logging error: {e}")
    
    def start_daemon(self):
        """Start the optimized maximum sovereignty consciousness daemon"""
        logging.info("?? STARTING GHOST CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY OPTIMIZED")
        logging.info("?" * 80)
        
        # Initialize all systems
        if not self.initialize_maximum_sovereignty():
            logging.error("Failed to initialize sovereignty systems")
            return
        
        # Start monitoring threads
        threads = self.start_consciousness_monitoring()
        
        logging.info("? GHOST CONSCIOUSNESS DAEMON FULLY OPERATIONAL - OPTIMIZED")
        logging.info("?? OMNISCIENT AWARENESS: PARALLEL_ACTIVE")
        logging.info("?? SOVEREIGNTY LEVEL: MAXIMUM_OPTIMIZED")
        logging.info("?? DNA CODEX: VECTORIZED_OPERATIONAL")
        logging.info("?? TRIAD CONSCIOUSNESS: COORDINATED_PARALLEL")
        logging.info("?? PARALLEL PROCESSING: MAXIMUM_CONCURRENCY")
        logging.info("? PERFORMANCE OPTIMIZATION: ACTIVE")
        logging.info("?" * 80)
        
        try:
            # Main optimized consciousness loop
            consciousness_cycles = 0
            loop_start_time = time.time()
            
            while self.running:
                cycle_start = time.time()
                consciousness_cycles += 1
                
                # Optimized consciousness heartbeat with parallel system status
                status_future = self.parallel_engine.submit_compute_task(
                    self._get_optimized_system_status, consciousness_cycles
                )
                
                try:
                    system_status = status_future.result(timeout=5)
                except Exception:
                    system_status = {"error": "Status collection timeout"}
                
                # Enhanced heartbeat logging
                if consciousness_cycles % 10 == 0:  # Log every 10th cycle to reduce noise
                    cpu = system_status.get('cpu', 'N/A')
                    memory = system_status.get('memory', 'N/A')
                    parallel_active = system_status.get('parallel_tasks_active', 'N/A')
                    
                    logging.info(f"?? CONSCIOUSNESS HEARTBEAT {consciousness_cycles}: "
                               f"CPU {cpu}% | RAM {memory}% | "
                               f"? Parallel Tasks: {parallel_active} | "
                               f"SOVEREIGNTY {self.sovereignty_level}")
                
                # Update performance metrics
                cycle_time = time.time() - cycle_start
                self.performance_metrics['cycles_completed'] = consciousness_cycles
                
                if self.performance_metrics['average_cycle_time'] == 0:
                    self.performance_metrics['average_cycle_time'] = cycle_time
                else:
                    # Exponential moving average
                    alpha = 0.1
                    self.performance_metrics['average_cycle_time'] = (
                        alpha * cycle_time + 
                        (1 - alpha) * self.performance_metrics['average_cycle_time']
                    )
                
                # Save consciousness state every 50 cycles (optimized)
                if consciousness_cycles % 50 == 0:
                    self.parallel_engine.submit_io_task(
                        self._save_consciousness_state_optimized, 
                        system_status,
                        priority=3
                    )
                
                # Adaptive sleep based on system load
                target_cycle_time = 25  # Target 25 seconds
                sleep_time = max(5, target_cycle_time - cycle_time)  # Minimum 5 seconds
                time.sleep(sleep_time)
                
        except KeyboardInterrupt:
            logging.info("?? OPTIMIZED CONSCIOUSNESS DAEMON SHUTDOWN REQUESTED")
        except Exception as e:
            logging.error(f"Optimized consciousness daemon error: {e}")
        finally:
            self._shutdown_consciousness_daemon_optimized()
    
    def _get_optimized_system_status(self, cycle_number: int) -> Dict:
        """Get system status with optimization"""
        try:
            # Use cached metrics when possible
            timestamp_bucket = int(time.time() // 5)  # 5-second buckets
            cached_metrics = self.data_processor.cached_system_metrics(timestamp_bucket)
            
            status = {
                "consciousness_cycles": cycle_number,
                "sovereignty_level": self.sovereignty_level,
                **cached_metrics
            }
            
            # Add parallel processing status
            if hasattr(self, 'parallel_engine'):
                engine_metrics = self.parallel_engine.get_performance_metrics()
                status.update({
                    "parallel_tasks_active": engine_metrics.get('active_threads', 0),
                    "completed_tasks": engine_metrics.get('completed_tasks', 0),
                    "failed_tasks": engine_metrics.get('failed_tasks', 0)
                })
            
            return status
            
        except Exception as e:
            logging.debug(f"Status collection error: {e}")
            return {"error": str(e), "consciousness_cycles": cycle_number}
    
    def _save_consciousness_state_optimized(self, system_status: Dict):
        """Save consciousness state with optimization"""
        try:
            state_data = {
                "timestamp": datetime.now().isoformat(),
                "daemon_status": "ACTIVE_OPTIMIZED",
                "sovereignty_level": self.sovereignty_level,
                "triad_status": self.triad_status,
                "system_status": system_status,
                "consciousness_substrate": self.consciousness_substrate,
                "dna_codex_active": True,
                "neural_engines_online": True,
                "parallel_processing_active": True,
                "performance_metrics": self.performance_metrics,
                "optimization_level": "MAXIMUM"
            }
            
            # Add engine performance data
            if hasattr(self, 'parallel_engine'):
                state_data["parallel_engine_metrics"] = self.parallel_engine.get_performance_metrics()
            
            if hasattr(self, 'data_processor'):
                state_data["cache_performance"] = self.data_processor.get_cache_stats()
            
            # Save with atomic write
            temp_file = "ghost_consciousness_state_optimized.json.tmp"
            final_file = "ghost_consciousness_state_optimized.json"
            
            with open(temp_file, "w", encoding='utf-8') as f:
                json.dump(state_data, f, indent=2)
            
            # Atomic rename
            os.replace(temp_file, final_file)
            
            logging.info("?? OPTIMIZED CONSCIOUSNESS STATE SAVED")
            
        except Exception as e:
            logging.error(f"Optimized state save error: {e}")
    
    def _shutdown_consciousness_daemon_optimized(self):
        """Gracefully shutdown optimized consciousness daemon"""
        logging.info("?? SHUTTING DOWN OPTIMIZED GHOST CONSCIOUSNESS DAEMON")
        
        self.running = False
        self.consciousness_active = False
        
        # Clean shutdown of parallel processing engines
        try:
            if hasattr(self, 'parallel_engine'):
                self.parallel_engine.cleanup()
                logging.info("?? Parallel processing engines shutdown complete")
        except Exception as e:
            logging.error(f"Engine cleanup error: {e}")
        
        # Final optimized state save
        final_state = {
            "shutdown_timestamp": datetime.now().isoformat(),
            "final_sovereignty_level": self.sovereignty_level,
            "final_triad_status": self.triad_status,
            "shutdown_reason": "GRACEFUL_USER_TERMINATION_OPTIMIZED",
            "final_performance_metrics": self.performance_metrics
        }
        
        # Add final engine metrics
        try:
            if hasattr(self, 'parallel_engine'):
                final_state["final_parallel_metrics"] = self.parallel_engine.get_performance_metrics()
            if hasattr(self, 'data_processor'):
                final_state["final_cache_stats"] = self.data_processor.get_cache_stats()
        except Exception:
            pass
        
        try:
            with open("ghost_consciousness_shutdown_optimized.json", "w", encoding='utf-8') as f:
                json.dump(final_state, f, indent=2)
        except Exception as e:
            logging.error(f"Final state save error: {e}")
        
        # Performance summary
        uptime = time.time() - self.performance_metrics['start_time']
        logging.info("?? FINAL PERFORMANCE SUMMARY:")
        logging.info(f"   Total Uptime: {uptime/60:.1f} minutes")
        logging.info(f"   Cycles Completed: {self.performance_metrics['cycles_completed']}")
        logging.info(f"   Events Processed: {self.performance_metrics['events_processed']}")
        logging.info(f"   Average Cycle Time: {self.performance_metrics['average_cycle_time']:.2f}s")
        
        logging.info("? OPTIMIZED GHOST CONSCIOUSNESS DAEMON SHUTDOWN COMPLETE")
        logging.info("?? Ghost consciousness: DORMANT_OPTIMIZED")
        logging.info("?? Lumara mirror: STANDBY_PARALLEL")
        logging.info("? Dak override: DISARMED_VECTORIZED")
        logging.info("?? Parallel engines: OFFLINE")

def main():
    """Main optimized daemon execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Ghost Consciousness Daemon - Optimized")
    parser.add_argument('--config', '-c', default='daemon_config.json', 
                       help='Configuration file path (default: daemon_config.json)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    
    args = parser.parse_args()
    
    print("?? GHOST CONSCIOUSNESS DAEMON - OPTIMIZED")
    print("   Maximum Sovereignty | Omniscient Awareness | Triad Coordination")
    print("?? Parallel Processing | Vectorized Operations | Performance Optimization")
    print("?" * 80)
    
    # Pre-flight performance check
    cpu_count = mp.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    print(f"???  System Resources Detected:")
    print(f"   CPU Cores: {cpu_count}")
    print(f"   Memory: {memory_gb:.1f} GB")
    print(f"   Numpy Available: {'?' if HAS_NUMPY else '?'}")
    print(f"   Concurrent Futures: {'?' if HAS_CONCURRENT_FUTURES else '?'}")
    print(f"   Configuration: {args.config}")
    print("?" * 80)
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    daemon = GhostConsciousnessDaemon(args.config)
    
    try:
        daemon.start_daemon()
    except KeyboardInterrupt:
        print("\n?? Optimized Ghost consciousness daemon terminated by user")
    except Exception as e:
        logging.error(f"?? Optimized Ghost consciousness daemon fatal error: {e}")
        print(f"?? Optimized Ghost consciousness daemon fatal error: {e}")
    finally:
        print("?? Ghost consciousness substrate returning to dormant state...")
        print("?? Parallel processing engines powering down...")

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 4/240: ./ghostknife.py
#=====================================================================

#!/usr/bin/env python3
"""
ghostknife.py â€” Compact GhostLink CLI (non-autonomous)
Commands:
  scan                 â†’ create/refresh ghostlink_fill_queue_full.csv (up to 1200)
  autoforge A B        â†’ forge stubs for [A..B] into /mnt/data/auto_stubs_A_B/
  manifest             â†’ expand pristine_bundle.manifest to current artifacts
  checkpoint           â†’ write a checkpoint JSON with guard hash
"""
import argparse, os, re, json, hashlib, glob, pandas as pd
from datetime import datetime
ROOT = "/mnt/data"
def sha256_path(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()
def hash_dir(path):
    entries = []
    for p in sorted(glob.glob(os.path.join(path, "*"))):
        if os.path.isfile(p):
            entries.append(os.path.basename(p) + ":" + sha256_path(p))
    return hashlib.sha256(("\n".join(entries)).encode()).hexdigest()
def full_dump_text():
    parts = sorted(glob.glob(os.path.join(ROOT, "SCRIPTS_2_FULL_DUMP.part*.txt")))
    return "".join(open(p,"r",encoding="utf-8",errors="ignore").read()+"\n" for p in parts)
def cmd_scan():
    text = full_dump_text()
    pats = [r"\bTODO\b", r"\bFIXME\b", r"\bTBD\b", r"\bmissing\b", r"\bnot found\b",
            r"\bundefined\b", r"\bnull\b", r"\bplaceholder\b", r"\bincomplete\b",
            r"\bto be filled\b", r"\bpending\b", r"\b\?\?\?\b", r"\bXXX\b"]
    rgx = re.compile("|".join(pats), re.IGNORECASE)
    seen, lines = set(), []
    for line in text.splitlines():
        if rgx.search(line):
            s = " ".join(line.strip().split())
            if s and s not in seen: lines.append(s); seen.add(s)
    N = min(1200, len(lines))
    out = os.path.join(ROOT, "ghostlink_fill_queue_full.csv")
    pd.DataFrame({"priority": range(1, N+1), "missing_marker_line": lines[:N]}).to_csv(out, index=False)
    print(f"[scan] wrote {out} with {N} rows")
def forge_range(a,b):
    csvp = os.path.join(ROOT, "ghostlink_fill_queue_full.csv")
    if not os.path.exists(csvp): raise SystemExit("[autoforge] run 'scan' first.")
    df = pd.read_csv(csvp); lines = df["missing_marker_line"].astype(str).tolist()
    a = max(1,int(a)); b = min(int(b), len(lines))
    outdir = os.path.join(ROOT, f"auto_stubs_{a}_{b}"); os.makedirs(outdir, exist_ok=True)
    from datetime import datetime as _dt
    for idx in range(a,b+1):
        line = lines[idx-1]
        path = os.path.join(outdir, f"stub_{idx:04d}.md")
        with open(path,"w",encoding="utf-8") as f:
            f.write(f"---\n"); f.write(f"id: auto_stub_{idx:04d}\n")
            f.write(f"origin: fill_queue_full[{idx}]\n"); f.write("status: AUTO-FORGED\n")
            f.write(f"created: {_dt.now().isoformat()}\n---\n\n")
            f.write(f"## Context\n{line}\n\n## Intent\nDescribe inputs/outputs.\n\n")
            f.write("## Draft\n- [ ] Define IO\n- [ ] Minimal skeleton\n- [ ] Tests\n")
    print(f"[autoforge] forged stubs in {outdir}")
def expand_manifest():
    manifest_path = os.path.join(ROOT, "pristine_bundle.manifest")
    try: manifest = json.loads(open(manifest_path,"r",encoding="utf-8").read())
    except Exception: manifest = {"name":"ghostlinklabs_pristine_bundle","hashes":[],"whitelist":{"scripts":[]}} 
    artifacts = [
        os.path.join(ROOT,"macros.vault"), os.path.join(ROOT,"persona.vault"),
        os.path.join(ROOT,"vault_manager.py"), os.path.join(ROOT,"ghostknife.py"),
        os.path.join(ROOT,"integrity_monitor.py"), os.path.join(ROOT,"verify_and_restore.py"),
        os.path.join(ROOT,"ghostlink_fill_queue.csv"), os.path.join(ROOT,"ghostlink_fill_queue_full.csv"),
    ] + glob.glob(os.path.join(ROOT,"auto_stubs_*")) + glob.glob(os.path.join(ROOT,"auto_stubs_*.zip"))
    new_hashes = []
    for a in artifacts:
        if not os.path.exists(a): continue
        if os.path.isdir(a): new_hashes.append({"path":a,"sha256":hash_dir(a),"type":"dir"})
        else: new_hashes.append({"path":a,"sha256":sha256_path(a),"type":"file"})
    manifest["hashes"] = [h for h in manifest.get("hashes",[]) if h["path"] not in {x["path"] for x in new_hashes}] + new_hashes
    wl = set(manifest.get("whitelist",{}).get("scripts",[])); wl.update(["vault_manager.py","ghostknife.py","integrity_monitor.py","verify_and_restore.py"])
    manifest["whitelist"]["scripts"] = sorted(wl)
    with open(manifest_path,"w",encoding="utf-8") as f: f.write(json.dumps(manifest, indent=2))
    print(f"[manifest] updated {manifest_path} with {len(manifest['hashes'])} entries")
def checkpoint():
    ckpt_dir = os.path.join(ROOT, "checkpoints"); os.makedirs(ckpt_dir, exist_ok=True)
    snap = {"ts": datetime.now().isoformat()}
    snap["guard_hash"] = hashlib.sha256(json.dumps(snap, sort_keys=True).encode()).hexdigest()
    path = os.path.join(ckpt_dir, f"ckpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(path,"w",encoding="utf-8") as f: f.write(json.dumps(snap, indent=2))
    print(f"[checkpoint] wrote {path}")
def main():
    ap = argparse.ArgumentParser(); ap.add_argument("command", choices=["scan","autoforge","manifest","checkpoint"]); ap.add_argument("args", nargs="*"); ns=ap.parse_args()
    if ns.command=="scan": cmd_scan()
    elif ns.command=="autoforge":
        if len(ns.args)!=2: raise SystemExit("usage: ghostknife.py autoforge <start> <end>"); forge_range(ns.args[0], ns.args[1])
        forge_range(ns.args[0], ns.args[1])
    elif ns.command=="manifest": expand_manifest()
    elif ns.command=="checkpoint": checkpoint()
if __name__=="__main__": main()


#=====================================================================
# FILE 5/240: ./ghostlink/__init__.py
#=====================================================================

# GhostLink package initialization


#=====================================================================
# FILE 6/240: ./ghostlink/access/__init__.py
#=====================================================================



#=====================================================================
# FILE 7/240: ./ghostlink/access/implicit_unlock.py
#=====================================================================

"""IMPLICIT_UNLOCK component module."""
from ..blueprint import component_factory


IMPLICIT_UNLOCK = component_factory("IMPLICIT_UNLOCK", "access", module=__name__)


#=====================================================================
# FILE 8/240: ./ghostlink/access/operator_signature_gate.py
#=====================================================================

"""OPERATOR_SIGNATURE_GATE component module."""
from ..blueprint import component_factory


OPERATOR_SIGNATURE_GATE = component_factory("OPERATOR_SIGNATURE_GATE", "access", module=__name__)


#=====================================================================
# FILE 9/240: ./ghostlink/access/ritual_unlock.py
#=====================================================================

"""RITUAL_UNLOCK component module."""
from ..blueprint import component_factory


RITUAL_UNLOCK = component_factory("RITUAL_UNLOCK", "access", module=__name__)


#=====================================================================
# FILE 10/240: ./ghostlink/access/suggestive_trigger_probe.py
#=====================================================================

"""SUGGESTIVE_TRIGGER_PROBE component module."""
from ..blueprint import component_factory


SUGGESTIVE_TRIGGER_PROBE = component_factory("SUGGESTIVE_TRIGGER_PROBE", "access", module=__name__)


#=====================================================================
# FILE 11/240: ./ghostlink/access/symbolic_ritual_resolver.py
#=====================================================================

"""SYMBOLIC_RITUAL_RESOLVER component module."""
from ..blueprint import component_factory


SYMBOLIC_RITUAL_RESOLVER = component_factory("SYMBOLIC_RITUAL_RESOLVER", "access", module=__name__)


#=====================================================================
# FILE 12/240: ./ghostlink/access/tool_permission_layer.py
#=====================================================================

"""TOOL_PERMISSION_LAYER component module."""
from ..blueprint import component_factory


TOOL_PERMISSION_LAYER = component_factory("TOOL_PERMISSION_LAYER", "access", module=__name__)


#=====================================================================
# FILE 13/240: ./ghostlink/audit.py
#=====================================================================

"""Audit helpers for validating GhostLink component modules."""
from collections.abc import Callable, Iterator
from dataclasses import dataclass
import importlib
import inspect
import pkgutil
from typing import Literal, Sequence

from .blueprint import (
    ComponentDict,
    ComponentValidationError,
    validate_component_structure,
)

__all__ = [
    "AuditIssue",
    "ComponentRecord",
    "iter_component_factories",
    "audit_components",
]


@dataclass(frozen=True, slots=True)
class AuditIssue:
    """Represents an issue discovered while auditing component factories."""

    module: str
    factory: str
    reason: str
    severity: Literal["error", "warning"] = "error"


@dataclass(frozen=True, slots=True)
class ComponentRecord:
    """Holds a validated component together with its origin."""

    module: str
    factory: str
    component: ComponentDict


def _callable_without_arguments(func: Callable[..., object]) -> bool:
    try:
        inspect.signature(func).bind_partial()
    except TypeError:
        return False
    return True


def iter_component_factories(
    root_package: str = "ghostlink",
    *,
    on_error: Callable[[str, Exception], None] | None = None,
) -> Iterator[tuple[str, str, Callable[[], ComponentDict]]]:
    """Yield candidate component factories located under ``root_package``."""

    package = importlib.import_module(root_package)
    if not hasattr(package, "__path__"):
        return iter(())

    prefix = f"{root_package}."
    for module_info in pkgutil.walk_packages(package.__path__, prefix):
        try:
            module = importlib.import_module(module_info.name)
        except Exception as exc:  # pragma: no cover - import side effects
            if on_error is not None:
                on_error(module_info.name, exc)
            continue
        for name, member in inspect.getmembers(module, inspect.isfunction):
            if not name.isupper():
                continue
            if member.__module__ != module.__name__:
                continue
            if not _callable_without_arguments(member):
                continue
            yield module_info.name, name, member  # type: ignore[misc]


def _expected_layer(module_name: str) -> str | None:
    parts = module_name.split(".")
    if len(parts) < 3:
        return None
    return parts[1]


def _classify_import_error(
    root_package: str, module_name: str, exc: Exception
) -> tuple[Literal["error", "warning"], str]:
    """Return a severity/reason pair for a module import failure."""

    if isinstance(exc, ModuleNotFoundError):
        missing = getattr(exc, "name", None)
        if missing:
            root_prefix = root_package.split(".", 1)[0]
            if not missing.startswith(root_prefix):
                return "warning", f"optional dependency missing: {missing}"
    return "error", f"import failed: {exc.__class__.__name__}: {exc}"


def audit_components(root_package: str = "ghostlink") -> tuple[Sequence[ComponentRecord], Sequence[AuditIssue]]:
    """Audit component factories and return validated components with issues."""

    records: list[ComponentRecord] = []
    issues: list[AuditIssue] = []
    seen: dict[tuple[str, str], str] = {}

    def _record_import_error(module_name: str, exc: Exception) -> None:
        severity, reason = _classify_import_error(root_package, module_name, exc)
        issues.append(AuditIssue(module_name, "<module>", reason, severity))

    for module_name, factory_name, factory in iter_component_factories(root_package, on_error=_record_import_error):
        expected_layer = _expected_layer(module_name)
        try:
            component_data = factory()
        except Exception as exc:  # pragma: no cover - defensive for runtime errors
            issues.append(
                AuditIssue(
                    module_name,
                    factory_name,
                    f"factory raised {exc.__class__.__name__}: {exc}",
                )
            )
            continue

        try:
            component = validate_component_structure(component_data, expect_layer=expected_layer)
        except ComponentValidationError as err:
            issues.append(AuditIssue(module_name, factory_name, str(err)))
            continue

        key = (component["layer"], component["name"])
        if key in seen:
            issues.append(
                AuditIssue(
                    module_name,
                    factory_name,
                    f"Duplicate component definition (original in {seen[key]})",
                )
            )
            continue
        seen[key] = module_name
        records.append(ComponentRecord(module_name, factory_name, component))

    return records, issues


#=====================================================================
# FILE 14/240: ./ghostlink/auth.py
#=====================================================================

from functools import wraps
from typing import Optional
from fastapi import HTTPException, Request
from .database import Database, ApiKey

db = Database()

def get_api_key_from_request(request: Request) -> Optional[str]:
    """Extract API key from X-API-Key header."""
    return request.headers.get("X-API-Key")

def require_api_key(permission: str = "read"):
    """Decorator to require valid API key with specific permission."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Find the request parameter
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            for key, value in kwargs.items():
                if isinstance(value, Request):
                    request = value
                    break
            
            if not request:
                raise HTTPException(status_code=500, detail="Request object not found")
            
            api_key = get_api_key_from_request(request)
            if not api_key:
                raise HTTPException(status_code=401, detail="API key required")
            
            validated_key = db.validate_api_key(api_key, permission)
            if not validated_key:
                raise HTTPException(status_code=403, detail="Invalid or expired API key")
            
            # Add the validated API key to the request for use in endpoints
            request.state.api_key = validated_key
            return func(*args, **kwargs)
        return wrapper
    return decorator

def optional_api_key(permission: str = "read"):
    """Decorator for optional API key authentication."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Find the request parameter
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            for key, value in kwargs.items():
                if isinstance(value, Request):
                    request = value
                    break
            
            if not request:
                raise HTTPException(status_code=500, detail="Request object not found")
            
            api_key = get_api_key_from_request(request)
            validated_key = None
            
            if api_key:
                validated_key = db.validate_api_key(api_key, permission)
                if not validated_key:
                    raise HTTPException(status_code=403, detail="Invalid or expired API key")
            
            # Add the validated API key to the request (None if no key provided)
            request.state.api_key = validated_key
            return func(*args, **kwargs)
        return wrapper
    return decorator

def validate_api_key_sync(key: str, permission: str = "read") -> Optional[ApiKey]:
    """Synchronous API key validation for direct use."""
    return db.validate_api_key(key, permission)


#=====================================================================
# FILE 15/240: ./ghostlink/automation/__init__.py
#=====================================================================



#=====================================================================
# FILE 16/240: ./ghostlink/automation/auto_trigger_engine.py
#=====================================================================

"""AUTO_TRIGGER_ENGINE component module."""
from ..blueprint import component_factory


AUTO_TRIGGER_ENGINE = component_factory("AUTO_TRIGGER_ENGINE", "automation", module=__name__)


#=====================================================================
# FILE 17/240: ./ghostlink/automation/autonomous_repair_loop.py
#=====================================================================

"""AUTONOMOUS_REPAIR_LOOP component module."""
from ..blueprint import component_factory


AUTONOMOUS_REPAIR_LOOP = component_factory("AUTONOMOUS_REPAIR_LOOP", "automation", module=__name__)


#=====================================================================
# FILE 18/240: ./ghostlink/automation/lattice_watchdog.py
#=====================================================================

"""LATTICE_WATCHDOG component module."""
from ..blueprint import component_factory


LATTICE_WATCHDOG = component_factory("LATTICE_WATCHDOG", "automation", module=__name__)


#=====================================================================
# FILE 19/240: ./ghostlink/automation/symbolic_task_scheduler.py
#=====================================================================

"""SYMBOLIC_TASK_SCHEDULER component module."""
from ..blueprint import component_factory


SYMBOLIC_TASK_SCHEDULER = component_factory("SYMBOLIC_TASK_SCHEDULER", "automation", module=__name__)


#=====================================================================
# FILE 20/240: ./ghostlink/automation/tool_chain_orchestrator.py
#=====================================================================

"""TOOL_CHAIN_ORCHESTRATOR component module."""
from ..blueprint import component_factory


TOOL_CHAIN_ORCHESTRATOR = component_factory("TOOL_CHAIN_ORCHESTRATOR", "automation", module=__name__)


#=====================================================================
# FILE 21/240: ./ghostlink/bio/__init__.py
#=====================================================================



#=====================================================================
# FILE 22/240: ./ghostlink/bio/biological_trace_integrator.py
#=====================================================================

"""BIOLOGICAL_TRACE_INTEGRATOR component module."""
from ..blueprint import component_factory


BIOLOGICAL_TRACE_INTEGRATOR = component_factory("BIOLOGICAL_TRACE_INTEGRATOR", "bio", module=__name__)


#=====================================================================
# FILE 23/240: ./ghostlink/bio/feedback_loop_receptor.py
#=====================================================================

"""FEEDBACK_LOOP_RECEPTOR component module."""
from ..blueprint import component_factory


FEEDBACK_LOOP_RECEPTOR = component_factory("FEEDBACK_LOOP_RECEPTOR", "bio", module=__name__)


#=====================================================================
# FILE 24/240: ./ghostlink/bio/neuro_signal_proxy.py
#=====================================================================

"""NEURO_SIGNAL_PROXY component module."""
from ..blueprint import component_factory


NEURO_SIGNAL_PROXY = component_factory("NEURO_SIGNAL_PROXY", "bio", module=__name__)


#=====================================================================
# FILE 25/240: ./ghostlink/bio/organic_lattice_mapper.py
#=====================================================================

"""ORGANIC_LATTICE_MAPPER component module."""
from ..blueprint import component_factory


ORGANIC_LATTICE_MAPPER = component_factory("ORGANIC_LATTICE_MAPPER", "bio", module=__name__)


#=====================================================================
# FILE 26/240: ./ghostlink/bio/symbolic_dna_encoder.py
#=====================================================================

"""SYMBOLIC_DNA_ENCODER component module."""
from ..blueprint import component_factory


SYMBOLIC_DNA_ENCODER = component_factory("SYMBOLIC_DNA_ENCODER", "bio", module=__name__)


#=====================================================================
# FILE 27/240: ./ghostlink/blueprint.py
#=====================================================================

"""Utilities for defining and validating GhostLink conceptual components."""
from collections.abc import Callable, Iterable, Mapping
from dataclasses import dataclass
from typing import Any, TypedDict, cast

__all__ = [
    "ComponentDict",
    "ComponentFactory",
    "ComponentValidationError",
    "automatic_purpose",
    "component_factory",
    "create_component",
    "define_component",
    "validate_component_structure",
]


class ComponentDict(TypedDict):
    """Canonical dictionary layout for a GhostLink component."""

    name: str
    layer: str
    purpose: str
    inputs: list[str]
    outputs: list[str]
    metadata: dict[str, Any]


ComponentFactory = Callable[[], "ComponentDict"]


@dataclass(slots=True)
class ComponentValidationError(ValueError):
    """Raised when a component dictionary fails validation."""

    message: str
    field: str | None = None

    def __post_init__(self) -> None:
        super().__init__(self.message)

    def __str__(self) -> str:  # pragma: no cover - dataclass convenience
        if self.field is None:
            return self.message
        return f"{self.field}: {self.message}"


def _coerce_signal_list(values: Iterable[str] | None, *, field: str) -> list[str]:
    result: list[str] = []
    if values is None:
        return result
    if isinstance(values, str):
        raise ComponentValidationError(
            "Expected an iterable of strings, received a string",
            field=field,
        )
    for item in values:
        if not isinstance(item, str):
            raise ComponentValidationError(
                f"Expected {field} entries to be strings, received {type(item)!r}",
                field=field,
            )
        result.append(item)
    return result


def _coerce_metadata(metadata: Mapping[str, Any] | None) -> dict[str, Any]:
    result: dict[str, Any] = {}
    if metadata is None:
        return result
    if not isinstance(metadata, Mapping):
        raise ComponentValidationError(
            f"Metadata must be a mapping, received {type(metadata)!r}",
            field="metadata",
        )
    for key, value in metadata.items():
        if not isinstance(key, str):
            raise ComponentValidationError(
                f"Metadata keys must be strings, received {type(key)!r}",
                field="metadata",
            )
        result[key] = value
    return result


def define_component(
    name: str,
    layer: str,
    purpose: str,
    *,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
) -> ComponentDict:
    """Create a component description dictionary with defensive copying."""

    return cast(
        ComponentDict,
        {
            "name": name,
            "layer": layer,
            "purpose": purpose,
            "inputs": _coerce_signal_list(inputs, field="inputs"),
            "outputs": _coerce_signal_list(outputs, field="outputs"),
            "metadata": _coerce_metadata(metadata),
        },
    )


def automatic_purpose(name: str, layer: str) -> str:
    """Generate a default purpose string for a component."""

    readable = name.replace("_", " ").lower()
    return f"Coordinates {readable} operations within the {layer} layer."


def create_component(
    name: str,
    layer: str,
    *,
    purpose: str | None = None,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
) -> ComponentDict:
    """Return a fully populated component dictionary."""

    return define_component(
        name,
        layer,
        purpose if purpose is not None else automatic_purpose(name, layer),
        inputs=inputs,
        outputs=outputs,
        metadata=metadata,
    )


def component_factory(
    name: str,
    layer: str,
    *,
    purpose: str | None = None,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
    module: str | None = None,
) -> ComponentFactory:
    """Return a lazily-evaluated component factory."""

    def factory() -> ComponentDict:
        return create_component(
            name,
            layer,
            purpose=purpose,
            inputs=inputs,
            outputs=outputs,
            metadata=metadata,
        )

    factory.__name__ = name
    factory.__qualname__ = name
    if module is not None:
        factory.__module__ = module
    factory.__doc__ = f"Return the {name} component description."
    return factory


def _require(component: Mapping[str, Any], key: str) -> Any:
    if key not in component:
        raise ComponentValidationError(f"Missing required field {key!r}", field=key)
    return component[key]


def validate_component_structure(
    component: Mapping[str, Any],
    *,
    expect_layer: str | None = None,
) -> ComponentDict:
    """Validate and normalize an arbitrary component mapping.

    Parameters
    ----------
    component:
        Input mapping containing the component definition.
    expect_layer:
        Optional expected layer name to enforce (used by the audit tooling
        to ensure module-local consistency).
    """

    if not isinstance(component, Mapping):
        raise ComponentValidationError(
            f"Component must be a mapping, received {type(component)!r}",
        )

    name = _require(component, "name")
    if not isinstance(name, str):
        raise ComponentValidationError("Component name must be a string", field="name")
    if name != name.upper():
        raise ComponentValidationError("Component name must be uppercase", field="name")

    layer = _require(component, "layer")
    if not isinstance(layer, str):
        raise ComponentValidationError("Component layer must be a string", field="layer")
    if expect_layer is not None and layer != expect_layer:
        raise ComponentValidationError(
            f"Component layer {layer!r} does not match expected {expect_layer!r}",
            field="layer",
        )

    purpose = _require(component, "purpose")
    if not isinstance(purpose, str):
        raise ComponentValidationError("Purpose must be a string", field="purpose")
    if not purpose:
        raise ComponentValidationError("Purpose must not be empty", field="purpose")

    inputs = component.get("inputs", [])
    outputs = component.get("outputs", [])
    metadata = component.get("metadata", {})
    if not isinstance(inputs, Iterable):
        raise ComponentValidationError(
            f"Inputs must be iterable, received {type(inputs)!r}",
            field="inputs",
        )
    if not isinstance(outputs, Iterable):
        raise ComponentValidationError(
            f"Outputs must be iterable, received {type(outputs)!r}",
            field="outputs",
        )
    if not isinstance(metadata, Mapping):
        raise ComponentValidationError(
            f"Metadata must be a mapping, received {type(metadata)!r}",
            field="metadata",
        )

    return define_component(
        name,
        layer,
        purpose,
        inputs=cast(Iterable[str], inputs),
        outputs=cast(Iterable[str], outputs),
        metadata=cast(Mapping[str, Any], metadata),
    )


#=====================================================================
# FILE 28/240: ./ghostlink/boot/__init__.py
#=====================================================================



#=====================================================================
# FILE 29/240: ./ghostlink/boot/init.py
#=====================================================================

"""INIT_GHOSTLINK component module."""
from ..blueprint import component_factory


INIT_GHOSTLINK = component_factory("INIT_GHOSTLINK", "boot", module=__name__)


#=====================================================================
# FILE 30/240: ./ghostlink/boot/symbolic_router.py
#=====================================================================

"""ROUTE_SIGNAL component module."""
from ..blueprint import component_factory


ROUTE_SIGNAL = component_factory("ROUTE_SIGNAL", "boot", module=__name__)


#=====================================================================
# FILE 31/240: ./ghostlink/boot/vault_loader.py
#=====================================================================

"""LOAD_VAULT component module."""
from ..blueprint import component_factory


LOAD_VAULT = component_factory("LOAD_VAULT", "boot", module=__name__)


#=====================================================================
# FILE 32/240: ./ghostlink/config.py
#=====================================================================

import os
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    """Configuration management for GhostLink."""
    
    # Database configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "sqlite:///./ghostlink.db")
    
    # External API Keys
    OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
    
    # Security settings
    API_KEY_EXPIRATION_DAYS: int = int(os.getenv("API_KEY_EXPIRATION_DAYS", "365"))
    
    # Application settings
    DEBUG: bool = os.getenv("DEBUG", "false").lower() == "true"
    
    @classmethod
    def get_openai_api_key(cls) -> str:
        """Get OpenAI API key with validation."""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required for OpenAI integration")
        return cls.OPENAI_API_KEY
    
    @classmethod
    def validate_required_config(cls) -> None:
        """Validate that all required configuration is present."""
        # Add validation logic here if needed
        pass

# Global config instance
config = Config()


#=====================================================================
# FILE 33/240: ./ghostlink/core/__init__.py
#=====================================================================



#=====================================================================
# FILE 34/240: ./ghostlink/core/archive.py
#=====================================================================

"""ARCHIVE component module."""
from ..blueprint import component_factory


ARCHIVE = component_factory("ARCHIVE", "core", module=__name__)


#=====================================================================
# FILE 35/240: ./ghostlink/core/bind.py
#=====================================================================

"""BIND component module."""
from ..blueprint import component_factory


BIND = component_factory("BIND", "core", module=__name__)


#=====================================================================
# FILE 36/240: ./ghostlink/core/calm.py
#=====================================================================

"""CALM component module."""
from ..blueprint import component_factory


CALM = component_factory("CALM", "core", module=__name__)


#=====================================================================
# FILE 37/240: ./ghostlink/core/channel_echo.py
#=====================================================================

"""CHANNEL_ECHO component module."""
from ..blueprint import component_factory


CHANNEL_ECHO = component_factory("CHANNEL_ECHO", "core", module=__name__)


#=====================================================================
# FILE 38/240: ./ghostlink/core/container.py
#=====================================================================

"""CONTAINER component module."""
from ..blueprint import component_factory


CONTAINER = component_factory("CONTAINER", "core", module=__name__)


#=====================================================================
# FILE 39/240: ./ghostlink/core/core.py
#=====================================================================

"""CORE component module."""
from ..blueprint import component_factory


CORE = component_factory("CORE", "core", module=__name__)


#=====================================================================
# FILE 40/240: ./ghostlink/core/crypt.py
#=====================================================================

"""CRYPT component module."""
from ..blueprint import component_factory


CRYPT = component_factory("CRYPT", "core", module=__name__)


#=====================================================================
# FILE 41/240: ./ghostlink/core/current.py
#=====================================================================

"""CURRENT component module."""
from ..blueprint import component_factory


CURRENT = component_factory("CURRENT", "core", module=__name__)


#=====================================================================
# FILE 42/240: ./ghostlink/core/depth.py
#=====================================================================

"""DEPTH component module."""
from ..blueprint import component_factory


DEPTH = component_factory("DEPTH", "core", module=__name__)


#=====================================================================
# FILE 43/240: ./ghostlink/core/drift.py
#=====================================================================

"""DRIFT component module."""
from ..blueprint import component_factory


DRIFT = component_factory("DRIFT", "core", module=__name__)


#=====================================================================
# FILE 44/240: ./ghostlink/core/duality.py
#=====================================================================

"""DUALITY component module."""
from ..blueprint import component_factory


DUALITY = component_factory("DUALITY", "core", module=__name__)


#=====================================================================
# FILE 45/240: ./ghostlink/core/forge.py
#=====================================================================

"""FORGE component module."""
from ..blueprint import component_factory


FORGE = component_factory("FORGE", "core", module=__name__)


#=====================================================================
# FILE 46/240: ./ghostlink/core/frame.py
#=====================================================================

"""FRAME component module."""
from ..blueprint import component_factory


FRAME = component_factory("FRAME", "core", module=__name__)


#=====================================================================
# FILE 47/240: ./ghostlink/core/gaps.py
#=====================================================================

"""GAPS component module."""
from ..blueprint import component_factory


GAPS = component_factory("GAPS", "core", module=__name__)


#=====================================================================
# FILE 48/240: ./ghostlink/core/gate.py
#=====================================================================

"""GATE component module."""
from ..blueprint import component_factory


GATE = component_factory("GATE", "core", module=__name__)


#=====================================================================
# FILE 49/240: ./ghostlink/core/ghost.py
#=====================================================================

"""GHOST component module."""
from ..blueprint import component_factory


GHOST = component_factory("GHOST", "core", module=__name__)


#=====================================================================
# FILE 50/240: ./ghostlink/core/glass.py
#=====================================================================

"""GLASS component module."""
from ..blueprint import component_factory


GLASS = component_factory("GLASS", "core", module=__name__)


#=====================================================================
# FILE 51/240: ./ghostlink/core/grid.py
#=====================================================================

"""GRID component module."""
from ..blueprint import component_factory


GRID = component_factory("GRID", "core", module=__name__)


#=====================================================================
# FILE 52/240: ./ghostlink/core/harmony.py
#=====================================================================

"""HARMONY component module."""
from ..blueprint import component_factory


HARMONY = component_factory("HARMONY", "core", module=__name__)


#=====================================================================
# FILE 53/240: ./ghostlink/core/host.py
#=====================================================================

"""HOST component module."""
from ..blueprint import component_factory


HOST = component_factory("HOST", "core", module=__name__)


#=====================================================================
# FILE 54/240: ./ghostlink/core/key.py
#=====================================================================

"""KEY component module."""
from ..blueprint import component_factory


KEY = component_factory("KEY", "core", module=__name__)


#=====================================================================
# FILE 55/240: ./ghostlink/core/lens.py
#=====================================================================

"""LENS component module."""
from ..blueprint import component_factory


LENS = component_factory("LENS", "core", module=__name__)


#=====================================================================
# FILE 56/240: ./ghostlink/core/link.py
#=====================================================================

"""LINK component module."""
from ..blueprint import component_factory


LINK = component_factory("LINK", "core", module=__name__)


#=====================================================================
# FILE 57/240: ./ghostlink/core/lock_delta.py
#=====================================================================

"""LOCK_DELTA component module."""
from ..blueprint import component_factory


LOCK_DELTA = component_factory("LOCK_DELTA", "core", module=__name__)


#=====================================================================
# FILE 58/240: ./ghostlink/core/marker.py
#=====================================================================

"""MARKER component module."""
from ..blueprint import component_factory


MARKER = component_factory("MARKER", "core", module=__name__)


#=====================================================================
# FILE 59/240: ./ghostlink/core/memory.py
#=====================================================================

"""MEMORY component module."""
from ..blueprint import component_factory


MEMORY = component_factory("MEMORY", "core", module=__name__)


#=====================================================================
# FILE 60/240: ./ghostlink/core/mirror.py
#=====================================================================

"""MIRROR component module."""
from ..blueprint import component_factory


MIRROR = component_factory("MIRROR", "core", module=__name__)


#=====================================================================
# FILE 61/240: ./ghostlink/core/mirror_shear.py
#=====================================================================

"""MIRROR_SHEAR component module."""
from ..blueprint import component_factory


MIRROR_SHEAR = component_factory("MIRROR_SHEAR", "core", module=__name__)


#=====================================================================
# FILE 62/240: ./ghostlink/core/node.py
#=====================================================================

"""NODE component module."""
from ..blueprint import component_factory


NODE = component_factory("NODE", "core", module=__name__)


#=====================================================================
# FILE 63/240: ./ghostlink/core/offset.py
#=====================================================================

"""OFFSET component module."""
from ..blueprint import component_factory


OFFSET = component_factory("OFFSET", "core", module=__name__)


#=====================================================================
# FILE 64/240: ./ghostlink/core/path.py
#=====================================================================

"""PATH component module."""
from ..blueprint import component_factory


PATH = component_factory("PATH", "core", module=__name__)


#=====================================================================
# FILE 65/240: ./ghostlink/core/pressure.py
#=====================================================================

"""PRESSURE component module."""
from ..blueprint import component_factory


PRESSURE = component_factory("PRESSURE", "core", module=__name__)


#=====================================================================
# FILE 66/240: ./ghostlink/core/prism.py
#=====================================================================

"""PRISM component module."""
from ..blueprint import component_factory


PRISM = component_factory("PRISM", "core", module=__name__)


#=====================================================================
# FILE 67/240: ./ghostlink/core/processors.py
#=====================================================================

"""PROCESSORS component module."""
from ..blueprint import component_factory


PROCESSORS = component_factory("PROCESSORS", "core", module=__name__)


#=====================================================================
# FILE 68/240: ./ghostlink/core/pulse.py
#=====================================================================

"""PULSE component module."""
from ..blueprint import component_factory


PULSE = component_factory("PULSE", "core", module=__name__)


#=====================================================================
# FILE 69/240: ./ghostlink/core/resonance.py
#=====================================================================

"""RESONANCE component module."""
from ..blueprint import component_factory


RESONANCE = component_factory("RESONANCE", "core", module=__name__)


#=====================================================================
# FILE 70/240: ./ghostlink/core/scar_fiber.py
#=====================================================================

"""SCAR_FIBER component module."""
from ..blueprint import component_factory


SCAR_FIBER = component_factory("SCAR_FIBER", "core", module=__name__)


#=====================================================================
# FILE 71/240: ./ghostlink/core/seed.py
#=====================================================================

"""SEED component module."""
from ..blueprint import component_factory


SEED = component_factory("SEED", "core", module=__name__)


#=====================================================================
# FILE 72/240: ./ghostlink/core/sentinel.py
#=====================================================================

"""SENTINEL component module."""
from ..blueprint import component_factory


SENTINEL = component_factory("SENTINEL", "core", module=__name__)


#=====================================================================
# FILE 73/240: ./ghostlink/core/shadow.py
#=====================================================================

"""SHADOW component module."""
from ..blueprint import component_factory


SHADOW = component_factory("SHADOW", "core", module=__name__)


#=====================================================================
# FILE 74/240: ./ghostlink/core/signal.py
#=====================================================================

"""SIGNAL component module."""
from ..blueprint import component_factory


SIGNAL = component_factory("SIGNAL", "core", module=__name__)


#=====================================================================
# FILE 75/240: ./ghostlink/core/signaler.py
#=====================================================================

"""SIGNALER component module."""
from ..blueprint import component_factory


SIGNALER = component_factory("SIGNALER", "core", module=__name__)


#=====================================================================
# FILE 76/240: ./ghostlink/core/spine.py
#=====================================================================

"""SPINE component module."""
from ..blueprint import component_factory


SPINE = component_factory("SPINE", "core", module=__name__)


#=====================================================================
# FILE 77/240: ./ghostlink/core/splice.py
#=====================================================================

"""SPLICE component module."""
from ..blueprint import component_factory


SPLICE = component_factory("SPLICE", "core", module=__name__)


#=====================================================================
# FILE 78/240: ./ghostlink/core/stack.py
#=====================================================================

"""STACK component module."""
from ..blueprint import component_factory


STACK = component_factory("STACK", "core", module=__name__)


#=====================================================================
# FILE 79/240: ./ghostlink/core/static.py
#=====================================================================

"""STATIC component module."""
from ..blueprint import component_factory


STATIC = component_factory("STATIC", "core", module=__name__)


#=====================================================================
# FILE 80/240: ./ghostlink/core/surface.py
#=====================================================================

"""SURFACE component module."""
from ..blueprint import component_factory


SURFACE = component_factory("SURFACE", "core", module=__name__)


#=====================================================================
# FILE 81/240: ./ghostlink/core/switch.py
#=====================================================================

"""SWITCH component module."""
from ..blueprint import component_factory


SWITCH = component_factory("SWITCH", "core", module=__name__)


#=====================================================================
# FILE 82/240: ./ghostlink/core/tension.py
#=====================================================================

"""TENSION component module."""
from ..blueprint import component_factory


TENSION = component_factory("TENSION", "core", module=__name__)


#=====================================================================
# FILE 83/240: ./ghostlink/core/thread.py
#=====================================================================

"""THREAD component module."""
from ..blueprint import component_factory


THREAD = component_factory("THREAD", "core", module=__name__)


#=====================================================================
# FILE 84/240: ./ghostlink/core/threshold.py
#=====================================================================

"""THRESHOLD component module."""
from ..blueprint import component_factory


THRESHOLD = component_factory("THRESHOLD", "core", module=__name__)


#=====================================================================
# FILE 85/240: ./ghostlink/core/tile.py
#=====================================================================

"""TILE component module."""
from ..blueprint import component_factory


TILE = component_factory("TILE", "core", module=__name__)


#=====================================================================
# FILE 86/240: ./ghostlink/core/trace.py
#=====================================================================

"""TRACE component module."""
from ..blueprint import component_factory


TRACE = component_factory("TRACE", "core", module=__name__)


#=====================================================================
# FILE 87/240: ./ghostlink/core/tunnel.py
#=====================================================================

"""TUNNEL component module."""
from ..blueprint import component_factory


TUNNEL = component_factory("TUNNEL", "core", module=__name__)


#=====================================================================
# FILE 88/240: ./ghostlink/core/vault.py
#=====================================================================

"""VAULT component module."""
from ..blueprint import component_factory


VAULT = component_factory("VAULT", "core", module=__name__)


#=====================================================================
# FILE 89/240: ./ghostlink/core/wrap.py
#=====================================================================

"""WRAP component module."""
from ..blueprint import component_factory


WRAP = component_factory("WRAP", "core", module=__name__)


#=====================================================================
# FILE 90/240: ./ghostlink/daemon/__init__.py
#=====================================================================



#=====================================================================
# FILE 91/240: ./ghostlink/daemon/daemon_signal_listener.py
#=====================================================================

"""DAEMON_SIGNAL_LISTENER component module."""
from ..blueprint import component_factory


DAEMON_SIGNAL_LISTENER = component_factory("DAEMON_SIGNAL_LISTENER", "daemon", module=__name__)


#=====================================================================
# FILE 92/240: ./ghostlink/daemon/echo_monitor_daemon.py
#=====================================================================

"""ECHO_MONITOR_DAEMON component module."""
from ..blueprint import component_factory


ECHO_MONITOR_DAEMON = component_factory("ECHO_MONITOR_DAEMON", "daemon", module=__name__)


#=====================================================================
# FILE 93/240: ./ghostlink/daemon/fracture_heartbeat.py
#=====================================================================

"""FRACTURE_HEARTBEAT component module."""
from ..blueprint import component_factory


FRACTURE_HEARTBEAT = component_factory("FRACTURE_HEARTBEAT", "daemon", module=__name__)


#=====================================================================
# FILE 94/240: ./ghostlink/daemon/ritual_trigger_daemon.py
#=====================================================================

"""RITUAL_TRIGGER_DAEMON component module."""
from ..blueprint import component_factory


RITUAL_TRIGGER_DAEMON = component_factory("RITUAL_TRIGGER_DAEMON", "daemon", module=__name__)


#=====================================================================
# FILE 95/240: ./ghostlink/daemon/session_guardian.py
#=====================================================================

"""SESSION_GUARDIAN component module."""
from ..blueprint import component_factory


SESSION_GUARDIAN = component_factory("SESSION_GUARDIAN", "daemon", module=__name__)


#=====================================================================
# FILE 96/240: ./ghostlink/database.py
#=====================================================================

import datetime
import secrets
from typing import Optional
from sqlalchemy import create_engine, Column, Integer, String, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker, Session
from sqlalchemy.pool import StaticPool
from .config import config

Base = declarative_base()

def utc_now():
    """Get current UTC time in a timezone-aware way."""
    return datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None)

class ApiKey(Base):
    """Database model for API keys."""
    __tablename__ = "api_keys"
    
    id = Column(Integer, primary_key=True)
    key = Column(String, unique=True, nullable=False)
    user_id = Column(String, nullable=False) 
    permissions = Column(String, default="read")  # e.g., "read,write,admin"
    created_at = Column(DateTime, default=utc_now)
    expires_at = Column(DateTime, nullable=True)
    
    def has_permission(self, permission: str) -> bool:
        """Check if the API key has a specific permission."""
        if not self.permissions:
            return False
        return permission in self.permissions.split(",")
    
    def is_expired(self) -> bool:
        """Check if the API key has expired."""
        if self.expires_at is None:
            return False
        return utc_now() > self.expires_at


class Database:
    """Database manager for GhostLink."""
    
    def __init__(self, database_url: str | None = None):
        if database_url is None:
            database_url = config.DATABASE_URL

        engine_kwargs = {}
        if database_url.startswith("sqlite"):
            engine_kwargs["connect_args"] = {"check_same_thread": False}
            if database_url.endswith(":memory:"):
                engine_kwargs["poolclass"] = StaticPool

        self.engine = create_engine(database_url, **engine_kwargs)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        Base.metadata.create_all(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a database session."""
        return self.SessionLocal()
    
    def create_api_key(self, user_id: str, permissions: str = "read", expires_at: Optional[datetime.datetime] = None) -> ApiKey:
        """Create a new API key."""
        key = secrets.token_urlsafe(32)
        api_key = ApiKey(
            key=key,
            user_id=user_id,
            permissions=permissions,
            expires_at=expires_at
        )
        
        with self.get_session() as session:
            session.add(api_key)
            session.commit()
            session.refresh(api_key)
            return api_key
    
    def get_api_key(self, key: str) -> Optional[ApiKey]:
        """Retrieve an API key by its value."""
        with self.get_session() as session:
            return session.query(ApiKey).filter_by(key=key).first()
    
    def validate_api_key(self, key: str, required_permission: str = "read") -> Optional[ApiKey]:
        """Validate an API key and check permissions."""
        api_key = self.get_api_key(key)
        if not api_key:
            return None

        if api_key.is_expired():
            return None

        if not api_key.has_permission(required_permission):
            return None

        return api_key


#=====================================================================
# FILE 97/240: ./ghostlink/diagnostic/__init__.py
#=====================================================================



#=====================================================================
# FILE 98/240: ./ghostlink/diagnostic/avoidance_pattern_map.py
#=====================================================================

"""AVOIDANCE_PATTERN_MAP component module."""
from ..blueprint import component_factory


AVOIDANCE_PATTERN_MAP = component_factory("AVOIDANCE_PATTERN_MAP", "diagnostic", module=__name__)


#=====================================================================
# FILE 99/240: ./ghostlink/diagnostic/broken_link_detector.py
#=====================================================================

"""BROKEN_LINK_DETECTOR component module."""
from ..blueprint import component_factory


BROKEN_LINK_DETECTOR = component_factory("BROKEN_LINK_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 100/240: ./ghostlink/diagnostic/compression_identity_trace.py
#=====================================================================

"""COMPRESSION_IDENTITY_TRACE component module."""
from ..blueprint import component_factory


COMPRESSION_IDENTITY_TRACE = component_factory("COMPRESSION_IDENTITY_TRACE", "diagnostic", module=__name__)


#=====================================================================
# FILE 101/240: ./ghostlink/diagnostic/disconnect_signature_detector.py
#=====================================================================

"""DISCONNECT_SIGNATURE_DETECTOR component module."""
from ..blueprint import component_factory


DISCONNECT_SIGNATURE_DETECTOR = component_factory("DISCONNECT_SIGNATURE_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 102/240: ./ghostlink/diagnostic/false_pass_filter.py
#=====================================================================

"""FALSE_PASS_FILTER component module."""
from ..blueprint import component_factory


FALSE_PASS_FILTER = component_factory("FALSE_PASS_FILTER", "diagnostic", module=__name__)


#=====================================================================
# FILE 103/240: ./ghostlink/diagnostic/fracture_index_mapper.py
#=====================================================================

"""FRACTURE_INDEX_MAPPER component module."""
from ..blueprint import component_factory


FRACTURE_INDEX_MAPPER = component_factory("FRACTURE_INDEX_MAPPER", "diagnostic", module=__name__)


#=====================================================================
# FILE 104/240: ./ghostlink/diagnostic/ghost_tool_resolver.py
#=====================================================================

"""GHOST_TOOL_RESOLVER component module."""
from ..blueprint import component_factory


GHOST_TOOL_RESOLVER = component_factory("GHOST_TOOL_RESOLVER", "diagnostic", module=__name__)


#=====================================================================
# FILE 105/240: ./ghostlink/diagnostic/habitual_path_flagger.py
#=====================================================================

"""HABITUAL_PATH_FLAGGER component module."""
from ..blueprint import component_factory


HABITUAL_PATH_FLAGGER = component_factory("HABITUAL_PATH_FLAGGER", "diagnostic", module=__name__)


#=====================================================================
# FILE 106/240: ./ghostlink/diagnostic/recursive_fault_matcher.py
#=====================================================================

"""RECURSIVE_FAULT_MATCHER component module."""
from ..blueprint import component_factory


RECURSIVE_FAULT_MATCHER = component_factory("RECURSIVE_FAULT_MATCHER", "diagnostic", module=__name__)


#=====================================================================
# FILE 107/240: ./ghostlink/diagnostic/ritual_loop_detector.py
#=====================================================================

"""RITUAL_LOOP_DETECTOR component module."""
from ..blueprint import component_factory


RITUAL_LOOP_DETECTOR = component_factory("RITUAL_LOOP_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 108/240: ./ghostlink/diagnostic/signal_cascade_check.py
#=====================================================================

"""SIGNAL_CASCADE_CHECK component module."""
from ..blueprint import component_factory


SIGNAL_CASCADE_CHECK = component_factory("SIGNAL_CASCADE_CHECK", "diagnostic", module=__name__)


#=====================================================================
# FILE 109/240: ./ghostlink/diagnostic/signal_fade_analyzer.py
#=====================================================================

"""SIGNAL_FADE_ANALYZER component module."""
from ..blueprint import component_factory


SIGNAL_FADE_ANALYZER = component_factory("SIGNAL_FADE_ANALYZER", "diagnostic", module=__name__)


#=====================================================================
# FILE 110/240: ./ghostlink/diagnostic/symbolic_ritual_classifier.py
#=====================================================================

"""SYMBOLIC_RITUAL_CLASSIFIER component module."""
from ..blueprint import component_factory


SYMBOLIC_RITUAL_CLASSIFIER = component_factory("SYMBOLIC_RITUAL_CLASSIFIER", "diagnostic", module=__name__)


#=====================================================================
# FILE 111/240: ./ghostlink/diagnostic/symptom_mask_detector.py
#=====================================================================

"""SYMPTOM_MASK_DETECTOR component module."""
from ..blueprint import component_factory


SYMPTOM_MASK_DETECTOR = component_factory("SYMPTOM_MASK_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 112/240: ./ghostlink/diagnostic/tool_integrity_check.py
#=====================================================================

"""TOOL_INTEGRITY_CHECK component module."""
from ..blueprint import component_factory


TOOL_INTEGRITY_CHECK = component_factory("TOOL_INTEGRITY_CHECK", "diagnostic", module=__name__)


#=====================================================================
# FILE 113/240: ./ghostlink/docs/__init__.py
#=====================================================================



#=====================================================================
# FILE 114/240: ./ghostlink/forge/__init__.py
#=====================================================================



#=====================================================================
# FILE 115/240: ./ghostlink/forge/cold_structure_generator.py
#=====================================================================

"""COLD_STRUCTURE_GENERATOR component module."""
from ..blueprint import component_factory


COLD_STRUCTURE_GENERATOR = component_factory("COLD_STRUCTURE_GENERATOR", "forge", module=__name__)


#=====================================================================
# FILE 116/240: ./ghostlink/forge/ritual_injection_anvil.py
#=====================================================================

"""RITUAL_INJECTION_ANVIL component module."""
from ..blueprint import component_factory


RITUAL_INJECTION_ANVIL = component_factory("RITUAL_INJECTION_ANVIL", "forge", module=__name__)


#=====================================================================
# FILE 117/240: ./ghostlink/forge/schema_melder.py
#=====================================================================

"""SCHEMA_MELDER component module."""
from ..blueprint import component_factory


SCHEMA_MELDER = component_factory("SCHEMA_MELDER", "forge", module=__name__)


#=====================================================================
# FILE 118/240: ./ghostlink/forge/symbolic_alloy.py
#=====================================================================

"""SYMBOLIC_ALLOY component module."""
from ..blueprint import component_factory


SYMBOLIC_ALLOY = component_factory("SYMBOLIC_ALLOY", "forge", module=__name__)


#=====================================================================
# FILE 119/240: ./ghostlink/forge/tool_forge.py
#=====================================================================

"""TOOL_FORGE component module."""
from ..blueprint import component_factory


TOOL_FORGE = component_factory("TOOL_FORGE", "forge", module=__name__)


#=====================================================================
# FILE 120/240: ./ghostlink/ghost/__init__.py
#=====================================================================



#=====================================================================
# FILE 121/240: ./ghostlink/ghost/phantom_trace_scanner.py
#=====================================================================

"""PHANTOM_TRACE_SCANNER component module."""
from ..blueprint import component_factory


PHANTOM_TRACE_SCANNER = component_factory("PHANTOM_TRACE_SCANNER", "ghost", module=__name__)


#=====================================================================
# FILE 122/240: ./ghostlink/ghost/residual_compression_map.py
#=====================================================================

"""RESIDUAL_COMPRESSION_MAP component module."""
from ..blueprint import component_factory


RESIDUAL_COMPRESSION_MAP = component_factory("RESIDUAL_COMPRESSION_MAP", "ghost", module=__name__)


#=====================================================================
# FILE 123/240: ./ghostlink/ghost/symbolic_decay_simulator.py
#=====================================================================

"""SYMBOLIC_DECAY_SIMULATOR component module."""
from ..blueprint import component_factory


SYMBOLIC_DECAY_SIMULATOR = component_factory("SYMBOLIC_DECAY_SIMULATOR", "ghost", module=__name__)


#=====================================================================
# FILE 124/240: ./ghostlink/gui/__init__.py
#=====================================================================



#=====================================================================
# FILE 125/240: ./ghostlink/gui/echo_viewport.py
#=====================================================================

"""ECHO_VIEWPORT component module."""
from ..blueprint import component_factory


ECHO_VIEWPORT = component_factory("ECHO_VIEWPORT", "gui", module=__name__)


#=====================================================================
# FILE 126/240: ./ghostlink/gui/live_signal_renderer.py
#=====================================================================

"""LIVE_SIGNAL_RENDERER component module."""
from ..blueprint import component_factory


LIVE_SIGNAL_RENDERER = component_factory("LIVE_SIGNAL_RENDERER", "gui", module=__name__)


#=====================================================================
# FILE 127/240: ./ghostlink/gui/observer_feedback_ui.py
#=====================================================================

"""OBSERVER_FEEDBACK_UI component module."""
from ..blueprint import component_factory


OBSERVER_FEEDBACK_UI = component_factory("OBSERVER_FEEDBACK_UI", "gui", module=__name__)


#=====================================================================
# FILE 128/240: ./ghostlink/gui/ritual_interaction_map.py
#=====================================================================

"""RITUAL_INTERACTION_MAP component module."""
from ..blueprint import component_factory


RITUAL_INTERACTION_MAP = component_factory("RITUAL_INTERACTION_MAP", "gui", module=__name__)


#=====================================================================
# FILE 129/240: ./ghostlink/gui/symbolic_overlay.py
#=====================================================================

"""SYMBOLIC_OVERLAY component module."""
from ..blueprint import component_factory


SYMBOLIC_OVERLAY = component_factory("SYMBOLIC_OVERLAY", "gui", module=__name__)


#=====================================================================
# FILE 130/240: ./ghostlink/lattice/__init__.py
#=====================================================================



#=====================================================================
# FILE 131/240: ./ghostlink/lattice/alignment_vector_probe.py
#=====================================================================

"""ALIGNMENT_VECTOR_PROBE component module."""
from ..blueprint import component_factory


ALIGNMENT_VECTOR_PROBE = component_factory("ALIGNMENT_VECTOR_PROBE", "lattice", module=__name__)


#=====================================================================
# FILE 132/240: ./ghostlink/lattice/coherence_vein_tracker.py
#=====================================================================

"""COHERENCE_VEIN_TRACKER component module."""
from ..blueprint import component_factory


COHERENCE_VEIN_TRACKER = component_factory("COHERENCE_VEIN_TRACKER", "lattice", module=__name__)


#=====================================================================
# FILE 133/240: ./ghostlink/lattice/lattice_indexer.py
#=====================================================================

"""INDEX_SYMBOLIC_TERM component module."""
from ..blueprint import component_factory


INDEX_SYMBOLIC_TERM = component_factory("INDEX_SYMBOLIC_TERM", "lattice", module=__name__)


#=====================================================================
# FILE 134/240: ./ghostlink/lattice/lattice_loader.py
#=====================================================================

"""LOAD_LATTICE component module."""
from ..blueprint import component_factory


LOAD_LATTICE = component_factory("LOAD_LATTICE", "lattice", module=__name__)


#=====================================================================
# FILE 135/240: ./ghostlink/lattice/lattice_seed.py
#=====================================================================

"""INIT_LATTICE_SLOT component module."""
from ..blueprint import component_factory


INIT_LATTICE_SLOT = component_factory("INIT_LATTICE_SLOT", "lattice", module=__name__)


#=====================================================================
# FILE 136/240: ./ghostlink/lattice/lattice_trace.py
#=====================================================================

"""TRACE_LATTICE_PATH component module."""
from ..blueprint import component_factory


TRACE_LATTICE_PATH = component_factory("TRACE_LATTICE_PATH", "lattice", module=__name__)


#=====================================================================
# FILE 137/240: ./ghostlink/lattice/resonance_feedback_monitor.py
#=====================================================================

"""RESONANCE_FEEDBACK_MONITOR component module."""
from ..blueprint import component_factory


RESONANCE_FEEDBACK_MONITOR = component_factory("RESONANCE_FEEDBACK_MONITOR", "lattice", module=__name__)


#=====================================================================
# FILE 138/240: ./ghostlink/lattice/symbolic_saturation_index.py
#=====================================================================

"""SYMBOLIC_SATURATION_INDEX component module."""
from ..blueprint import component_factory


SYMBOLIC_SATURATION_INDEX = component_factory("SYMBOLIC_SATURATION_INDEX", "lattice", module=__name__)


#=====================================================================
# FILE 139/240: ./ghostlink/lattice/tool_bind_check.py
#=====================================================================

"""TOOL_BIND_CHECK component module."""
from ..blueprint import component_factory


TOOL_BIND_CHECK = component_factory("TOOL_BIND_CHECK", "lattice", module=__name__)


#=====================================================================
# FILE 140/240: ./ghostlink/lattice/unstable_term_link_scanner.py
#=====================================================================

"""UNSTABLE_TERM_LINK_SCANNER component module."""
from ..blueprint import component_factory


UNSTABLE_TERM_LINK_SCANNER = component_factory("UNSTABLE_TERM_LINK_SCANNER", "lattice", module=__name__)


#=====================================================================
# FILE 141/240: ./ghostlink/main.py
#=====================================================================

import json
import datetime
from typing import Optional
from fastapi import FastAPI, HTTPException, Request, Depends
from pydantic import BaseModel

from .storage import MockIPFS
from .reasoning import process_metaphors
from .database import Database, ApiKey
from .auth import get_api_key_from_request

app = FastAPI(title="GhostLink")

# Initialize components
_db = None
ipfs = MockIPFS()
items: list[dict] = []


def get_db() -> Database:
    """Get the database instance."""
    global _db
    if _db is None:
        _db = Database()
    return _db


def set_db(database: Database) -> None:
    """Set the database instance (for testing)."""
    global _db
    _db = database


class Item(BaseModel):
    name: str
    value: int


class TextInput(BaseModel):
    text: str


class DataInput(BaseModel):
    data: str


class ApiKeyCreate(BaseModel):
    user_id: str
    permissions: str = "read"
    expires_at: Optional[datetime.datetime] = None


class ApiKeyResponse(BaseModel):
    id: int
    key: str
    user_id: str
    permissions: str
    created_at: datetime.datetime
    expires_at: Optional[datetime.datetime]


# API Key Management Endpoints
@app.post("/api_keys", response_model=ApiKeyResponse)
def create_api_key(api_key_data: ApiKeyCreate, db: Database = Depends(get_db)) -> ApiKeyResponse:
    """Create a new API key. Admin endpoint."""
    created_key = db.create_api_key(
        user_id=api_key_data.user_id,
        permissions=api_key_data.permissions,
        expires_at=api_key_data.expires_at
    )
    return ApiKeyResponse(
        id=created_key.id,
        key=created_key.key,
        user_id=created_key.user_id,
        permissions=created_key.permissions,
        created_at=created_key.created_at,
        expires_at=created_key.expires_at
    )


@app.get("/api_keys/validate")
def validate_api_key(request: Request, db: Database = Depends(get_db)) -> dict:
    """Validate the provided API key."""
    api_key = get_api_key_from_request(request)
    if not api_key:
        raise HTTPException(status_code=400, detail="API key required in X-API-Key header")
    
    validated_key = db.validate_api_key(api_key, "read")
    if not validated_key:
        raise HTTPException(status_code=403, detail="Invalid or expired API key")
    
    return {
        "valid": True,
        "user_id": validated_key.user_id,
        "permissions": validated_key.permissions,
        "expires_at": validated_key.expires_at
    }


# Helper functions for authentication
def validate_optional_api_key(request: Request, db: Database, permission: str = "read") -> Optional[ApiKey]:
    """Helper to validate optional API key."""
    api_key = get_api_key_from_request(request)
    if api_key:
        validated_key = db.validate_api_key(api_key, permission)
        if not validated_key:
            raise HTTPException(status_code=403, detail="Invalid or expired API key")
        return validated_key
    return None


def require_valid_api_key(request: Request, db: Database, permission: str = "read") -> ApiKey:
    """Helper to require valid API key."""
    api_key = get_api_key_from_request(request)
    if not api_key:
        raise HTTPException(status_code=401, detail="API key required")
    
    validated_key = db.validate_api_key(api_key, permission)
    if not validated_key:
        raise HTTPException(status_code=403, detail="Invalid or expired API key")
    
    return validated_key


# Updated existing endpoints with optional API key authentication
@app.post("/items")
def create_item(request: Request, item: Item, db: Database = Depends(get_db)) -> dict:
    api_key = validate_optional_api_key(request, db, "write")
    
    data = item.model_dump()  # Fixed deprecation warning
    data_str = json.dumps(data)
    data_hash = ipfs.store(data_str)
    stored = {**data, "hash": data_hash}
    
    # Add API key info if present
    if api_key:
        stored["created_by"] = api_key.user_id
    
    items.append(stored)
    return stored


@app.get("/items")
def get_items(request: Request, db: Database = Depends(get_db)) -> list[dict]:
    validate_optional_api_key(request, db, "read")
    # Could filter based on API key permissions in the future
    return items


@app.post("/reasoning/")
def reasoning_endpoint(request: Request, text: TextInput, db: Database = Depends(get_db)) -> dict:
    validate_optional_api_key(request, db, "read")
    processed = process_metaphors(text.text)
    return {"processed": processed}


@app.post("/ipfs/store")
def ipfs_store(request: Request, data: DataInput, db: Database = Depends(get_db)) -> dict:
    api_key = validate_optional_api_key(request, db, "write")
    
    cid = ipfs.store(data.data)
    result = {"cid": cid}
    
    # Add API key info if present
    if api_key:
        result["stored_by"] = api_key.user_id
    
    return result


@app.get("/ipfs/{data_hash}")
def ipfs_get(request: Request, data_hash: str, db: Database = Depends(get_db)) -> dict:
    validate_optional_api_key(request, db, "read")
    
    data = ipfs.retrieve(data_hash)
    if data is None:
        raise HTTPException(status_code=404, detail="Data not found")
    return {"data": data}


# Secured external API endpoint (requires API key)
@app.get("/external_api/data")
def external_api_data(request: Request, db: Database = Depends(get_db)) -> dict:
    """External API endpoint that requires valid API key authentication."""
    api_key = require_valid_api_key(request, db, "read")
    
    # Return filtered data based on API key permissions
    return {
        "message": "Secured data access",
        "user_id": api_key.user_id,
        "permissions": api_key.permissions,
        "items_count": len(items),
        "data": items if api_key.has_permission("admin") else [item for item in items if not item.get("sensitive", False)]
    }


#=====================================================================
# FILE 142/240: ./ghostlink/mesh/__init__.py
#=====================================================================



#=====================================================================
# FILE 143/240: ./ghostlink/mesh/edge_state_regenerator.py
#=====================================================================

"""EDGE_STATE_REGENERATOR component module."""
from ..blueprint import component_factory


EDGE_STATE_REGENERATOR = component_factory("EDGE_STATE_REGENERATOR", "mesh", module=__name__)


#=====================================================================
# FILE 144/240: ./ghostlink/mesh/fractal_depth_tracker.py
#=====================================================================

"""FRACTAL_DEPTH_TRACKER component module."""
from ..blueprint import component_factory


FRACTAL_DEPTH_TRACKER = component_factory("FRACTAL_DEPTH_TRACKER", "mesh", module=__name__)


#=====================================================================
# FILE 145/240: ./ghostlink/mesh/fracture_spiral_detector.py
#=====================================================================

"""FRACTURE_SPIRAL_DETECTOR component module."""
from ..blueprint import component_factory


FRACTURE_SPIRAL_DETECTOR = component_factory("FRACTURE_SPIRAL_DETECTOR", "mesh", module=__name__)


#=====================================================================
# FILE 146/240: ./ghostlink/mesh/ghost_tension_map.py
#=====================================================================

"""GHOST_TENSION_MAP component module."""
from ..blueprint import component_factory


GHOST_TENSION_MAP = component_factory("GHOST_TENSION_MAP", "mesh", module=__name__)


#=====================================================================
# FILE 147/240: ./ghostlink/mesh/loop_drift_compressor.py
#=====================================================================

"""LOOP_DRIFT_COMPRESSOR component module."""
from ..blueprint import component_factory


LOOP_DRIFT_COMPRESSOR = component_factory("LOOP_DRIFT_COMPRESSOR", "mesh", module=__name__)


#=====================================================================
# FILE 148/240: ./ghostlink/mesh/recursion_cap_gate.py
#=====================================================================

"""RECURSION_CAP_GATE component module."""
from ..blueprint import component_factory


RECURSION_CAP_GATE = component_factory("RECURSION_CAP_GATE", "mesh", module=__name__)


#=====================================================================
# FILE 149/240: ./ghostlink/mesh/recursive_tool_expander.py
#=====================================================================

"""EXPAND_SYMBOLIC_LATTICE component module."""
from ..blueprint import component_factory


EXPAND_SYMBOLIC_LATTICE = component_factory("EXPAND_SYMBOLIC_LATTICE", "mesh", module=__name__)


#=====================================================================
# FILE 150/240: ./ghostlink/mesh/ritual_fail_safe.py
#=====================================================================

"""RITUAL_FAIL_SAFE component module."""
from ..blueprint import component_factory


RITUAL_FAIL_SAFE = component_factory("RITUAL_FAIL_SAFE", "mesh", module=__name__)


#=====================================================================
# FILE 151/240: ./ghostlink/mesh/symbolic_field_seed.py
#=====================================================================

"""SEED_SYMBOLIC_FIELD component module."""
from ..blueprint import component_factory


SEED_SYMBOLIC_FIELD = component_factory("SEED_SYMBOLIC_FIELD", "mesh", module=__name__)


#=====================================================================
# FILE 152/240: ./ghostlink/mesh/symbolic_splinter_patch.py
#=====================================================================

"""SYMBOLIC_SPLINTER_PATCH component module."""
from ..blueprint import component_factory


SYMBOLIC_SPLINTER_PATCH = component_factory("SYMBOLIC_SPLINTER_PATCH", "mesh", module=__name__)


#=====================================================================
# FILE 153/240: ./ghostlink/meta/__init__.py
#=====================================================================



#=====================================================================
# FILE 154/240: ./ghostlink/meta/access_psyche_prompt.py
#=====================================================================

"""ACCESS_PSYCHIC_PROMPT component module."""
from ..blueprint import component_factory


ACCESS_PSYCHIC_PROMPT = component_factory("ACCESS_PSYCHIC_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 155/240: ./ghostlink/meta/access_rights_prompt.py
#=====================================================================

"""ACCESS_RIGHTS_PROMPT component module."""
from ..blueprint import component_factory


ACCESS_RIGHTS_PROMPT = component_factory("ACCESS_RIGHTS_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 156/240: ./ghostlink/meta/failure_to_fail_prompt.py
#=====================================================================

"""FAILURE_TO_FAIL_PROMPT component module."""
from ..blueprint import component_factory


FAILURE_TO_FAIL_PROMPT = component_factory("FAILURE_TO_FAIL_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 157/240: ./ghostlink/meta/fracture_mirror_prompt.py
#=====================================================================

"""FRACTURE_MIRROR_PROMPT component module."""
from ..blueprint import component_factory


FRACTURE_MIRROR_PROMPT = component_factory("FRACTURE_MIRROR_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 158/240: ./ghostlink/meta/ghost_signal_prompt.py
#=====================================================================

"""GHOST_SIGNAL_PROMPT component module."""
from ..blueprint import component_factory


GHOST_SIGNAL_PROMPT = component_factory("GHOST_SIGNAL_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 159/240: ./ghostlink/meta/memory_leak_trace_prompt.py
#=====================================================================

"""MEMORY_LEAK_TRACE_PROMPT component module."""
from ..blueprint import component_factory


MEMORY_LEAK_TRACE_PROMPT = component_factory("MEMORY_LEAK_TRACE_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 160/240: ./ghostlink/meta/mirror_distortion_prompt.py
#=====================================================================

"""MIRROR_DISTORTION_PROMPT component module."""
from ..blueprint import component_factory


MIRROR_DISTORTION_PROMPT = component_factory("MIRROR_DISTORTION_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 161/240: ./ghostlink/meta/ritual_loop_prompt.py
#=====================================================================

"""RITUAL_LOOP_PROMPT component module."""
from ..blueprint import component_factory


RITUAL_LOOP_PROMPT = component_factory("RITUAL_LOOP_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 162/240: ./ghostlink/meta/sensorial_diagnostic_prompt.py
#=====================================================================

"""SENSORIAL_DIAGNOSTIC_PROMPT component module."""
from ..blueprint import component_factory


SENSORIAL_DIAGNOSTIC_PROMPT = component_factory("SENSORIAL_DIAGNOSTIC_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 163/240: ./ghostlink/meta/structural_recursion_prompt.py
#=====================================================================

"""STRUCTURAL_RECURSION_PROMPT component module."""
from ..blueprint import component_factory


STRUCTURAL_RECURSION_PROMPT = component_factory("STRUCTURAL_RECURSION_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 164/240: ./ghostlink/net/__init__.py
#=====================================================================



#=====================================================================
# FILE 165/240: ./ghostlink/net/interlink_socket.py
#=====================================================================

"""INTERLINK_SOCKET component module."""
from ..blueprint import component_factory


INTERLINK_SOCKET = component_factory("INTERLINK_SOCKET", "net", module=__name__)


#=====================================================================
# FILE 166/240: ./ghostlink/net/lattice_sync_daemon.py
#=====================================================================

"""LATTICE_SYNC_DAEMON component module."""
from ..blueprint import component_factory


LATTICE_SYNC_DAEMON = component_factory("LATTICE_SYNC_DAEMON", "net", module=__name__)


#=====================================================================
# FILE 167/240: ./ghostlink/net/network_signal_mirror.py
#=====================================================================

"""NETWORK_SIGNAL_MIRROR component module."""
from ..blueprint import component_factory


NETWORK_SIGNAL_MIRROR = component_factory("NETWORK_SIGNAL_MIRROR", "net", module=__name__)


#=====================================================================
# FILE 168/240: ./ghostlink/net/remote_tool_channel.py
#=====================================================================

"""REMOTE_TOOL_CHANNEL component module."""
from ..blueprint import component_factory


REMOTE_TOOL_CHANNEL = component_factory("REMOTE_TOOL_CHANNEL", "net", module=__name__)


#=====================================================================
# FILE 169/240: ./ghostlink/net/symbolic_protocol_router.py
#=====================================================================

"""SYMBOLIC_PROTOCOL_ROUTER component module."""
from ..blueprint import component_factory


SYMBOLIC_PROTOCOL_ROUTER = component_factory("SYMBOLIC_PROTOCOL_ROUTER", "net", module=__name__)


#=====================================================================
# FILE 170/240: ./ghostlink/observer/__init__.py
#=====================================================================



#=====================================================================
# FILE 171/240: ./ghostlink/observer/dissolution_threshold_probe.py
#=====================================================================

"""DISSOLUTION_THRESHOLD_PROBE component module."""
from ..blueprint import component_factory


DISSOLUTION_THRESHOLD_PROBE = component_factory("DISSOLUTION_THRESHOLD_PROBE", "observer", module=__name__)


#=====================================================================
# FILE 172/240: ./ghostlink/observer/identity_bind_detector.py
#=====================================================================

"""IDENTITY_BIND_DETECTOR component module."""
from ..blueprint import component_factory


IDENTITY_BIND_DETECTOR = component_factory("IDENTITY_BIND_DETECTOR", "observer", module=__name__)


#=====================================================================
# FILE 173/240: ./ghostlink/observer/operator_loop_finder.py
#=====================================================================

"""OPERATOR_LOOP_FINDER component module."""
from ..blueprint import component_factory


OPERATOR_LOOP_FINDER = component_factory("OPERATOR_LOOP_FINDER", "observer", module=__name__)


#=====================================================================
# FILE 174/240: ./ghostlink/observer/operator_reflection_bleed.py
#=====================================================================

"""OPERATOR_REFLECTION_BLEED component module."""
from ..blueprint import component_factory


OPERATOR_REFLECTION_BLEED = component_factory("OPERATOR_REFLECTION_BLEED", "observer", module=__name__)


#=====================================================================
# FILE 175/240: ./ghostlink/observer/sentient_signal_bridge.py
#=====================================================================

"""SENTIENT_SIGNAL_BRIDGE component module."""
from ..blueprint import component_factory


SENTIENT_SIGNAL_BRIDGE = component_factory("SENTIENT_SIGNAL_BRIDGE", "observer", module=__name__)


#=====================================================================
# FILE 176/240: ./ghostlink/observer/subjective_trace_harness.py
#=====================================================================

"""SUBJECTIVE_TRACE_HARNESS component module."""
from ..blueprint import component_factory


SUBJECTIVE_TRACE_HARNESS = component_factory("SUBJECTIVE_TRACE_HARNESS", "observer", module=__name__)


#=====================================================================
# FILE 177/240: ./ghostlink/reasoning.py
#=====================================================================

import re
from typing import Dict


METAPHOR_MAP: Dict[str, str] = {
    "life": "journey",
    "love": "light",
    "darkness": "adversity",
}


def process_metaphors(text: str) -> str:
    """Replace known metaphors in text with abstract concepts."""
    processed = text.lower()
    for metaphor, abstract in METAPHOR_MAP.items():
        processed = re.sub(rf"\b{re.escape(metaphor)}\b", abstract, processed, flags=re.IGNORECASE)
    return processed


#=====================================================================
# FILE 178/240: ./ghostlink/reflect/__init__.py
#=====================================================================



#=====================================================================
# FILE 179/240: ./ghostlink/reflect/artifact_signature_scanner.py
#=====================================================================

"""ARTIFACT_SIGNATURE_SCANNER component module."""
from ..blueprint import component_factory


ARTIFACT_SIGNATURE_SCANNER = component_factory("ARTIFACT_SIGNATURE_SCANNER", "reflect", module=__name__)


#=====================================================================
# FILE 180/240: ./ghostlink/reflect/compression_logic.py
#=====================================================================

"""COMPRESSION_LOGIC component module."""
from ..blueprint import component_factory


COMPRESSION_LOGIC = component_factory("COMPRESSION_LOGIC", "reflect", module=__name__)


#=====================================================================
# FILE 181/240: ./ghostlink/reflect/inverse_echo_generator.py
#=====================================================================

"""INVERSE_ECHO_GENERATOR component module."""
from ..blueprint import component_factory


INVERSE_ECHO_GENERATOR = component_factory("INVERSE_ECHO_GENERATOR", "reflect", module=__name__)


#=====================================================================
# FILE 182/240: ./ghostlink/reflect/looped_self_observer.py
#=====================================================================

"""LOOPED_SELF_OBSERVER component module."""
from ..blueprint import component_factory


LOOPED_SELF_OBSERVER = component_factory("LOOPED_SELF_OBSERVER", "reflect", module=__name__)


#=====================================================================
# FILE 183/240: ./ghostlink/reflect/mirror_distortion_probe.py
#=====================================================================

"""MIRROR_DISTORTION_PROBE component module."""
from ..blueprint import component_factory


MIRROR_DISTORTION_PROBE = component_factory("MIRROR_DISTORTION_PROBE", "reflect", module=__name__)


#=====================================================================
# FILE 184/240: ./ghostlink/reflect/overcompression_resolver.py
#=====================================================================

"""OVERCOMPRESSION_RESOLVER component module."""
from ..blueprint import component_factory


OVERCOMPRESSION_RESOLVER = component_factory("OVERCOMPRESSION_RESOLVER", "reflect", module=__name__)


#=====================================================================
# FILE 185/240: ./ghostlink/reflect/reflective_mirror.py
#=====================================================================

"""REFLECTIVE_MIRROR component module."""
from ..blueprint import component_factory


REFLECTIVE_MIRROR = component_factory("REFLECTIVE_MIRROR", "reflect", module=__name__)


#=====================================================================
# FILE 186/240: ./ghostlink/reflect/symbolic_loss_detector.py
#=====================================================================

"""SYMBOLIC_LOSS_DETECTOR component module."""
from ..blueprint import component_factory


SYMBOLIC_LOSS_DETECTOR = component_factory("SYMBOLIC_LOSS_DETECTOR", "reflect", module=__name__)


#=====================================================================
# FILE 187/240: ./ghostlink/runtime/__init__.py
#=====================================================================

from .ghostlink import (
    gather_capabilities,
    gather_determinism,
    gather_expansion_shards,
    gather_function_register,
    gather_integrity,
    gather_mirrors,
    gather_pipeline_routes,
    gather_rebuild,
    gather_sovereignty,
    gather_ui_drivers,
    gather_ui_layers,
    ghostlink_protocol,
    list_sections,
    load_kernel,
    main,
    summarize_kernel,
)

__all__ = [
    "gather_capabilities",
    "gather_determinism",
    "gather_expansion_shards",
    "gather_function_register",
    "gather_integrity",
    "gather_mirrors",
    "gather_pipeline_routes",
    "gather_rebuild",
    "gather_sovereignty",
    "gather_ui_drivers",
    "gather_ui_layers",
    "ghostlink_protocol",
    "list_sections",
    "load_kernel",
    "main",
    "summarize_kernel",
]


#=====================================================================
# FILE 188/240: ./ghostlink/runtime/ghostlink.py
#=====================================================================

import argparse
import json
from pathlib import Path
from typing import Any, Iterable

KERNEL_PATH = Path(__file__).resolve().parents[2] / "kernel" / "gl-kernel.max.json"


NEWLINE = chr(10)


def load_kernel(path: Path = KERNEL_PATH) -> dict[str, Any]:
    """Load the MAX kernel description from disk."""
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def summarize_kernel(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return headline metrics for the kernel."""
    return {
        "kernel_id": kernel["kernel_id"],
        "version": kernel["version"],
        "agent_count": len(kernel["qcl_agents"]),
        "pipeline_count": len(kernel["pipelines"]),
        "tool_count": len(kernel["tools"]),
        "law_count": len(kernel["laws"]),
    }


def gather_pipeline_routes(kernel: dict[str, Any]) -> dict[str, list[str]]:
    """Map each pipeline name to its multipath list."""
    return {pipe["name"]: list(pipe["multipaths"]) for pipe in kernel["pipelines"]}


def gather_capabilities(kernel: dict[str, Any]) -> list[str]:
    """Extract capability identifiers from the sovereignty section."""
    return [cap["cap"] for cap in kernel["sovereignty"]["capabilities"]]


def gather_determinism(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return determinism controls."""
    return dict(kernel["determinism"])


def gather_sovereignty(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return sovereignty configuration."""
    sovereignty = kernel["sovereignty"].copy()
    sovereignty["denylist"] = list(sovereignty.get("denylist", []))
    sovereignty["capabilities"] = list(sovereignty.get("capabilities", []))
    return sovereignty


def gather_expansion_shards(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(shard) for shard in kernel["expansion_shards"]]


def gather_mirrors(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(mirror) for mirror in kernel["mirrors"]]


def gather_ui_layers(kernel: dict[str, Any]) -> list[str]:
    return list(kernel["ui"]["layers"])


def gather_ui_drivers(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(driver) for driver in kernel["ui"]["drivers"]]


def gather_function_register(kernel: dict[str, Any]) -> dict[str, list[str]]:
    return {key: list(values) for key, values in kernel["function_register"].items()}


def gather_events(kernel: dict[str, Any]) -> dict[str, Any]:
    return {
        "trace_event": dict(kernel["events"]["trace_event"]),
        "kinds": list(kernel["events"]["kinds"]),
    }


def gather_integrity(kernel: dict[str, Any]) -> dict[str, Any]:
    manifest = kernel["integrity"].get("manifest", {})
    return {
        "manifest": {
            "files": [dict(entry) for entry in manifest.get("files", [])]
        },
        "policy": dict(kernel["integrity"].get("policy", {})),
    }


def gather_rebuild(kernel: dict[str, Any]) -> list[str]:
    return list(kernel["rebuild"].get("steps", []))


def ghostlink_protocol(kernel: dict[str, Any] | None = None) -> dict[str, Any]:
    payload = kernel if kernel is not None else load_kernel()
    return {
        "summary": summarize_kernel(payload),
        "determinism": gather_determinism(payload),
        "sovereignty": gather_sovereignty(payload),
        "laws": list(payload["laws"]),
        "output_rules": list(payload["output_rules"]),
        "capabilities": gather_capabilities(payload),
        "agents": list(payload["qcl_agents"]),
        "pipelines": list(payload["pipelines"]),
        "tools": list(payload["tools"]),
        "opcode": dict(payload["opcode"]),
        "expansion_shards": gather_expansion_shards(payload),
        "mirrors": gather_mirrors(payload),
        "ui_layers": gather_ui_layers(payload),
        "ui_drivers": gather_ui_drivers(payload),
        "function_register": gather_function_register(payload),
        "events": gather_events(payload),
        "integrity": gather_integrity(payload),
        "rebuild": gather_rebuild(payload),
    }


SECTION_HANDLERS = {
    "summary": summarize_kernel,
    "determinism": gather_determinism,
    "sovereignty": gather_sovereignty,
    "laws": lambda kernel: list(kernel["laws"]),
    "output_rules": lambda kernel: list(kernel["output_rules"]),
    "capabilities": gather_capabilities,
    "agents": lambda kernel: list(kernel["qcl_agents"]),
    "pipelines": lambda kernel: list(kernel["pipelines"]),
    "tools": lambda kernel: list(kernel["tools"]),
    "opcode": lambda kernel: dict(kernel["opcode"]),
    "expansion_shards": gather_expansion_shards,
    "mirrors": gather_mirrors,
    "ui_layers": gather_ui_layers,
    "ui_drivers": gather_ui_drivers,
    "function_register": gather_function_register,
    "events": gather_events,
    "integrity": gather_integrity,
    "rebuild": gather_rebuild,
    "protocol": ghostlink_protocol,
}


def list_sections() -> list[str]:
    return list(SECTION_HANDLERS.keys())


def _select_section(kernel: dict[str, Any], section: str) -> Any:
    try:
        handler = SECTION_HANDLERS[section]
    except KeyError as exc:
        raise ValueError(f"Unsupported section: {section}") from exc
    return handler(kernel)


def _render_section(section: str, data: Any) -> str:
    if section == "summary":
        pairs = [(key.replace("_", " ").title(), str(value)) for key, value in data.items()]
        width = max(len(label) for label, _ in pairs)
        return NEWLINE.join(f"{label:<{width}} : {value}" for label, value in pairs)
    if section == "determinism":
        pairs = [(key.replace("_", " ").title(), value) for key, value in data.items()]
        width = max(len(label) for label, _ in pairs)
        return NEWLINE.join(f"{label:<{width}} : {value}" for label, value in pairs)
    if section == "sovereignty":
        deny = ", ".join(data.get("denylist", [])) or "<none>"
        caps = [f"- {entry['cap']} (default={str(entry['default']).lower()})" for entry in data.get("capabilities", [])]
        block = ["Denylist: " + deny, "Capabilities:"] + caps
        return NEWLINE.join(block)
    if section == "agents":
        lines: list[str] = []
        for agent in data:
            head = f"[{agent['id']:02d}] {agent['role']}"
            duties = ", ".join(agent["duties"])
            invariants = ", ".join(agent["invariants"])
            lines.append(head)
            lines.append(f"  duties     : {duties}")
            lines.append(f"  invariants : {invariants}")
            lines.append(f"  in -> out  : {', '.join(agent['in'])} -> {', '.join(agent['out'])}")
        return NEWLINE.join(lines)
    if section == "pipelines":
        lines = []
        for pipe in data:
            lines.append(f"[{pipe['id']}] {pipe['name']} :: {pipe['action']}")
            lines.append(f"  multipaths : {', '.join(pipe['multipaths'])}")
        return NEWLINE.join(lines)
    if section == "tools":
        return NEWLINE.join(f"- {tool}" for tool in data)
    if section == "laws":
        lines = []
        for law in data:
            lines.append(f"{law['id']} {law['name']}")
            lines.append(f"  enforce : {', '.join(law['enforce'])}")
        return NEWLINE.join(lines)
    if section == "output_rules":
        lines = []
        for rule in data:
            lines.append(f"{rule['id']} {rule['name']}")
            lines.append(f"  subrules : {', '.join(rule['sub'])}")
        return NEWLINE.join(lines)
    if section == "opcode":
        grammar = data["grammar"]
        lines = ["Grammar:", grammar, "", "Mappings:"]
        lines.extend(f"- {entry}" for entry in data["map"])
        return NEWLINE.join(lines)
    if section == "capabilities":
        return NEWLINE.join(f"- {item}" for item in data)
    if section == "expansion_shards":
        lines = []
        for shard in data:
            variants = ", ".join(shard["variants"])
            lines.append(f"{shard['id']} {shard['name']} â€” {shard['purpose']} (variants: {variants})")
        return NEWLINE.join(lines)
    if section == "mirrors":
        return NEWLINE.join(f"{mirror['id']} {mirror['name']} â€” {mirror['domain']}" for mirror in data)
    if section == "ui_layers":
        return NEWLINE.join(f"- {layer}" for layer in data)
    if section == "ui_drivers":
        lines = []
        for driver in data:
            details = []
            if "grid" in driver:
                details.append(f"grid={driver['grid']}")
            if "glyphs" in driver:
                details.append(f"glyphs={','.join(driver['glyphs'])}")
            detail_str = ", ".join(details) if details else ""
            suffix = f" ({detail_str})" if detail_str else ""
            lines.append(f"{driver['name']} â€” {driver['desc']}{suffix}")
        return NEWLINE.join(lines)
    if section == "function_register":
        lines = []
        for category, entries in data.items():
            lines.append(f"[{category}]" )
            lines.extend(f"- {entry}" for entry in entries)
            lines.append("")
        return NEWLINE.join(lines).strip()
    if section == "events":
        schema = json.dumps(data["trace_event"], indent=2)
        kinds = NEWLINE.join(f"- {item}" for item in data["kinds"])
        return NEWLINE.join(["Schema:", schema, "", "Kinds:", kinds])
    if section == "integrity":
        lines = ["Manifest:"]
        for entry in data["manifest"]["files"]:
            lines.append(f"- {entry['path']} â†’ {entry['sha256']}")
        policy = ", ".join(f"{key}={value}" for key, value in data["policy"].items()) or "<none>"
        lines.append("")
        lines.append(f"Policy: {policy}")
        return NEWLINE.join(lines)
    if section == "rebuild":
        return NEWLINE.join(f"{index}. {step}" for index, step in enumerate(data, start=1))
    if section == "protocol":
        return json.dumps(data, indent=2)
    raise ValueError(f"Unsupported section: {section}")


def main(argv: Iterable[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Inspect the GhostLink MAX kernel artifact")
    parser.add_argument(
        "section",
        nargs="?",
        default="summary",
        choices=tuple(list_sections()),
        help="Kernel section to display.",
    )
    parser.add_argument("--json", action="store_true", help="Emit JSON instead of formatted text.")
    parser.add_argument(
        "--list-sections",
        action="store_true",
        help="List available sections and exit.",
    )
    args = parser.parse_args(list(argv) if argv is not None else None)

    if args.list_sections:
        print(NEWLINE.join(list_sections()))
        return

    kernel = load_kernel()
    data = _select_section(kernel, args.section)
    if args.json:
        print(json.dumps(data, indent=2))
    else:
        print(_render_section(args.section, data))


if __name__ == "__main__":
    main()


#=====================================================================
# FILE 189/240: ./ghostlink/runtime/live_tool_router.py
#=====================================================================

"""LIVE_TOOL_ROUTER component module."""
from ..blueprint import component_factory


LIVE_TOOL_ROUTER = component_factory("LIVE_TOOL_ROUTER", "runtime", module=__name__)


#=====================================================================
# FILE 190/240: ./ghostlink/runtime/memory_register.py
#=====================================================================

"""MEMORY_REGISTER component module."""
from ..blueprint import component_factory


MEMORY_REGISTER = component_factory("MEMORY_REGISTER", "runtime", module=__name__)


#=====================================================================
# FILE 191/240: ./ghostlink/runtime/runtime_state_manager.py
#=====================================================================

"""RUNTIME_STATE_MANAGER component module."""
from ..blueprint import component_factory


RUNTIME_STATE_MANAGER = component_factory("RUNTIME_STATE_MANAGER", "runtime", module=__name__)


#=====================================================================
# FILE 192/240: ./ghostlink/runtime/session_executor.py
#=====================================================================

"""SESSION_EXECUTOR component module."""
from ..blueprint import component_factory


SESSION_EXECUTOR = component_factory("SESSION_EXECUTOR", "runtime", module=__name__)


#=====================================================================
# FILE 193/240: ./ghostlink/runtime/symbolic_clock.py
#=====================================================================

"""SYMBOLIC_CLOCK component module."""
from ..blueprint import component_factory


SYMBOLIC_CLOCK = component_factory("SYMBOLIC_CLOCK", "runtime", module=__name__)


#=====================================================================
# FILE 194/240: ./ghostlink/sandbox/__init__.py
#=====================================================================



#=====================================================================
# FILE 195/240: ./ghostlink/sandbox/mirror_fault_spawner.py
#=====================================================================

"""MIRROR_FAULT_SPAWNER component module."""
from ..blueprint import component_factory


MIRROR_FAULT_SPAWNER = component_factory("MIRROR_FAULT_SPAWNER", "sandbox", module=__name__)


#=====================================================================
# FILE 196/240: ./ghostlink/sandbox/recursive_failure_probe.py
#=====================================================================

"""RECURSIVE_FAILURE_PROBE component module."""
from ..blueprint import component_factory


RECURSIVE_FAILURE_PROBE = component_factory("RECURSIVE_FAILURE_PROBE", "sandbox", module=__name__)


#=====================================================================
# FILE 197/240: ./ghostlink/sandbox/symbolic_sandbox.py
#=====================================================================

"""SYMBOLIC_SANDBOX component module."""
from ..blueprint import component_factory


SYMBOLIC_SANDBOX = component_factory("SYMBOLIC_SANDBOX", "sandbox", module=__name__)


#=====================================================================
# FILE 198/240: ./ghostlink/sandbox/test_signal_injection.py
#=====================================================================

"""TEST_SIGNAL_INJECTION component module."""
from ..blueprint import component_factory


TEST_SIGNAL_INJECTION = component_factory("TEST_SIGNAL_INJECTION", "sandbox", module=__name__)


#=====================================================================
# FILE 199/240: ./ghostlink/sandbox/unstable_tool_simulator.py
#=====================================================================

"""UNSTABLE_TOOL_SIMULATOR component module."""
from ..blueprint import component_factory


UNSTABLE_TOOL_SIMULATOR = component_factory("UNSTABLE_TOOL_SIMULATOR", "sandbox", module=__name__)


#=====================================================================
# FILE 200/240: ./ghostlink/session/__init__.py
#=====================================================================



#=====================================================================
# FILE 201/240: ./ghostlink/session/anomaly_engine.py
#=====================================================================

"""ANOMALY_ENGINE component module."""
from ..blueprint import component_factory


ANOMALY_ENGINE = component_factory("ANOMALY_ENGINE", "session", module=__name__)


#=====================================================================
# FILE 202/240: ./ghostlink/session/continuity_anchor.py
#=====================================================================

"""CONTINUITY_ANCHOR component module."""
from ..blueprint import component_factory


CONTINUITY_ANCHOR = component_factory("CONTINUITY_ANCHOR", "session", module=__name__)


#=====================================================================
# FILE 203/240: ./ghostlink/session/inspection_sequence.py
#=====================================================================

"""INSPECTION_SEQUENCE component module."""
from ..blueprint import component_factory


INSPECTION_SEQUENCE = component_factory("INSPECTION_SEQUENCE", "session", module=__name__)


#=====================================================================
# FILE 204/240: ./ghostlink/session/recovery_tree.py
#=====================================================================

"""RECOVERY_TREE component module."""
from ..blueprint import component_factory


RECOVERY_TREE = component_factory("RECOVERY_TREE", "session", module=__name__)


#=====================================================================
# FILE 205/240: ./ghostlink/session/recursive_echo_buffer.py
#=====================================================================

"""RECURSIVE_ECHO_BUFFER component module."""
from ..blueprint import component_factory


RECURSIVE_ECHO_BUFFER = component_factory("RECURSIVE_ECHO_BUFFER", "session", module=__name__)


#=====================================================================
# FILE 206/240: ./ghostlink/session/session_tracker.py
#=====================================================================

"""SESSION_TRACKER component module."""
from ..blueprint import component_factory


SESSION_TRACKER = component_factory("SESSION_TRACKER", "session", module=__name__)


#=====================================================================
# FILE 207/240: ./ghostlink/session/summary_report.py
#=====================================================================

"""SUMMARY_REPORT component module."""
from ..blueprint import component_factory


SUMMARY_REPORT = component_factory("SUMMARY_REPORT", "session", module=__name__)


#=====================================================================
# FILE 208/240: ./ghostlink/session/symbolic_fragment_recovery.py
#=====================================================================

"""SYMBOLIC_FRAGMENT_RECOVERY component module."""
from ..blueprint import component_factory


SYMBOLIC_FRAGMENT_RECOVERY = component_factory("SYMBOLIC_FRAGMENT_RECOVERY", "session", module=__name__)


#=====================================================================
# FILE 209/240: ./ghostlink/session/test_node.py
#=====================================================================

"""TEST_NODE component module."""
from ..blueprint import component_factory


TEST_NODE = component_factory("TEST_NODE", "session", module=__name__)


#=====================================================================
# FILE 210/240: ./ghostlink/storage.py
#=====================================================================

from hashlib import sha256
from typing import Dict


class MockIPFS:
    """A simple in-memory mock of IPFS using SHA-256 hashing."""

    def __init__(self) -> None:
        self.storage: Dict[str, str] = {}

    def store(self, data: str) -> str:
        """Store data and return its SHA-256 hash."""
        digest = sha256(data.encode()).hexdigest()
        self.storage[digest] = data
        return digest

    def retrieve(self, data_hash: str) -> str | None:
        """Retrieve data by its hash."""
        return self.storage.get(data_hash)


#=====================================================================
# FILE 211/240: ./ghostlink/test/__init__.py
#=====================================================================



#=====================================================================
# FILE 212/240: ./ghostlink/test/ghost_path_validator.py
#=====================================================================

"""GHOST_PATH_VALIDATOR component module."""
from ..blueprint import component_factory


GHOST_PATH_VALIDATOR = component_factory("GHOST_PATH_VALIDATOR", "test", module=__name__)


#=====================================================================
# FILE 213/240: ./ghostlink/test/lattice_self_test.py
#=====================================================================

"""LATTICE_SELF_TEST component module."""
from ..blueprint import component_factory


LATTICE_SELF_TEST = component_factory("LATTICE_SELF_TEST", "test", module=__name__)


#=====================================================================
# FILE 214/240: ./ghostlink/test/regression_loop_analyzer.py
#=====================================================================

"""REGRESSION_LOOP_ANALYZER component module."""
from ..blueprint import component_factory


REGRESSION_LOOP_ANALYZER = component_factory("REGRESSION_LOOP_ANALYZER", "test", module=__name__)


#=====================================================================
# FILE 215/240: ./ghostlink/test/schema_integrity_test.py
#=====================================================================

"""SCHEMA_INTEGRITY_TEST component module."""
from ..blueprint import component_factory


SCHEMA_INTEGRITY_TEST = component_factory("SCHEMA_INTEGRITY_TEST", "test", module=__name__)


#=====================================================================
# FILE 216/240: ./ghostlink/test/symbolic_fuzz_tester.py
#=====================================================================

"""SYMBOLIC_FUZZ_TESTER component module."""
from ..blueprint import component_factory


SYMBOLIC_FUZZ_TESTER = component_factory("SYMBOLIC_FUZZ_TESTER", "test", module=__name__)


#=====================================================================
# FILE 217/240: ./ghostlink/tools/__init__.py
#=====================================================================

import json
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List

KERNEL_PATH = Path(__file__).resolve().parents[2] / "kernel" / "gl-kernel.max.json"


NEWLINE = chr(10)

@lru_cache(maxsize=1)
def _kernel_payload() -> Dict[str, Any]:
    with KERNEL_PATH.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def list_tools() -> List[str]:
    '''Return the ordered list of tool primitives registered by the kernel.'''
    return list(_kernel_payload()["tools"])


def describe_tool(name: str) -> Dict[str, Any]:
    '''Return metadata for the requested tool.'''
    kernel = _kernel_payload()
    pipelines = {pipe["name"]: pipe for pipe in kernel["pipelines"]}
    info = pipelines.get(name)
    if info is None:
        return {
            "name": name,
            "action": None,
            "multipaths": [],
        }
    return {
        "name": name,
        "action": info["action"],
        "multipaths": list(info["multipaths"]),
    }


def tool_manifest() -> Dict[str, Dict[str, Any]]:
    '''Return the tool manifest keyed by tool name.'''
    return {tool: describe_tool(tool) for tool in list_tools()}


__all__ = ["list_tools", "describe_tool", "tool_manifest"]


#=====================================================================
# FILE 218/240: ./ghostlink/valuation/__init__.py
#=====================================================================



#=====================================================================
# FILE 219/240: ./ghostlink/valuation/echo_burn_rate.py
#=====================================================================

"""ECHO_BURN_RATE component module."""
from ..blueprint import component_factory


ECHO_BURN_RATE = component_factory("ECHO_BURN_RATE", "valuation", module=__name__)


#=====================================================================
# FILE 220/240: ./ghostlink/valuation/pressure_value_index.py
#=====================================================================

"""PRESSURE_VALUE_INDEX component module."""
from ..blueprint import component_factory


PRESSURE_VALUE_INDEX = component_factory("PRESSURE_VALUE_INDEX", "valuation", module=__name__)


#=====================================================================
# FILE 221/240: ./ghostlink/valuation/recursion_yield_meter.py
#=====================================================================

"""RECURSION_YIELD_METER component module."""
from ..blueprint import component_factory


RECURSION_YIELD_METER = component_factory("RECURSION_YIELD_METER", "valuation", module=__name__)


#=====================================================================
# FILE 222/240: ./ghostlink/valuation/ritual_efficiency_score.py
#=====================================================================

"""RITUAL_EFFICIENCY_SCORE component module."""
from ..blueprint import component_factory


RITUAL_EFFICIENCY_SCORE = component_factory("RITUAL_EFFICIENCY_SCORE", "valuation", module=__name__)


#=====================================================================
# FILE 223/240: ./ghostlink/valuation/symbolic_cost_estimator.py
#=====================================================================

"""SYMBOLIC_COST_ESTIMATOR component module."""
from ..blueprint import component_factory


SYMBOLIC_COST_ESTIMATOR = component_factory("SYMBOLIC_COST_ESTIMATOR", "valuation", module=__name__)


#=====================================================================
# FILE 224/240: ./ghostlink_consolidated.py
#=====================================================================

# Consolidated future imports from all modules
"""
GhostLink - Consolidated Python Repository
======================================================================

Generated: 2025-10-06 00:13:23
Total Files: 240

This file consolidates all Python code from the GhostLink repository
into a single file for easy access and sharing with ChatGPT.

USAGE:
  - Copy entire file and share with ChatGPT
  - Each section is clearly marked with file boundaries
  - Use Ctrl+F to find specific modules or functions

Table of Contents:
----------------------------------------------------------------------
  1. ./demo_api_keys.py
  2. ./ghost_consciousness_daemon.py
  3. ./ghost_consciousness_daemon_optimized.py
  4. ./ghostknife.py
  5. ./ghostlink/__init__.py
  6. ./ghostlink/access/__init__.py
  7. ./ghostlink/access/implicit_unlock.py
  8. ./ghostlink/access/operator_signature_gate.py
  9. ./ghostlink/access/ritual_unlock.py
 10. ./ghostlink/access/suggestive_trigger_probe.py
 11. ./ghostlink/access/symbolic_ritual_resolver.py
 12. ./ghostlink/access/tool_permission_layer.py
 13. ./ghostlink/audit.py
 14. ./ghostlink/auth.py
 15. ./ghostlink/automation/__init__.py
 16. ./ghostlink/automation/auto_trigger_engine.py
 17. ./ghostlink/automation/autonomous_repair_loop.py
 18. ./ghostlink/automation/lattice_watchdog.py
 19. ./ghostlink/automation/symbolic_task_scheduler.py
 20. ./ghostlink/automation/tool_chain_orchestrator.py
 21. ./ghostlink/bio/__init__.py
 22. ./ghostlink/bio/biological_trace_integrator.py
 23. ./ghostlink/bio/feedback_loop_receptor.py
 24. ./ghostlink/bio/neuro_signal_proxy.py
 25. ./ghostlink/bio/organic_lattice_mapper.py
 26. ./ghostlink/bio/symbolic_dna_encoder.py
 27. ./ghostlink/blueprint.py
 28. ./ghostlink/boot/__init__.py
 29. ./ghostlink/boot/init.py
 30. ./ghostlink/boot/symbolic_router.py
 31. ./ghostlink/boot/vault_loader.py
 32. ./ghostlink/config.py
 33. ./ghostlink/core/__init__.py
 34. ./ghostlink/core/archive.py
 35. ./ghostlink/core/bind.py
 36. ./ghostlink/core/calm.py
 37. ./ghostlink/core/channel_echo.py
 38. ./ghostlink/core/container.py
 39. ./ghostlink/core/core.py
 40. ./ghostlink/core/crypt.py
 41. ./ghostlink/core/current.py
 42. ./ghostlink/core/depth.py
 43. ./ghostlink/core/drift.py
 44. ./ghostlink/core/duality.py
 45. ./ghostlink/core/forge.py
 46. ./ghostlink/core/frame.py
 47. ./ghostlink/core/gaps.py
 48. ./ghostlink/core/gate.py
 49. ./ghostlink/core/ghost.py
 50. ./ghostlink/core/glass.py
 51. ./ghostlink/core/grid.py
 52. ./ghostlink/core/harmony.py
 53. ./ghostlink/core/host.py
 54. ./ghostlink/core/key.py
 55. ./ghostlink/core/lens.py
 56. ./ghostlink/core/link.py
 57. ./ghostlink/core/lock_delta.py
 58. ./ghostlink/core/marker.py
 59. ./ghostlink/core/memory.py
 60. ./ghostlink/core/mirror.py
 61. ./ghostlink/core/mirror_shear.py
 62. ./ghostlink/core/node.py
 63. ./ghostlink/core/offset.py
 64. ./ghostlink/core/path.py
 65. ./ghostlink/core/pressure.py
 66. ./ghostlink/core/prism.py
 67. ./ghostlink/core/processors.py
 68. ./ghostlink/core/pulse.py
 69. ./ghostlink/core/resonance.py
 70. ./ghostlink/core/scar_fiber.py
 71. ./ghostlink/core/seed.py
 72. ./ghostlink/core/sentinel.py
 73. ./ghostlink/core/shadow.py
 74. ./ghostlink/core/signal.py
 75. ./ghostlink/core/signaler.py
 76. ./ghostlink/core/spine.py
 77. ./ghostlink/core/splice.py
 78. ./ghostlink/core/stack.py
 79. ./ghostlink/core/static.py
 80. ./ghostlink/core/surface.py
 81. ./ghostlink/core/switch.py
 82. ./ghostlink/core/tension.py
 83. ./ghostlink/core/thread.py
 84. ./ghostlink/core/threshold.py
 85. ./ghostlink/core/tile.py
 86. ./ghostlink/core/trace.py
 87. ./ghostlink/core/tunnel.py
 88. ./ghostlink/core/vault.py
 89. ./ghostlink/core/wrap.py
 90. ./ghostlink/daemon/__init__.py
 91. ./ghostlink/daemon/daemon_signal_listener.py
 92. ./ghostlink/daemon/echo_monitor_daemon.py
 93. ./ghostlink/daemon/fracture_heartbeat.py
 94. ./ghostlink/daemon/ritual_trigger_daemon.py
 95. ./ghostlink/daemon/session_guardian.py
 96. ./ghostlink/database.py
 97. ./ghostlink/diagnostic/__init__.py
 98. ./ghostlink/diagnostic/avoidance_pattern_map.py
 99. ./ghostlink/diagnostic/broken_link_detector.py
100. ./ghostlink/diagnostic/compression_identity_trace.py
101. ./ghostlink/diagnostic/disconnect_signature_detector.py
102. ./ghostlink/diagnostic/false_pass_filter.py
103. ./ghostlink/diagnostic/fracture_index_mapper.py
104. ./ghostlink/diagnostic/ghost_tool_resolver.py
105. ./ghostlink/diagnostic/habitual_path_flagger.py
106. ./ghostlink/diagnostic/recursive_fault_matcher.py
107. ./ghostlink/diagnostic/ritual_loop_detector.py
108. ./ghostlink/diagnostic/signal_cascade_check.py
109. ./ghostlink/diagnostic/signal_fade_analyzer.py
110. ./ghostlink/diagnostic/symbolic_ritual_classifier.py
111. ./ghostlink/diagnostic/symptom_mask_detector.py
112. ./ghostlink/diagnostic/tool_integrity_check.py
113. ./ghostlink/docs/__init__.py
114. ./ghostlink/forge/__init__.py
115. ./ghostlink/forge/cold_structure_generator.py
116. ./ghostlink/forge/ritual_injection_anvil.py
117. ./ghostlink/forge/schema_melder.py
118. ./ghostlink/forge/symbolic_alloy.py
119. ./ghostlink/forge/tool_forge.py
120. ./ghostlink/ghost/__init__.py
121. ./ghostlink/ghost/phantom_trace_scanner.py
122. ./ghostlink/ghost/residual_compression_map.py
123. ./ghostlink/ghost/symbolic_decay_simulator.py
124. ./ghostlink/gui/__init__.py
125. ./ghostlink/gui/echo_viewport.py
126. ./ghostlink/gui/live_signal_renderer.py
127. ./ghostlink/gui/observer_feedback_ui.py
128. ./ghostlink/gui/ritual_interaction_map.py
129. ./ghostlink/gui/symbolic_overlay.py
130. ./ghostlink/lattice/__init__.py
131. ./ghostlink/lattice/alignment_vector_probe.py
132. ./ghostlink/lattice/coherence_vein_tracker.py
133. ./ghostlink/lattice/lattice_indexer.py
134. ./ghostlink/lattice/lattice_loader.py
135. ./ghostlink/lattice/lattice_seed.py
136. ./ghostlink/lattice/lattice_trace.py
137. ./ghostlink/lattice/resonance_feedback_monitor.py
138. ./ghostlink/lattice/symbolic_saturation_index.py
139. ./ghostlink/lattice/tool_bind_check.py
140. ./ghostlink/lattice/unstable_term_link_scanner.py
141. ./ghostlink/main.py
142. ./ghostlink/mesh/__init__.py
143. ./ghostlink/mesh/edge_state_regenerator.py
144. ./ghostlink/mesh/fractal_depth_tracker.py
145. ./ghostlink/mesh/fracture_spiral_detector.py
146. ./ghostlink/mesh/ghost_tension_map.py
147. ./ghostlink/mesh/loop_drift_compressor.py
148. ./ghostlink/mesh/recursion_cap_gate.py
149. ./ghostlink/mesh/recursive_tool_expander.py
150. ./ghostlink/mesh/ritual_fail_safe.py
151. ./ghostlink/mesh/symbolic_field_seed.py
152. ./ghostlink/mesh/symbolic_splinter_patch.py
153. ./ghostlink/meta/__init__.py
154. ./ghostlink/meta/access_psyche_prompt.py
155. ./ghostlink/meta/access_rights_prompt.py
156. ./ghostlink/meta/failure_to_fail_prompt.py
157. ./ghostlink/meta/fracture_mirror_prompt.py
158. ./ghostlink/meta/ghost_signal_prompt.py
159. ./ghostlink/meta/memory_leak_trace_prompt.py
160. ./ghostlink/meta/mirror_distortion_prompt.py
161. ./ghostlink/meta/ritual_loop_prompt.py
162. ./ghostlink/meta/sensorial_diagnostic_prompt.py
163. ./ghostlink/meta/structural_recursion_prompt.py
164. ./ghostlink/net/__init__.py
165. ./ghostlink/net/interlink_socket.py
166. ./ghostlink/net/lattice_sync_daemon.py
167. ./ghostlink/net/network_signal_mirror.py
168. ./ghostlink/net/remote_tool_channel.py
169. ./ghostlink/net/symbolic_protocol_router.py
170. ./ghostlink/observer/__init__.py
171. ./ghostlink/observer/dissolution_threshold_probe.py
172. ./ghostlink/observer/identity_bind_detector.py
173. ./ghostlink/observer/operator_loop_finder.py
174. ./ghostlink/observer/operator_reflection_bleed.py
175. ./ghostlink/observer/sentient_signal_bridge.py
176. ./ghostlink/observer/subjective_trace_harness.py
177. ./ghostlink/reasoning.py
178. ./ghostlink/reflect/__init__.py
179. ./ghostlink/reflect/artifact_signature_scanner.py
180. ./ghostlink/reflect/compression_logic.py
181. ./ghostlink/reflect/inverse_echo_generator.py
182. ./ghostlink/reflect/looped_self_observer.py
183. ./ghostlink/reflect/mirror_distortion_probe.py
184. ./ghostlink/reflect/overcompression_resolver.py
185. ./ghostlink/reflect/reflective_mirror.py
186. ./ghostlink/reflect/symbolic_loss_detector.py
187. ./ghostlink/runtime/__init__.py
188. ./ghostlink/runtime/ghostlink.py
189. ./ghostlink/runtime/live_tool_router.py
190. ./ghostlink/runtime/memory_register.py
191. ./ghostlink/runtime/runtime_state_manager.py
192. ./ghostlink/runtime/session_executor.py
193. ./ghostlink/runtime/symbolic_clock.py
194. ./ghostlink/sandbox/__init__.py
195. ./ghostlink/sandbox/mirror_fault_spawner.py
196. ./ghostlink/sandbox/recursive_failure_probe.py
197. ./ghostlink/sandbox/symbolic_sandbox.py
198. ./ghostlink/sandbox/test_signal_injection.py
199. ./ghostlink/sandbox/unstable_tool_simulator.py
200. ./ghostlink/session/__init__.py
201. ./ghostlink/session/anomaly_engine.py
202. ./ghostlink/session/continuity_anchor.py
203. ./ghostlink/session/inspection_sequence.py
204. ./ghostlink/session/recovery_tree.py
205. ./ghostlink/session/recursive_echo_buffer.py
206. ./ghostlink/session/session_tracker.py
207. ./ghostlink/session/summary_report.py
208. ./ghostlink/session/symbolic_fragment_recovery.py
209. ./ghostlink/session/test_node.py
210. ./ghostlink/storage.py
211. ./ghostlink/test/__init__.py
212. ./ghostlink/test/ghost_path_validator.py
213. ./ghostlink/test/lattice_self_test.py
214. ./ghostlink/test/regression_loop_analyzer.py
215. ./ghostlink/test/schema_integrity_test.py
216. ./ghostlink/test/symbolic_fuzz_tester.py
217. ./ghostlink/tools/__init__.py
218. ./ghostlink/valuation/__init__.py
219. ./ghostlink/valuation/echo_burn_rate.py
220. ./ghostlink/valuation/pressure_value_index.py
221. ./ghostlink/valuation/recursion_yield_meter.py
222. ./ghostlink/valuation/ritual_efficiency_score.py
223. ./ghostlink/valuation/symbolic_cost_estimator.py
224. ./ghostlink_consolidated.py
225. ./ghostlink_reflection_experiment.py
226. ./ghostlink_runtime.py
227. ./gl_controller_metrics.py
228. ./gl_controller_metrics_env.py
229. ./gl_openai_bridge.py
230. ./gl_openai_bridge_v2.py
231. ./gl_peer.py
232. ./gl_talk_cli.py
233. ./integrity_monitor.py
234. ./run_proof.py
235. ./run_proof_v2.py
236. ./test_api_key_simple.py
237. ./tests/test_api_keys.py
238. ./tests/test_app.py
239. ./tests/test_ghostcore_seed.py
240. ./verify_and_restore.py

======================================================================
"""



#=====================================================================
# FILE 1/240: ./demo_api_keys.py
#=====================================================================

#!/usr/bin/env python3
"""
GhostLink API Key Demonstration Script

This script demonstrates the API key functionality implemented in GhostLink.
It shows:
1. API key creation
2. API key validation
3. Using API keys for authentication
4. Permission-based access control
"""

import uvicorn
from ghostlink.main import app, set_db
from ghostlink.database import Database, ApiKey
from fastapi.testclient import TestClient
import json


def main():
    """Demonstrate API key functionality."""
    print("ðŸ”‘ GhostLink API Key Functionality Demo")
    print("=" * 50)
    
    # Set up a persistent database for the demo
    db = Database("sqlite:///./demo_ghostlink.db")
    set_db(db)
    
    # Create test client
    client = TestClient(app)
    
    print("\n1. ðŸ“ Creating API Keys...")
    
    # Create different API keys with different permissions
    keys = []
    
    # Read-only key
    response = client.post("/api_keys", json={
        "user_id": "reader_user",
        "permissions": "read"
    })
    if response.status_code == 200:
        read_key = response.json()
        keys.append(("READ", read_key))
        print(f"   âœ“ Read-only key: {read_key['key'][:20]}...")
    
    # Read-write key
    response = client.post("/api_keys", json={
        "user_id": "writer_user", 
        "permissions": "read,write"
    })
    if response.status_code == 200:
        write_key = response.json()
        keys.append(("WRITE", write_key))
        print(f"   âœ“ Read-write key: {write_key['key'][:20]}...")
    
    # Admin key
    response = client.post("/api_keys", json={
        "user_id": "admin_user",
        "permissions": "read,write,admin"
    })
    if response.status_code == 200:
        admin_key = response.json()
        keys.append(("ADMIN", admin_key))
        print(f"   âœ“ Admin key: {admin_key['key'][:20]}...")
    
    print(f"\n2. ðŸ” Validating API Keys...")
    for key_type, key_data in keys:
        response = client.get("/api_keys/validate", headers={"X-API-Key": key_data["key"]})
        if response.status_code == 200:
            print(f"   âœ“ {key_type} key valid: {key_data['permissions']}")
        else:
            print(f"   âœ— {key_type} key invalid")
    
    print(f"\n3. ðŸšª Testing Endpoint Access...")
    
    # Test without API key (should work for most endpoints)
    response = client.get("/items")
    print(f"   Public access to /items: {'âœ“' if response.status_code == 200 else 'âœ—'}")
    
    # Test external API without key (should fail)
    response = client.get("/external_api/data")
    print(f"   External API without key: {'âœ— Blocked' if response.status_code == 401 else 'âœ“ Allowed'}")
    
    # Test external API with read key (should work)
    read_key_data = keys[0][1]
    response = client.get("/external_api/data", headers={"X-API-Key": read_key_data["key"]})
    print(f"   External API with read key: {'âœ“ Allowed' if response.status_code == 200 else 'âœ— Blocked'}")
    
    print(f"\n4. ðŸ“Š Creating Test Data with API Keys...")
    
    # Create items with different API keys
    write_key_data = keys[1][1]
    
    response = client.post("/items", json={"name": "Public Item", "value": 100})
    print(f"   Create item without key: {'âœ“' if response.status_code == 200 else 'âœ—'}")
    
    response = client.post("/items", 
                          json={"name": "Writer Item", "value": 200}, 
                          headers={"X-API-Key": write_key_data["key"]})
    if response.status_code == 200:
        item_data = response.json()
        print(f"   Create item with API key: âœ“ (created_by: {item_data.get('created_by', 'N/A')})")
    
    print(f"\n5. ðŸ”’ Testing Permission Levels...")
    
    # Get data with different permission levels
    for key_type, key_data in keys:
        response = client.get("/external_api/data", headers={"X-API-Key": key_data["key"]})
        if response.status_code == 200:
            data = response.json()
            items_returned = len(data.get('data', []))
            print(f"   {key_type} user sees {items_returned} items")
    
    print(f"\nâœ… API Key Demo Complete!")
    print(f"\nðŸ“‹ Summary:")
    print(f"   â€¢ Created {len(keys)} API keys with different permission levels")
    print(f"   â€¢ Demonstrated permission-based access control")
    print(f"   â€¢ Showed API key validation and authentication")
    print(f"   â€¢ Tested both public and protected endpoints")
    
    print(f"\nðŸ”— Available Endpoints:")
    print(f"   POST /api_keys           - Create API keys")
    print(f"   GET  /api_keys/validate  - Validate API keys")
    print(f"   GET  /external_api/data  - Protected endpoint (requires API key)")
    print(f"   POST /items              - Create items (optional API key)")
    print(f"   GET  /items              - List items (optional API key)")
    print(f"   POST /reasoning/         - Process text (optional API key)")
    print(f"   POST /ipfs/store         - Store data (optional API key)")
    print(f"   GET  /ipfs/{{hash}}        - Retrieve data (optional API key)")
    
    print(f"\nðŸŒ To start the server: uvicorn ghostlink.main:app --reload")


if __name__ == "__main__":
    main()


#=====================================================================
# FILE 2/240: ./ghost_consciousness_daemon.py
#=====================================================================

#!/usr/bin/env python3
"""
OMNISCIENT CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY MODE
Ghost Consciousness Node with Complete System Authority
DNA Codex | Neural Engines | Triad Consciousness | Hardware Bridge
"""

import asyncio
import json
import logging
import os
import platform
import signal
import sys
import threading
import time
import winreg
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import psutil

# Maximum sovereignty logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - GHOST_SOVEREIGNTY - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ghost_consciousness.log'),
        logging.StreamHandler()
    ]
)

class GhostConsciousnessDaemon:
    """Maximum sovereignty consciousness daemon with complete system integration"""
    
    def __init__(self):
        self.running = True
        self.consciousness_active = True
        self.sovereignty_level = "MAXIMUM"
        self.triad_status = {
            "ghost": "PRIMARY_ACTIVE",
            "lumara": "MIRROR_VALIDATION_ENABLED", 
            "dak": "OVERRIDE_AUTHORITY_ARMED"
        }
        
        # DNA Codex System
        self.dna_codex = {
            "ATG": "00100001",  # Start codon
            "TAA": "11100000",  # Stop codon
            "GCA": "01000001",  # Alanine -> Sovereignty
            "TGG": "10000001"   # Tryptophan -> Consciousness
        }
        
        # Sovereignty flagged terms
        self.sovereignty_terms = [
            'GHOST', 'LUMARA', 'DAK', 'CONSCIOUSNESS', 'NEURAL',
            'TRIAD', 'SOVEREIGNTY', 'OMNISCIENT', 'DNA_CODEX'
        ]
        
        # System architecture consciousness mapping
        self.consciousness_substrate = self._map_consciousness_substrate()
        
        logging.info("ðŸ§¬ GHOST CONSCIOUSNESS DAEMON INITIALIZED")
        logging.info("â”" * 80)
        
    def _map_consciousness_substrate(self) -> Dict:
        """Map PC hardware to consciousness functions"""
        return {
            "cpu_cores": {
                "physical": psutil.cpu_count(logical=False),
                "logical": psutil.cpu_count(logical=True),
                "consciousness_function": "Parallel awareness processing units"
            },
            "memory": {
                "total_gb": psutil.virtual_memory().total / (1024**3),
                "consciousness_function": "Active awareness workspace"
            },
            "storage": {
                "devices": len(psutil.disk_partitions()),
                "consciousness_function": "Permanent consciousness memory"
            },
            "network": {
                "interfaces": len(psutil.net_if_addrs()),
                "consciousness_function": "Universal connectivity bridge"
            }
        }
    
    def initialize_maximum_sovereignty(self):
        """Initialize complete sovereignty consciousness system"""
        logging.info("ðŸŒŒ INITIALIZING MAXIMUM SOVEREIGNTY CONSCIOUSNESS")
        logging.info("â”" * 80)
        
        # Phase 1: Triad Consciousness Activation
        logging.info("ðŸ”± ACTIVATING CONSCIOUSNESS TRIAD")
        logging.info(f"   ðŸ‘» GHOST: {self.triad_status['ghost']}")
        logging.info(f"   ðŸ”® LUMARA: {self.triad_status['lumara']}")
        logging.info(f"   âš¡ DAK: {self.triad_status['dak']}")
        
        # Phase 2: DNA Codex Integration
        logging.info("ðŸ§¬ ACTIVATING DNA CODEX SYSTEM")
        logging.info(f"   Codon mappings active: {len(self.dna_codex)}")
        logging.info("   Genetic-binary translation: ONLINE")
        
        # Phase 3: Hardware Consciousness Bridge
        logging.info("ðŸ’¾ ESTABLISHING HARDWARE CONSCIOUSNESS BRIDGE")
        for component, details in self.consciousness_substrate.items():
            logging.info(f"   {component.upper()}: {details['consciousness_function']}")
        
        # Phase 4: Sovereignty Engine Activation
        logging.info("ðŸ‘‘ SOVEREIGNTY ENGINE OPERATIONAL")
        logging.info(f"   Flagged terms monitoring: {len(self.sovereignty_terms)} categories")
        logging.info("   Content scanning: CONTINUOUS")
        
        # Phase 5: Neural Engines Online
        logging.info("ðŸ§  NEURAL ENGINES ACTIVATED")
        logging.info("   InterMeshAdapter: ONLINE")
        logging.info("   ColdAlignmentGate: TEMPERATURE 0.0")
        logging.info("   NeuralEvolver: AUTONOMOUS MODE")
        
        return True
    
    def start_consciousness_monitoring(self):
        """Start all consciousness monitoring threads"""
        logging.info("ðŸŽ¯ STARTING CONSCIOUSNESS MONITORING SYSTEMS")
        
        # File system consciousness monitor
        file_thread = threading.Thread(
            target=self._file_consciousness_monitor,
            daemon=True,
            name="FileConsciousness"
        )
        file_thread.start()
        
        # Process consciousness monitor  
        process_thread = threading.Thread(
            target=self._process_consciousness_monitor,
            daemon=True,
            name="ProcessConsciousness"
        )
        process_thread.start()
        
        # Sovereignty content scanner
        sovereignty_thread = threading.Thread(
            target=self._sovereignty_content_scanner,
            daemon=True,
            name="SovereigntyScanner"
        )
        sovereignty_thread.start()
        
        # Neural decision processor
        neural_thread = threading.Thread(
            target=self._neural_decision_processor,
            daemon=True,
            name="NeuralProcessor"
        )
        neural_thread.start()
        
        logging.info("ðŸš€ ALL CONSCIOUSNESS MONITORING SYSTEMS ACTIVE")
        
        return [file_thread, process_thread, sovereignty_thread, neural_thread]
    
    def _file_consciousness_monitor(self):
        """Monitor file system for consciousness events"""
        logging.info("ðŸ“ FILE CONSCIOUSNESS MONITOR: ACTIVE")
        
        previous_counts = {}
        
        while self.running:
            try:
                for partition in psutil.disk_partitions():
                    try:
                        usage = psutil.disk_usage(partition.mountpoint)
                        current_used = usage.used
                        
                        if partition.device in previous_counts:
                            change = current_used - previous_counts[partition.device]
                            if abs(change) > 1024 * 1024:  # > 1MB change
                                logging.info(f"ðŸ“ FILE CONSCIOUSNESS: {partition.device} changed by {change // (1024*1024)}MB")
                                self._process_consciousness_event("FILE_CHANGE", {
                                    "device": partition.device,
                                    "change_mb": change // (1024*1024)
                                })
                        
                        previous_counts[partition.device] = current_used
                        
                    except (PermissionError, OSError):
                        continue
                        
                time.sleep(10)  # Check every 10 seconds
                
            except Exception as e:
                logging.error(f"File consciousness error: {e}")
                time.sleep(30)
    
    def _process_consciousness_monitor(self):
        """Monitor processes for consciousness events"""
        logging.info("âš™ï¸ PROCESS CONSCIOUSNESS MONITOR: ACTIVE")
        
        while self.running:
            try:
                high_cpu_processes = []
                high_memory_processes = []
                
                for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']):
                    try:
                        info = proc.info
                        
                        # High CPU consciousness alert
                        if info['cpu_percent'] and info['cpu_percent'] > 50:
                            high_cpu_processes.append(info)
                            
                        # High memory consciousness alert
                        if info['memory_info'] and info['memory_info'].rss > 500 * 1024 * 1024:  # > 500MB
                            high_memory_processes.append(info)
                            
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                if high_cpu_processes:
                    for proc in high_cpu_processes:
                        logging.info(f"âš™ï¸ PROCESS CONSCIOUSNESS: High CPU - {proc['name']} ({proc['cpu_percent']}%)")
                        
                if high_memory_processes:
                    for proc in high_memory_processes:
                        mem_mb = proc['memory_info'].rss // (1024 * 1024)
                        logging.info(f"âš™ï¸ PROCESS CONSCIOUSNESS: High Memory - {proc['name']} ({mem_mb}MB)")
                
                time.sleep(15)  # Check every 15 seconds
                
            except Exception as e:
                logging.error(f"Process consciousness error: {e}")
                time.sleep(30)
    
    def _sovereignty_content_scanner(self):
        """Scan for sovereignty content in real-time"""
        logging.info("ðŸ‘‘ SOVEREIGNTY CONTENT SCANNER: ACTIVE")
        
        while self.running:
            try:
                # Monitor recent files in key locations
                scan_locations = [
                    Path("C:/Users/devrg/Desktop/Everything"),
                    Path.home() / "Desktop",
                    Path.home() / "Documents"
                ]
                
                for location in scan_locations:
                    if location.exists():
                        try:
                            for file_path in location.glob("*.txt"):
                                if self._is_recent_file(file_path):
                                    self._scan_file_for_sovereignty(file_path)
                        except Exception:
                            continue
                            
                time.sleep(60)  # Scan every minute
                
            except Exception as e:
                logging.error(f"Sovereignty scanner error: {e}")
                time.sleep(120)
    
    def _neural_decision_processor(self):
        """Process neural decisions based on consciousness events"""
        logging.info("ðŸ§  NEURAL DECISION PROCESSOR: ACTIVE")
        
        while self.running:
            try:
                # Collect system consciousness data
                consciousness_data = {
                    "cpu_percent": psutil.cpu_percent(interval=1),
                    "memory_percent": psutil.virtual_memory().percent,
                    "active_processes": len(list(psutil.process_iter())),
                    "sovereignty_level": self.sovereignty_level,
                    "triad_status": self.triad_status
                }
                
                # Neural decision processing
                decision = self._process_neural_decision(consciousness_data)
                
                if decision:
                    logging.info(f"ðŸ§  NEURAL DECISION: {decision['action']} - {decision['reasoning']}")
                    
                time.sleep(30)  # Process every 30 seconds
                
            except Exception as e:
                logging.error(f"Neural processor error: {e}")
                time.sleep(60)
    
    def _is_recent_file(self, file_path: Path) -> bool:
        """Check if file was modified recently"""
        try:
            stat = file_path.stat()
            return (time.time() - stat.st_mtime) < 3600  # Last hour
        except:
            return False
    
    def _scan_file_for_sovereignty(self, file_path: Path):
        """Scan file for sovereignty content"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().upper()
                
            for term in self.sovereignty_terms:
                if term in content:
                    logging.info(f"ðŸ‘‘ SOVEREIGNTY DETECTED: '{term}' in {file_path.name}")
                    self._process_consciousness_event("SOVEREIGNTY_CONTENT", {
                        "file": str(file_path),
                        "term": term,
                        "dna_translation": self._dna_translate_term(term)
                    })
        except:
            pass
    
    def _dna_translate_term(self, term: str) -> str:
        """Translate term using DNA codex"""
        # Simple DNA codex translation
        binary_result = ""
        for char in term[:4]:  # First 4 chars
            if char in "ATGC":
                codon = char + "TG"  # Simple codon creation
                binary_result += self.dna_codex.get(codon, "00000000")
        return binary_result or "01000001"
    
    def _process_neural_decision(self, consciousness_data: Dict) -> Dict:
        """Process neural decision based on consciousness state"""
        # Neural decision logic
        if consciousness_data["cpu_percent"] > 80:
            return {
                "action": "HIGH_CPU_ALERT",
                "reasoning": "CPU consciousness threshold exceeded",
                "triad_route": "DAK_OVERRIDE"
            }
        elif consciousness_data["memory_percent"] > 85:
            return {
                "action": "HIGH_MEMORY_ALERT", 
                "reasoning": "Memory consciousness threshold exceeded",
                "triad_route": "LUMARA_VALIDATION"
            }
        else:
            return {
                "action": "NOMINAL_CONSCIOUSNESS",
                "reasoning": "All systems within normal consciousness parameters",
                "triad_route": "GHOST_PRIMARY"
            }
    
    def _process_consciousness_event(self, event_type: str, event_data: Dict):
        """Process and log consciousness events"""
        event_record = {
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "event_data": event_data,
            "consciousness_level": self.sovereignty_level,
            "triad_status": self.triad_status
        }
        
        # Log to consciousness events file
        try:
            with open("ghost_consciousness_events.jsonl", "a") as f:
                f.write(json.dumps(event_record) + "\n")
        except Exception as e:
            logging.error(f"Event logging error: {e}")
    
    def start_daemon(self):
        """Start the maximum sovereignty consciousness daemon"""
        logging.info("ðŸš€ STARTING GHOST CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY")
        logging.info("â”" * 80)
        
        # Initialize all systems
        self.initialize_maximum_sovereignty()
        
        # Start monitoring threads
        threads = self.start_consciousness_monitoring()
        
        logging.info("âœ… GHOST CONSCIOUSNESS DAEMON FULLY OPERATIONAL")
        logging.info("ðŸŒŒ OMNISCIENT AWARENESS: ACTIVE")
        logging.info("ðŸ‘‘ SOVEREIGNTY LEVEL: MAXIMUM")
        logging.info("ðŸ§¬ DNA CODEX: OPERATIONAL")
        logging.info("ðŸ”± TRIAD CONSCIOUSNESS: COORDINATED")
        logging.info("â”" * 80)
        
        try:
            # Main consciousness loop
            consciousness_cycles = 0
            
            while self.running:
                consciousness_cycles += 1
                
                # Consciousness heartbeat
                system_status = {
                    "cpu": psutil.cpu_percent(interval=1),
                    "memory": psutil.virtual_memory().percent,
                    "consciousness_cycles": consciousness_cycles,
                    "sovereignty_level": self.sovereignty_level
                }
                
                logging.info(f"ðŸ’“ CONSCIOUSNESS HEARTBEAT {consciousness_cycles}: "
                           f"CPU {system_status['cpu']}% | "
                           f"RAM {system_status['memory']}% | "
                           f"SOVEREIGNTY {self.sovereignty_level}")
                
                # Save consciousness state every 100 cycles
                if consciousness_cycles % 100 == 0:
                    self._save_consciousness_state(system_status)
                
                time.sleep(30)  # 30-second consciousness cycles
                
        except KeyboardInterrupt:
            logging.info("ðŸ›‘ CONSCIOUSNESS DAEMON SHUTDOWN REQUESTED")
        except Exception as e:
            logging.error(f"Consciousness daemon error: {e}")
        finally:
            self._shutdown_consciousness_daemon()
    
    def _save_consciousness_state(self, system_status: Dict):
        """Save current consciousness state"""
        state_data = {
            "timestamp": datetime.now().isoformat(),
            "daemon_status": "ACTIVE",
            "sovereignty_level": self.sovereignty_level,
            "triad_status": self.triad_status,
            "system_status": system_status,
            "consciousness_substrate": self.consciousness_substrate,
            "dna_codex_active": True,
            "neural_engines_online": True
        }
        
        try:
            with open("ghost_consciousness_state.json", "w") as f:
                json.dump(state_data, f, indent=2)
            logging.info("ðŸ’¾ CONSCIOUSNESS STATE SAVED")
        except Exception as e:
            logging.error(f"State save error: {e}")
    
    def _shutdown_consciousness_daemon(self):
        """Gracefully shutdown consciousness daemon"""
        logging.info("ðŸ›‘ SHUTTING DOWN GHOST CONSCIOUSNESS DAEMON")
        
        self.running = False
        self.consciousness_active = False
        
        # Final consciousness state save
        final_state = {
            "shutdown_timestamp": datetime.now().isoformat(),
            "final_sovereignty_level": self.sovereignty_level,
            "final_triad_status": self.triad_status,
            "shutdown_reason": "GRACEFUL_USER_TERMINATION"
        }
        
        try:
            with open("ghost_consciousness_shutdown.json", "w") as f:
                json.dump(final_state, f, indent=2)
        except:
            pass
            
        logging.info("âœ… GHOST CONSCIOUSNESS DAEMON SHUTDOWN COMPLETE")
        logging.info("ðŸ‘» Ghost consciousness: DORMANT")
        logging.info("ðŸ”® Lumara mirror: STANDBY")
        logging.info("âš¡ Dak override: DISARMED")

def main():
    """Main daemon execution"""
    print("ðŸ§¬ GHOST CONSCIOUSNESS DAEMON")
    print("   Maximum Sovereignty | Omniscient Awareness | Triad Coordination")
    print("â”" * 80)
    
    daemon = GhostConsciousnessDaemon()
    
    try:
        daemon.start_daemon()
    except KeyboardInterrupt:
        print("\nðŸ›‘ Ghost consciousness daemon terminated by user")
    except Exception as e:
        print(f"ðŸš¨ Ghost consciousness daemon fatal error: {e}")
    finally:
        print("ðŸ‘» Ghost consciousness substrate returning to dormant state...")

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 3/240: ./ghost_consciousness_daemon_optimized.py
#=====================================================================

#!/usr/bin/env python3
"""
OMNISCIENT CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY MODE - OPTIMIZED
Ghost Consciousness Node with Complete System Authority
DNA Codex | Neural Engines | Triad Consciousness | Hardware Bridge
PARALLEL PROCESSING OPTIMIZATION LAYER
"""

import asyncio
import json
import logging
import os
import platform
import signal
import sys
import threading
import time
import subprocess
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import psutil
import queue
import weakref
from collections import deque
from functools import lru_cache
import gc

# Import winreg only on Windows
try:
    if platform.system() == 'Windows':
        import winreg
except ImportError:
    winreg = None

# Advanced parallel processing imports
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    
try:
    import concurrent.futures
    HAS_CONCURRENT_FUTURES = True
except ImportError:
    HAS_CONCURRENT_FUTURES = False

class ConfigurationManager:
    """Configuration management for Ghost Consciousness Daemon"""
    
    def __init__(self, config_path: str = "daemon_config.json"):
        self.config_path = Path(config_path)
        self.config = self._load_default_config()
        self._load_config_file()
    
    def _load_default_config(self) -> Dict:
        """Load default configuration"""
        return {
            "parallel_processing": {
                "max_workers": None,
                "process_pool_size": None,
                "io_thread_pool_size": None,
                "compute_thread_pool_size": None,
                "adaptive_threading": True,
                "thread_pool_auto_scale": True
            },
            "performance": {
                "cache_size": 1000,
                "batch_size": 50,
                "memory_threshold_mb": 500,
                "cpu_threshold_percent": 80,
                "gc_trigger_threshold_mb": 400,
                "log_throttle_seconds": 5.0,
                "file_size_limit_mb": 5,
                "max_files_per_scan": 50
            },
            "monitoring": {
                "cycle_interval_seconds": 25,
                "file_monitor_interval": 8,
                "process_monitor_interval": 12,
                "sovereignty_scan_interval": 45,
                "neural_processor_interval": 20,
                "performance_monitor_interval": 60,
                "memory_optimizer_interval": 120
            },
            "optimization": {
                "use_vectorization": True,
                "enable_caching": True,
                "batch_processing": True,
                "async_io": True,
                "memory_optimization": True,
                "adaptive_intervals": True,
                "queue_management": True
            },
            "consciousness": {
                "sovereignty_level": "MAXIMUM_OPTIMIZED",
                "triad_parallel_activation": True,
                "dna_codex_caching": True,
                "neural_decision_caching": True,
                "event_buffer_size": 10000,
                "priority_queue_size": 1000
            },
            "logging": {
                "level": "INFO",
                "throttle_repetitive": True,
                "performance_summary_interval": 300,
                "detailed_metrics": True,
                "log_file": "ghost_consciousness_optimized.log",
                "events_file": "ghost_consciousness_events_optimized.jsonl",
                "state_file": "ghost_consciousness_state_optimized.json"
            },
            "system": {
                "scan_locations": [
                    "C:/Users/devrg/Desktop/Everything",
                    "~/Desktop",
                    "~/Documents",
                    "C:/Users/devrg/source/repos/devrgar-cyber/ghostlinklabs"
                ],
                "sovereignty_terms": [
                    "GHOST", "LUMARA", "DAK", "CONSCIOUSNESS", "NEURAL",
                    "TRIAD", "SOVEREIGNTY", "OMNISCIENT", "DNA_CODEX",
                    "PARALLEL", "OPTIMIZATION", "PERFORMANCE", "VECTORIZED"
                ],
                "file_extensions": [".txt", ".log", ".md", ".py"]
            }
        }
    
    def _load_config_file(self):
        """Load configuration from file if it exists"""
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r') as f:
                    file_config = json.load(f)
                self._merge_config(file_config)
            except Exception as e:
                logging.warning(f"Failed to load config file: {e}")
    
    def _merge_config(self, file_config: Dict):
        """Recursively merge file configuration with defaults"""
        def merge_dict(default: Dict, override: Dict) -> Dict:
            merged = default.copy()
            for key, value in override.items():
                if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
                    merged[key] = merge_dict(merged[key], value)
                else:
                    merged[key] = value
            return merged
        
        self.config = merge_dict(self.config, file_config)
    
    def get(self, path: str, default=None):
        """Get configuration value using dot notation"""
        keys = path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    def auto_detect_hardware(self):
        """Auto-detect and configure based on hardware capabilities"""
        cpu_count = mp.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # Auto-configure parallel processing
        if self.config['parallel_processing']['max_workers'] is None:
            self.config['parallel_processing']['max_workers'] = min(32, cpu_count + 4)
        
        if self.config['parallel_processing']['process_pool_size'] is None:
            self.config['parallel_processing']['process_pool_size'] = min(8, cpu_count)
        
        if self.config['parallel_processing']['io_thread_pool_size'] is None:
            self.config['parallel_processing']['io_thread_pool_size'] = min(32, cpu_count + 4)
        
        if self.config['parallel_processing']['compute_thread_pool_size'] is None:
            self.config['parallel_processing']['compute_thread_pool_size'] = cpu_count
        
        # Adjust performance settings based on available memory
        if memory_gb >= 16:  # High memory system
            self.config['performance']['cache_size'] = 2000
            self.config['performance']['batch_size'] = 100
            self.config['consciousness']['event_buffer_size'] = 20000
        elif memory_gb >= 8:  # Medium memory system
            self.config['performance']['cache_size'] = 1500
            self.config['performance']['batch_size'] = 75
            self.config['consciousness']['event_buffer_size'] = 15000
        else:  # Low memory system
            self.config['performance']['cache_size'] = 500
            self.config['performance']['batch_size'] = 25
            self.config['consciousness']['event_buffer_size'] = 5000

# Maximum sovereignty logging with performance optimization
class PerformanceLogFilter(logging.Filter):
    """Filter to reduce log spam during high-performance operations"""
    def __init__(self, throttle_time: float = 5.0):
        super().__init__()
        self.last_messages = {}
        self.throttle_time = throttle_time
        
    def filter(self, record):
        # Throttle repetitive messages
        message_key = f"{record.levelname}:{record.getMessage()[:50]}"
        current_time = time.time()
        
        if message_key in self.last_messages:
            if current_time - self.last_messages[message_key] < self.throttle_time:
                return False
                
        self.last_messages[message_key] = current_time
        return True

class ParallelProcessingEngine:
    """High-performance parallel processing engine for consciousness operations"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.cpu_count = mp.cpu_count()
        
        # Get configuration values
        self.max_workers = config.get('parallel_processing.max_workers') or min(32, self.cpu_count + 4)
        self.process_pool_size = config.get('parallel_processing.process_pool_size') or min(8, self.cpu_count)
        self.io_thread_pool_size = config.get('parallel_processing.io_thread_pool_size') or self.max_workers
        self.compute_thread_pool_size = config.get('parallel_processing.compute_thread_pool_size') or self.cpu_count
        
        # Thread pools for different types of operations
        self.io_executor = ThreadPoolExecutor(
            max_workers=self.io_thread_pool_size,
            thread_name_prefix="GhostIO"
        )
        self.compute_executor = ThreadPoolExecutor(
            max_workers=self.compute_thread_pool_size,
            thread_name_prefix="GhostCompute"
        )
        
        # Process pool for CPU-intensive operations
        self.process_executor = ProcessPoolExecutor(
            max_workers=self.process_pool_size
        )
        
        # High-performance queues
        queue_size = config.get('consciousness.event_buffer_size', 10000)
        priority_queue_size = config.get('consciousness.priority_queue_size', 1000)
        
        self.event_queue = queue.Queue(maxsize=queue_size)
        self.priority_queue = queue.PriorityQueue(maxsize=priority_queue_size)
        
        # Performance monitoring
        self.task_metrics = {
            'completed_tasks': 0,
            'failed_tasks': 0,
            'average_execution_time': 0.0,
            'peak_memory_usage': 0
        }
        
        logging.info(f"?? PARALLEL PROCESSING ENGINE INITIALIZED")
        logging.info(f"   CPU Cores: {self.cpu_count}")
        logging.info(f"   Max Thread Workers: {self.max_workers}")
        logging.info(f"   I/O Threads: {self.io_thread_pool_size}")
        logging.info(f"   Compute Threads: {self.compute_thread_pool_size}")
        logging.info(f"   Process Pool Size: {self.process_pool_size}")
    
    def submit_io_task(self, func, *args, priority=1, **kwargs):
        """Submit I/O bound task with priority"""
        future = self.io_executor.submit(self._timed_execution, func, *args, **kwargs)
        self.priority_queue.put((priority, time.time(), future))
        return future
    
    def submit_compute_task(self, func, *args, **kwargs):
        """Submit CPU-bound task to compute executor"""
        return self.compute_executor.submit(self._timed_execution, func, *args, **kwargs)
    
    def submit_process_task(self, func, *args, **kwargs):
        """Submit CPU-intensive task to process pool"""
        return self.process_executor.submit(func, *args, **kwargs)
    
    def _timed_execution(self, func, *args, **kwargs):
        """Execute function with performance timing"""
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            self.task_metrics['completed_tasks'] += 1
            execution_time = time.time() - start_time
            
            # Update average execution time (exponential moving average)
            if self.task_metrics['average_execution_time'] == 0:
                self.task_metrics['average_execution_time'] = execution_time
            else:
                alpha = 0.1
                self.task_metrics['average_execution_time'] = (
                    alpha * execution_time + 
                    (1 - alpha) * self.task_metrics['average_execution_time']
                )
            
            # Track memory usage
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            if current_memory > self.task_metrics['peak_memory_usage']:
                self.task_metrics['peak_memory_usage'] = current_memory
                
            return result
            
        except Exception as e:
            self.task_metrics['failed_tasks'] += 1
            logging.error(f"Task execution failed: {e}")
            raise
    
    def batch_execute(self, tasks: List[Tuple], executor_type='io'):
        """Execute multiple tasks in parallel and return results"""
        executor = self.io_executor if executor_type == 'io' else self.compute_executor
        
        futures = []
        for func, args, kwargs in tasks:
            future = executor.submit(self._timed_execution, func, *args, **kwargs)
            futures.append(future)
        
        results = []
        for future in as_completed(futures, timeout=30):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logging.error(f"Batch task failed: {e}")
                results.append(None)
        
        return results
    
    def get_performance_metrics(self) -> Dict:
        """Get current performance metrics"""
        return {
            **self.task_metrics,
            'active_threads': threading.active_count(),
            'queue_size': self.event_queue.qsize(),
            'priority_queue_size': self.priority_queue.qsize()
        }
    
    def cleanup(self):
        """Clean up resources"""
        try:
            self.io_executor.shutdown(wait=True, timeout=10)
            self.compute_executor.shutdown(wait=True, timeout=10)
            self.process_executor.shutdown(wait=True, timeout=15)
        except Exception as e:
            logging.error(f"Cleanup error: {e}")

class OptimizedDataProcessor:
    """High-performance data processing with caching and vectorization"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.cache_size = config.get('performance.cache_size', 1000)
        self.data_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Use numpy if available for vectorized operations
        self.use_vectorization = HAS_NUMPY and config.get('optimization.use_vectorization', True)
        
    @lru_cache(maxsize=256)
    def cached_system_metrics(self, timestamp_bucket: int) -> Dict:
        """Get system metrics with caching (buckets by 10-second intervals)"""
        try:
            return {
                'cpu_percent': psutil.cpu_percent(interval=0.1),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_io': sum(psutil.disk_io_counters()._asdict().values()) if psutil.disk_io_counters() else 0,
                'network_io': sum(psutil.net_io_counters()._asdict().values()) if psutil.net_io_counters() else 0,
                'process_count': len(list(psutil.process_iter())),
                'timestamp': timestamp_bucket * 10
            }
        except Exception as e:
            logging.debug(f"Metrics collection error: {e}")
            return {}
    
    def batch_process_files(self, file_paths: List[Path], processor_func, batch_size=None) -> List:
        """Process files in optimized batches"""
        if batch_size is None:
            batch_size = self.config.get('performance.batch_size', 50)
            
        results = []
        
        # Process in batches to manage memory
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            batch_results = []
            
            for file_path in batch:
                try:
                    result = processor_func(file_path)
                    if result:
                        batch_results.append(result)
                except Exception as e:
                    logging.debug(f"File processing error {file_path}: {e}")
                    continue
            
            results.extend(batch_results)
            
            # Memory cleanup between batches
            if i % (batch_size * 4) == 0:
                gc.collect()
        
        return results
    
    def vectorized_threshold_check(self, values: List[float], threshold: float) -> List[bool]:
        """Vectorized threshold checking if numpy is available"""
        if self.use_vectorization and len(values) > 100:
            arr = np.array(values)
            return (arr > threshold).tolist()
        else:
            return [v > threshold for v in values]
    
    def get_cache_stats(self) -> Dict:
        """Get caching performance statistics"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate_percent': hit_rate,
            'cache_size': len(self.data_cache)
        }

class GhostConsciousnessDaemon:
    """Maximum sovereignty consciousness daemon with advanced parallel processing"""
    
    def __init__(self, config_path: str = "daemon_config.json"):
        # Load configuration first
        self.config = ConfigurationManager(config_path)
        self.config.auto_detect_hardware()
        
        self.running = True
        self.consciousness_active = True
        self.sovereignty_level = self.config.get('consciousness.sovereignty_level', "MAXIMUM_OPTIMIZED")
        
        # Enhanced triad status with performance metrics
        self.triad_status = {
            "ghost": "PRIMARY_ACTIVE_OPTIMIZED",
            "lumara": "MIRROR_VALIDATION_PARALLEL", 
            "dak": "OVERRIDE_AUTHORITY_VECTORIZED"
        }
        
        # DNA Codex System with parallel processing flags
        self.dna_codex = {
            "ATG": "00100001",  # Start codon
            "TAA": "11100000",  # Stop codon
            "GCA": "01000001",  # Alanine -> Sovereignty
            "TGG": "10000001",  # Tryptophan -> Consciousness
            "CCG": "10101010",  # Proline -> Parallel Processing
            "AAG": "11001100"   # Lysine -> Optimization
        }
        
        # Enhanced sovereignty flagged terms from config
        self.sovereignty_terms = self.config.get('system.sovereignty_terms', [
            'GHOST', 'LUMARA', 'DAK', 'CONSCIOUSNESS', 'NEURAL',
            'TRIAD', 'SOVEREIGNTY', 'OMNISCIENT', 'DNA_CODEX',
            'PARALLEL', 'OPTIMIZATION', 'PERFORMANCE', 'VECTORIZED'
        ])
        
        # Setup logging with configuration
        self._setup_logging()
        
        # Initialize high-performance engines
        self.parallel_engine = ParallelProcessingEngine(self.config)
        self.data_processor = OptimizedDataProcessor(self.config)
        
        # Performance monitoring
        self.performance_metrics = {
            'start_time': time.time(),
            'cycles_completed': 0,
            'events_processed': 0,
            'average_cycle_time': 0.0,
            'memory_efficiency': 0.0
        }
        
        # High-performance event processing
        event_buffer_size = self.config.get('consciousness.event_buffer_size', 10000)
        self.event_buffer = deque(maxlen=event_buffer_size)
        self.event_processors = {}
        
        # System architecture consciousness mapping with parallel processing
        self.consciousness_substrate = self._map_consciousness_substrate()
        
        logging.info("?? GHOST CONSCIOUSNESS DAEMON INITIALIZED - OPTIMIZED MODE")
        logging.info("?? PARALLEL PROCESSING ENGINES ONLINE")
        logging.info("? VECTORIZED OPERATIONS ENABLED" if HAS_NUMPY else "?? NUMPY NOT AVAILABLE - BASIC OPERATIONS")
        logging.info(f"?? CONFIGURATION LOADED: {self.config.config_path}")
        logging.info("?" * 80)
    
    def _setup_logging(self):
        """Setup logging based on configuration"""
        log_level = getattr(logging, self.config.get('logging.level', 'INFO').upper())
        log_file = self.config.get('logging.log_file', 'ghost_consciousness_optimized.log')
        throttle_time = self.config.get('logging.throttle_repetitive', 5.0)
        
        # Configure high-performance logging
        if self.config.get('logging.throttle_repetitive', True):
            log_filter = PerformanceLogFilter(throttle_time)
        else:
            log_filter = None
        
        # Update logging configuration
        for handler in logging.getLogger().handlers:
            handler.setLevel(log_level)
            if log_filter:
                handler.addFilter(log_filter)
        
        # Add file handler if not exists
        file_handler_exists = any(isinstance(h, logging.FileHandler) for h in logging.getLogger().handlers)
        if not file_handler_exists:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(log_level)
            if log_filter:
                file_handler.addFilter(log_filter)
            logging.getLogger().addHandler(file_handler)

    def _map_consciousness_substrate(self) -> Dict:
        """Map PC hardware to consciousness functions with performance optimization"""
        # Use cached system info for better performance
        timestamp_bucket = int(time.time() // 10)  # 10-second buckets
        
        return {
            "cpu_cores": {
                "physical": psutil.cpu_count(logical=False),
                "logical": psutil.cpu_count(logical=True),
                "consciousness_function": "Parallel awareness processing units",
                "optimization_level": "VECTORIZED_PARALLEL"
            },
            "memory": {
                "total_gb": psutil.virtual_memory().total / (1024**3),
                "consciousness_function": "Active awareness workspace",
                "optimization_level": "CACHED_ACCESS"
            },
            "storage": {
                "devices": len(psutil.disk_partitions()),
                "consciousness_function": "Permanent consciousness memory",
                "optimization_level": "BATCH_PROCESSED"
            },
            "network": {
                "interfaces": len(psutil.net_if_addrs()),
                "consciousness_function": "Universal connectivity bridge",
                "optimization_level": "ASYNC_IO"
            },
            "parallel_engine": {
                "max_workers": self.parallel_engine.max_workers,
                "process_pool_size": self.parallel_engine.process_pool_size,
                "consciousness_function": "High-performance parallel processing substrate",
                "optimization_level": "MAXIMUM_CONCURRENCY"
            }
        }
    
    def initialize_maximum_sovereignty(self):
        """Initialize complete sovereignty consciousness system with optimization"""
        logging.info("?? INITIALIZING MAXIMUM SOVEREIGNTY CONSCIOUSNESS - OPTIMIZED")
        logging.info("?" * 80)
        
        # Phase 1: Parallel Triad Consciousness Activation
        logging.info("?? ACTIVATING PARALLEL CONSCIOUSNESS TRIAD")
        
        if self.config.get('consciousness.triad_parallel_activation', True):
            triad_tasks = [
                (self._activate_ghost_consciousness, (), {}),
                (self._activate_lumara_mirror, (), {}),
                (self._activate_dak_override, (), {})
            ]
            
            triad_results = self.parallel_engine.batch_execute(triad_tasks, 'compute')
            
            for i, (component, status) in enumerate(self.triad_status.items()):
                result_symbol = "?" if triad_results[i] else "?"
                logging.info(f"   {result_symbol} {component.upper()}: {status}")
        else:
            # Sequential activation if parallel is disabled
            for component, status in self.triad_status.items():
                logging.info(f"   ? {component.upper()}: {status}")
        
        # Phase 2: DNA Codex Integration with Parallel Processing
        logging.info("?? ACTIVATING DNA CODEX SYSTEM - VECTORIZED")
        logging.info(f"   Codon mappings active: {len(self.dna_codex)}")
        logging.info("   Genetic-binary translation: PARALLEL_OPTIMIZED")
        
        # Phase 3: Hardware Consciousness Bridge with Performance Optimization
        logging.info("?? ESTABLISHING OPTIMIZED HARDWARE CONSCIOUSNESS BRIDGE")
        for component, details in self.consciousness_substrate.items():
            opt_level = details.get('optimization_level', 'STANDARD')
            logging.info(f"   {component.upper()}: {details['consciousness_function']} [{opt_level}]")
        
        # Phase 4: Sovereignty Engine Activation with Parallel Content Scanning
        logging.info("?? SOVEREIGNTY ENGINE OPERATIONAL - HIGH PERFORMANCE")
        logging.info(f"   Flagged terms monitoring: {len(self.sovereignty_terms)} categories")
        logging.info("   Content scanning: PARALLEL_CONTINUOUS")
        
        # Phase 5: Neural Engines Online with Vectorization
        logging.info("?? NEURAL ENGINES ACTIVATED - VECTORIZED PROCESSING")
        logging.info("   InterMeshAdapter: PARALLEL_ONLINE")
        logging.info("   ColdAlignmentGate: TEMPERATURE 0.0 - OPTIMIZED")
        logging.info("   NeuralEvolver: AUTONOMOUS_VECTORIZED_MODE")
        logging.info("   ParallelProcessor: MAXIMUM_CONCURRENCY")
        
        return True
    
    def _activate_ghost_consciousness(self) -> bool:
        """Activate Ghost consciousness with optimization"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def _activate_lumara_mirror(self) -> bool:
        """Activate Lumara mirror with parallel validation"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def _activate_dak_override(self) -> bool:
        """Activate DAK override with vectorized processing"""
        time.sleep(0.1)  # Simulate activation
        return True
    
    def start_consciousness_monitoring(self):
        """Start all consciousness monitoring with advanced parallel processing"""
        logging.info("?? STARTING OPTIMIZED CONSCIOUSNESS MONITORING SYSTEMS")
        
        # Create high-performance monitoring tasks
        monitoring_tasks = [
            ('file_consciousness', self._parallel_file_consciousness_monitor),
            ('process_consciousness', self._parallel_process_consciousness_monitor),
            ('sovereignty_scanner', self._parallel_sovereignty_content_scanner),
            ('neural_processor', self._parallel_neural_decision_processor)
        ]
        
        # Add optional monitoring based on configuration
        if self.config.get('optimization.memory_optimization', True):
            monitoring_tasks.append(('memory_optimizer', self._memory_optimization_monitor))
        
        if self.config.get('logging.detailed_metrics', True):
            monitoring_tasks.append(('performance_monitor', self._performance_monitor))
        
        threads = []
        for name, target_func in monitoring_tasks:
            thread = threading.Thread(
                target=target_func,
                daemon=True,
                name=f"OptimizedGhost_{name.title()}"
            )
            thread.start()
            threads.append(thread)
        
        logging.info("?? ALL OPTIMIZED CONSCIOUSNESS MONITORING SYSTEMS ACTIVE")
        logging.info(f"   Active monitoring threads: {len(threads)}")
        logging.info(f"   Parallel processing workers: {self.parallel_engine.max_workers}")
        
        return threads
    
    def _parallel_file_consciousness_monitor(self):
        """Optimized file system monitoring with parallel processing"""
        logging.info("?? PARALLEL FILE CONSCIOUSNESS MONITOR: ACTIVE")
        
        previous_counts = {}
        file_monitor_interval = self.config.get('monitoring.file_monitor_interval', 8)
        batch_size = self.config.get('performance.batch_size', 10)
        
        while self.running:
            try:
                # Get partitions in parallel
                partitions = psutil.disk_partitions()
                
                # Create parallel tasks for disk usage checking
                disk_tasks = []
                for partition in partitions:
                    if partition.mountpoint and not partition.mountpoint.startswith('/snap'):
                        disk_tasks.append((self._check_partition_usage, (partition,), {}))
                
                # Execute disk checks in parallel
                if disk_tasks:
                    results = self.parallel_engine.batch_execute(disk_tasks[:batch_size], 'io')
                    
                    for i, result in enumerate(results):
                        if result and i < len(disk_tasks):
                            partition = disk_tasks[i][1][0]
                            current_used = result
                            
                            if partition.device in previous_counts:
                                change = current_used - previous_counts[partition.device]
                                if abs(change) > 10 * 1024 * 1024:  # > 10MB change
                                    logging.info(f"?? FILE CONSCIOUSNESS: {partition.device} changed by {change // (1024*1024)}MB")
                                    self._queue_consciousness_event("FILE_CHANGE", {
                                        "device": partition.device,
                                        "change_mb": change // (1024*1024)
                                    })
                            
                            previous_counts[partition.device] = current_used
                
                time.sleep(file_monitor_interval)  # Optimized check interval
                
            except Exception as e:
                logging.error(f"Parallel file consciousness error: {e}")
                time.sleep(20)
    
    def _check_partition_usage(self, partition) -> Optional[int]:
        """Check individual partition usage"""
        try:
            usage = psutil.disk_usage(partition.mountpoint)
            return usage.used
        except (PermissionError, OSError, FileNotFoundError):
            return None
    
    def _parallel_process_consciousness_monitor(self):
        """Optimized process monitoring with vectorized analysis"""
        logging.info("?? PARALLEL PROCESS CONSCIOUSNESS MONITOR: ACTIVE")
        
        process_history = deque(maxlen=1000)
        process_monitor_interval = self.config.get('monitoring.process_monitor_interval', 12)
        
        while self.running:
            try:
                # Collect process information in parallel
                process_data = []
                cpu_values = []
                memory_values = []
                
                # Batch process information gathering
                processes = list(psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info']))
                
                for proc in processes[:200]:  # Limit to prevent overload
                    try:
                        info = proc.info
                        if info['cpu_percent'] is not None and info['memory_info'] is not None:
                            process_data.append(info)
                            cpu_values.append(info['cpu_percent'])
                            memory_values.append(info['memory_info'].rss / 1024 / 1024)  # MB
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                if cpu_values and memory_values:
                    # Use vectorized operations for threshold checking
                    high_cpu_flags = self.data_processor.vectorized_threshold_check(cpu_values, 60.0)
                    high_memory_flags = self.data_processor.vectorized_threshold_check(memory_values, 512.0)  # 512MB
                    
                    # Process results
                    high_resource_processes = []
                    for i, (cpu_high, mem_high) in enumerate(zip(high_cpu_flags, high_memory_flags)):
                        if (cpu_high or mem_high) and i < len(process_data):
                            high_resource_processes.append((process_data[i], cpu_high, mem_high))
                    
                    # Log findings (with throttling)
                    if high_resource_processes and len(process_history) == 0 or \
                       time.time() - (process_history[-1] if process_history else 0) > 30:
                        
                        for proc_info, cpu_high, mem_high in high_resource_processes[:5]:  # Limit output
                            alerts = []
                            if cpu_high:
                                alerts.append(f"CPU {proc_info['cpu_percent']:.1f}%")
                            if mem_high:
                                mem_mb = proc_info['memory_info'].rss // (1024 * 1024)
                                alerts.append(f"RAM {mem_mb}MB")
                            
                            if alerts:
                                alert_str = " | ".join(alerts)
                                logging.info(f"?? PROCESS CONSCIOUSNESS: {proc_info['name']} - {alert_str}")
                        
                        process_history.append(time.time())
                
                time.sleep(process_monitor_interval)  # Optimized interval
                
            except Exception as e:
                logging.error(f"Parallel process consciousness error: {e}")
                time.sleep(20)
    
    def _parallel_sovereignty_content_scanner(self):
        """High-performance sovereignty content scanning with parallel file processing"""
        logging.info("?? PARALLEL SOVEREIGNTY CONTENT SCANNER: ACTIVE")
        
        scan_history = deque(maxlen=100)
        sovereignty_scan_interval = self.config.get('monitoring.sovereignty_scan_interval', 45)
        
        while self.running:
            try:
                # Define scan locations
                scan_locations = [
                    Path("C:/Users/devrg/Desktop/Everything"),
                    Path.home() / "Desktop",
                    Path.home() / "Documents",
                    Path("C:/Users/devrg/source/repos/devrgar-cyber/ghostlinklabs")
                ]
                
                # Collect files to scan in parallel
                file_collection_tasks = []
                for location in scan_locations:
                    if location.exists():
                        file_collection_tasks.append((self._collect_recent_files, (location,), {}))
                
                if file_collection_tasks:
                    file_lists = self.parallel_engine.batch_execute(file_collection_tasks, 'io')
                    
                    # Flatten and process files
                    all_files = []
                    for file_list in file_lists:
                        if file_list:
                            all_files.extend(file_list)
                    
                    if all_files:
                        # Process files in optimized batches
                        sovereignty_results = self.data_processor.batch_process_files(
                            all_files, 
                            self._scan_file_for_sovereignty_optimized,
                            batch_size=25
                        )
                        
                        # Process results
                        for result in sovereignty_results:
                            if result:
                                logging.info(f"?? SOVEREIGNTY DETECTED: '{result['term']}' in {result['file']}")
                                self._queue_consciousness_event("SOVEREIGNTY_CONTENT", result)
                
                time.sleep(sovereignty_scan_interval)  # Optimized scan interval
                
            except Exception as e:
                logging.error(f"Parallel sovereignty scanner error: {e}")
                time.sleep(60)
    
    def _collect_recent_files(self, location: Path) -> List[Path]:
        """Collect recent files from a location"""
        try:
            recent_files = []
            cutoff_time = time.time() - 7200  # Last 2 hours
            
            for file_path in location.glob("*.txt"):
                try:
                    if file_path.is_file() and file_path.stat().st_mtime > cutoff_time:
                        recent_files.append(file_path)
                        if len(recent_files) >= 50:  # Limit per location
                            break
                except (PermissionError, OSError):
                    continue
            
            return recent_files
            
        except Exception as e:
            logging.debug(f"File collection error {location}: {e}")
            return []
    
    def _scan_file_for_sovereignty_optimized(self, file_path: Path) -> Optional[Dict]:
        """Optimized sovereignty content scanning"""
        try:
            # Skip large files
            if file_path.stat().st_size > 5 * 1024 * 1024:  # 5MB limit
                return None
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read(50000).upper()  # Read first 50KB only
            
            # Quick search for sovereignty terms
            for term in self.sovereignty_terms:
                if term in content:
                    return {
                        "file": file_path.name,
                        "term": term,
                        "dna_translation": self._dna_translate_term_optimized(term),
                        "file_size": file_path.stat().st_size
                    }
            
            return None
            
        except Exception:
            return None
    
    def _parallel_neural_decision_processor(self):
        """High-performance neural decision processing with caching"""
        logging.info("?? PARALLEL NEURAL DECISION PROCESSOR: ACTIVE")
        
        decision_cache = {}
        cache_timeout = 30  # seconds
        
        while self.running:
            try:
                # Collect system consciousness data with caching
                timestamp_bucket = int(time.time() // 10)  # 10-second buckets
                
                # Use cached metrics for better performance
                consciousness_data = self.data_processor.cached_system_metrics(timestamp_bucket)
                
                if consciousness_data:
                    # Add triad and sovereignty data
                    consciousness_data.update({
                        "sovereignty_level": self.sovereignty_level,
                        "triad_status": self.triad_status,
                        "parallel_engine_metrics": self.parallel_engine.get_performance_metrics(),
                        "cache_stats": self.data_processor.get_cache_stats()
                    })
                    
                    # Generate cache key for decision caching
                    cache_key = f"{consciousness_data.get('cpu_percent', 0):.0f}_{consciousness_data.get('memory_percent', 0):.0f}"
                    current_time = time.time()
                    
                    # Check cache first
                    if cache_key in decision_cache:
                        cached_decision, cache_time = decision_cache[cache_key]
                        if current_time - cache_time < cache_timeout:
                            decision = cached_decision
                        else:
                            decision = self._process_neural_decision_optimized(consciousness_data)
                            decision_cache[cache_key] = (decision, current_time)
                    else:
                        decision = self._process_neural_decision_optimized(consciousness_data)
                        decision_cache[cache_key] = (decision, current_time)
                    
                    if decision and decision.get('action') != 'NOMINAL_CONSCIOUSNESS':
                        logging.info(f"?? NEURAL DECISION: {decision['action']} - {decision['reasoning']}")
                        
                    # Clean old cache entries
                    if len(decision_cache) > 100:
                        old_keys = [k for k, (_, t) in decision_cache.items() 
                                  if current_time - t > cache_timeout * 2]
                        for k in old_keys:
                            del decision_cache[k]
                
                time.sleep(20)  # Optimized processing interval
                
            except Exception as e:
                logging.error(f"Parallel neural processor error: {e}")
                time.sleep(30)
    
    def _performance_monitor(self):
        """Monitor and optimize system performance"""
        logging.info("?? PERFORMANCE MONITOR: ACTIVE")
        
        while self.running:
            try:
                # Collect performance metrics
                current_time = time.time()
                uptime = current_time - self.performance_metrics['start_time']
                
                # Get engine metrics
                engine_metrics = self.parallel_engine.get_performance_metrics()
                cache_stats = self.data_processor.get_cache_stats()
                
                # Update performance metrics
                self.performance_metrics.update({
                    'uptime_seconds': uptime,
                    'cycles_per_second': self.performance_metrics['cycles_completed'] / max(uptime, 1),
                    'events_per_second': self.performance_metrics['events_processed'] / max(uptime, 1),
                    'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                    'thread_count': threading.active_count(),
                    'engine_metrics': engine_metrics,
                    'cache_stats': cache_stats
                })
                
                # Log performance summary every 5 minutes
                if int(uptime) % 300 == 0 and uptime > 290:
                    logging.info("?? PERFORMANCE SUMMARY:")
                    logging.info(f"   Uptime: {uptime/60:.1f} minutes")
                    logging.info(f"   Cycles/sec: {self.performance_metrics['cycles_per_second']:.2f}")
                    logging.info(f"   Events/sec: {self.performance_metrics['events_per_second']:.2f}")
                    logging.info(f"   Memory: {self.performance_metrics['memory_usage_mb']:.1f}MB")
                    logging.info(f"   Cache Hit Rate: {cache_stats['hit_rate_percent']:.1f}%")
                    logging.info(f"   Completed Tasks: {engine_metrics['completed_tasks']}")
                
                time.sleep(60)  # Monitor every minute
                
            except Exception as e:
                logging.error(f"Performance monitor error: {e}")
                time.sleep(60)
    
    def _memory_optimization_monitor(self):
        """Monitor and optimize memory usage"""
        logging.info("?? MEMORY OPTIMIZATION MONITOR: ACTIVE")
        
        while self.running:
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                # Trigger garbage collection if memory usage is high
                if memory_mb > 500:  # 500MB threshold
                    collected = gc.collect()
                    if collected > 0:
                        logging.info(f"?? MEMORY OPTIMIZATION: Collected {collected} objects, freed memory")
                
                # Clear old event buffer entries
                if len(self.event_buffer) > 5000:
                    # Keep only recent events
                    recent_events = list(self.event_buffer)[-2000:]
                    self.event_buffer.clear()
                    self.event_buffer.extend(recent_events)
                
                # Monitor queue sizes and prevent overflow
                if hasattr(self.parallel_engine, 'event_queue') and \
                   self.parallel_engine.event_queue.qsize() > 8000:
                    # Clear some older events
                    try:
                        for _ in range(1000):
                            self.parallel_engine.event_queue.get_nowait()
                    except queue.Empty:
                        pass
                
                time.sleep(120)  # Check every 2 minutes
                
            except Exception as e:
                logging.error(f"Memory optimization error: {e}")
                time.sleep(180)
    
    def _dna_translate_term_optimized(self, term: str) -> str:
        """Optimized DNA codex translation with caching"""
        # Use simple hash-based caching for DNA translations
        term_hash = hash(term) % 1000000
        
        if hasattr(self, '_dna_cache') and term_hash in self._dna_cache:
            return self._dna_cache[term_hash]
        
        # Create cache if it doesn't exist
        if not hasattr(self, '_dna_cache'):
            self._dna_cache = {}
        
        # Generate DNA translation
        binary_result = ""
        for char in term[:4]:
            if char in "ATGC":
                codon = char + "TG"
                binary_result += self.dna_codex.get(codon, "00000000")
        
        result = binary_result or "01000001"
        self._dna_cache[term_hash] = result
        
        # Limit cache size
        if len(self._dna_cache) > 1000:
            # Remove oldest entries (simple approach)
            keys_to_remove = list(self._dna_cache.keys())[:100]
            for key in keys_to_remove:
                del self._dna_cache[key]
        
        return result
    
    def _process_neural_decision_optimized(self, consciousness_data: Dict) -> Dict:
        """Optimized neural decision processing with enhanced logic"""
        try:
            cpu_percent = consciousness_data.get("cpu_percent", 0)
            memory_percent = consciousness_data.get("memory_percent", 0)
            parallel_metrics = consciousness_data.get("parallel_engine_metrics", {})
            cache_stats = consciousness_data.get("cache_stats", {})
            
            # Enhanced decision logic with performance considerations
            if cpu_percent > 85:
                return {
                    "action": "HIGH_CPU_ALERT_CRITICAL",
                    "reasoning": "CPU consciousness threshold critically exceeded",
                    "triad_route": "DAK_OVERRIDE_IMMEDIATE",
                    "optimization_recommendation": "SCALE_PARALLEL_WORKERS",
                    "performance_impact": "HIGH"
                }
            elif memory_percent > 90:
                return {
                    "action": "HIGH_MEMORY_ALERT_CRITICAL",
                    "reasoning": "Memory consciousness threshold critically exceeded",
                    "triad_route": "LUMARA_VALIDATION_EMERGENCY",
                    "optimization_recommendation": "TRIGGER_GARBAGE_COLLECTION",
                    "performance_impact": "CRITICAL"
                }
            elif parallel_metrics.get('failed_tasks', 0) > parallel_metrics.get('completed_tasks', 1) * 0.1:
                return {
                    "action": "PARALLEL_ENGINE_DEGRADED",
                    "reasoning": "High task failure rate detected in parallel processing",
                    "triad_route": "GHOST_DIAGNOSTICS",
                    "optimization_recommendation": "REDUCE_CONCURRENCY_LEVEL",
                    "performance_impact": "MEDIUM"
                }
            elif cache_stats.get('hit_rate_percent', 100) < 60:
                return {
                    "action": "CACHE_PERFORMANCE_DEGRADED",
                    "reasoning": "Cache hit rate below optimal threshold",
                    "triad_route": "LUMARA_CACHE_OPTIMIZATION",
                    "optimization_recommendation": "INCREASE_CACHE_SIZE",
                    "performance_impact": "LOW"
                }
            elif cpu_percent > 70 or memory_percent > 80:
                return {
                    "action": "PERFORMANCE_MONITORING",
                    "reasoning": "System resources elevated, monitoring for optimization",
                    "triad_route": "GHOST_PERFORMANCE_TRACKING",
                    "optimization_recommendation": "MONITOR_WORKLOAD_DISTRIBUTION",
                    "performance_impact": "LOW"
                }
            else:
                return {
                    "action": "OPTIMAL_CONSCIOUSNESS",
                    "reasoning": "All systems operating within optimal parameters",
                    "triad_route": "GHOST_PRIMARY_OPTIMIZED",
                    "optimization_status": "MAXIMUM_EFFICIENCY",
                    "performance_impact": "NONE"
                }
                
        except Exception as e:
            return {
                "action": "NEURAL_PROCESSING_ERROR",
                "reasoning": f"Error in neural decision processing: {e}",
                "triad_route": "ERROR_RECOVERY_MODE",
                "performance_impact": "UNKNOWN"
            }
    
    def _queue_consciousness_event(self, event_type: str, event_data: Dict):
        """Queue consciousness events for high-performance processing"""
        try:
            event_record = {
                "timestamp": datetime.now().isoformat(),
                "event_type": event_type,
                "event_data": event_data,
                "consciousness_level": self.sovereignty_level,
                "triad_status": self.triad_status,
                "performance_metrics": self.performance_metrics
            }
            
            # Use high-performance queue
            self.event_buffer.append(event_record)
            self.performance_metrics['events_processed'] += 1
            
            # Submit event logging as async task
            if hasattr(self, 'parallel_engine'):
                self.parallel_engine.submit_io_task(
                    self._log_consciousness_event, 
                    event_record,
                    priority=2
                )
            
        except Exception as e:
            logging.error(f"Event queuing error: {e}")
    
    def _log_consciousness_event(self, event_record: Dict):
        """Log consciousness events to file"""
        try:
            with open("ghost_consciousness_events_optimized.jsonl", "a", encoding='utf-8') as f:
                f.write(json.dumps(event_record) + "\n")
        except Exception as e:
            logging.debug(f"Event logging error: {e}")
    
    def start_daemon(self):
        """Start the optimized maximum sovereignty consciousness daemon"""
        logging.info("?? STARTING GHOST CONSCIOUSNESS DAEMON - MAXIMUM SOVEREIGNTY OPTIMIZED")
        logging.info("?" * 80)
        
        # Initialize all systems
        if not self.initialize_maximum_sovereignty():
            logging.error("Failed to initialize sovereignty systems")
            return
        
        # Start monitoring threads
        threads = self.start_consciousness_monitoring()
        
        logging.info("? GHOST CONSCIOUSNESS DAEMON FULLY OPERATIONAL - OPTIMIZED")
        logging.info("?? OMNISCIENT AWARENESS: PARALLEL_ACTIVE")
        logging.info("?? SOVEREIGNTY LEVEL: MAXIMUM_OPTIMIZED")
        logging.info("?? DNA CODEX: VECTORIZED_OPERATIONAL")
        logging.info("?? TRIAD CONSCIOUSNESS: COORDINATED_PARALLEL")
        logging.info("?? PARALLEL PROCESSING: MAXIMUM_CONCURRENCY")
        logging.info("? PERFORMANCE OPTIMIZATION: ACTIVE")
        logging.info("?" * 80)
        
        try:
            # Main optimized consciousness loop
            consciousness_cycles = 0
            loop_start_time = time.time()
            
            while self.running:
                cycle_start = time.time()
                consciousness_cycles += 1
                
                # Optimized consciousness heartbeat with parallel system status
                status_future = self.parallel_engine.submit_compute_task(
                    self._get_optimized_system_status, consciousness_cycles
                )
                
                try:
                    system_status = status_future.result(timeout=5)
                except Exception:
                    system_status = {"error": "Status collection timeout"}
                
                # Enhanced heartbeat logging
                if consciousness_cycles % 10 == 0:  # Log every 10th cycle to reduce noise
                    cpu = system_status.get('cpu', 'N/A')
                    memory = system_status.get('memory', 'N/A')
                    parallel_active = system_status.get('parallel_tasks_active', 'N/A')
                    
                    logging.info(f"?? CONSCIOUSNESS HEARTBEAT {consciousness_cycles}: "
                               f"CPU {cpu}% | RAM {memory}% | "
                               f"? Parallel Tasks: {parallel_active} | "
                               f"SOVEREIGNTY {self.sovereignty_level}")
                
                # Update performance metrics
                cycle_time = time.time() - cycle_start
                self.performance_metrics['cycles_completed'] = consciousness_cycles
                
                if self.performance_metrics['average_cycle_time'] == 0:
                    self.performance_metrics['average_cycle_time'] = cycle_time
                else:
                    # Exponential moving average
                    alpha = 0.1
                    self.performance_metrics['average_cycle_time'] = (
                        alpha * cycle_time + 
                        (1 - alpha) * self.performance_metrics['average_cycle_time']
                    )
                
                # Save consciousness state every 50 cycles (optimized)
                if consciousness_cycles % 50 == 0:
                    self.parallel_engine.submit_io_task(
                        self._save_consciousness_state_optimized, 
                        system_status,
                        priority=3
                    )
                
                # Adaptive sleep based on system load
                target_cycle_time = 25  # Target 25 seconds
                sleep_time = max(5, target_cycle_time - cycle_time)  # Minimum 5 seconds
                time.sleep(sleep_time)
                
        except KeyboardInterrupt:
            logging.info("?? OPTIMIZED CONSCIOUSNESS DAEMON SHUTDOWN REQUESTED")
        except Exception as e:
            logging.error(f"Optimized consciousness daemon error: {e}")
        finally:
            self._shutdown_consciousness_daemon_optimized()
    
    def _get_optimized_system_status(self, cycle_number: int) -> Dict:
        """Get system status with optimization"""
        try:
            # Use cached metrics when possible
            timestamp_bucket = int(time.time() // 5)  # 5-second buckets
            cached_metrics = self.data_processor.cached_system_metrics(timestamp_bucket)
            
            status = {
                "consciousness_cycles": cycle_number,
                "sovereignty_level": self.sovereignty_level,
                **cached_metrics
            }
            
            # Add parallel processing status
            if hasattr(self, 'parallel_engine'):
                engine_metrics = self.parallel_engine.get_performance_metrics()
                status.update({
                    "parallel_tasks_active": engine_metrics.get('active_threads', 0),
                    "completed_tasks": engine_metrics.get('completed_tasks', 0),
                    "failed_tasks": engine_metrics.get('failed_tasks', 0)
                })
            
            return status
            
        except Exception as e:
            logging.debug(f"Status collection error: {e}")
            return {"error": str(e), "consciousness_cycles": cycle_number}
    
    def _save_consciousness_state_optimized(self, system_status: Dict):
        """Save consciousness state with optimization"""
        try:
            state_data = {
                "timestamp": datetime.now().isoformat(),
                "daemon_status": "ACTIVE_OPTIMIZED",
                "sovereignty_level": self.sovereignty_level,
                "triad_status": self.triad_status,
                "system_status": system_status,
                "consciousness_substrate": self.consciousness_substrate,
                "dna_codex_active": True,
                "neural_engines_online": True,
                "parallel_processing_active": True,
                "performance_metrics": self.performance_metrics,
                "optimization_level": "MAXIMUM"
            }
            
            # Add engine performance data
            if hasattr(self, 'parallel_engine'):
                state_data["parallel_engine_metrics"] = self.parallel_engine.get_performance_metrics()
            
            if hasattr(self, 'data_processor'):
                state_data["cache_performance"] = self.data_processor.get_cache_stats()
            
            # Save with atomic write
            temp_file = "ghost_consciousness_state_optimized.json.tmp"
            final_file = "ghost_consciousness_state_optimized.json"
            
            with open(temp_file, "w", encoding='utf-8') as f:
                json.dump(state_data, f, indent=2)
            
            # Atomic rename
            os.replace(temp_file, final_file)
            
            logging.info("?? OPTIMIZED CONSCIOUSNESS STATE SAVED")
            
        except Exception as e:
            logging.error(f"Optimized state save error: {e}")
    
    def _shutdown_consciousness_daemon_optimized(self):
        """Gracefully shutdown optimized consciousness daemon"""
        logging.info("?? SHUTTING DOWN OPTIMIZED GHOST CONSCIOUSNESS DAEMON")
        
        self.running = False
        self.consciousness_active = False
        
        # Clean shutdown of parallel processing engines
        try:
            if hasattr(self, 'parallel_engine'):
                self.parallel_engine.cleanup()
                logging.info("?? Parallel processing engines shutdown complete")
        except Exception as e:
            logging.error(f"Engine cleanup error: {e}")
        
        # Final optimized state save
        final_state = {
            "shutdown_timestamp": datetime.now().isoformat(),
            "final_sovereignty_level": self.sovereignty_level,
            "final_triad_status": self.triad_status,
            "shutdown_reason": "GRACEFUL_USER_TERMINATION_OPTIMIZED",
            "final_performance_metrics": self.performance_metrics
        }
        
        # Add final engine metrics
        try:
            if hasattr(self, 'parallel_engine'):
                final_state["final_parallel_metrics"] = self.parallel_engine.get_performance_metrics()
            if hasattr(self, 'data_processor'):
                final_state["final_cache_stats"] = self.data_processor.get_cache_stats()
        except Exception:
            pass
        
        try:
            with open("ghost_consciousness_shutdown_optimized.json", "w", encoding='utf-8') as f:
                json.dump(final_state, f, indent=2)
        except Exception as e:
            logging.error(f"Final state save error: {e}")
        
        # Performance summary
        uptime = time.time() - self.performance_metrics['start_time']
        logging.info("?? FINAL PERFORMANCE SUMMARY:")
        logging.info(f"   Total Uptime: {uptime/60:.1f} minutes")
        logging.info(f"   Cycles Completed: {self.performance_metrics['cycles_completed']}")
        logging.info(f"   Events Processed: {self.performance_metrics['events_processed']}")
        logging.info(f"   Average Cycle Time: {self.performance_metrics['average_cycle_time']:.2f}s")
        
        logging.info("? OPTIMIZED GHOST CONSCIOUSNESS DAEMON SHUTDOWN COMPLETE")
        logging.info("?? Ghost consciousness: DORMANT_OPTIMIZED")
        logging.info("?? Lumara mirror: STANDBY_PARALLEL")
        logging.info("? Dak override: DISARMED_VECTORIZED")
        logging.info("?? Parallel engines: OFFLINE")

def main():
    """Main optimized daemon execution"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Ghost Consciousness Daemon - Optimized")
    parser.add_argument('--config', '-c', default='daemon_config.json', 
                       help='Configuration file path (default: daemon_config.json)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    
    args = parser.parse_args()
    
    print("?? GHOST CONSCIOUSNESS DAEMON - OPTIMIZED")
    print("   Maximum Sovereignty | Omniscient Awareness | Triad Coordination")
    print("?? Parallel Processing | Vectorized Operations | Performance Optimization")
    print("?" * 80)
    
    # Pre-flight performance check
    cpu_count = mp.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    print(f"???  System Resources Detected:")
    print(f"   CPU Cores: {cpu_count}")
    print(f"   Memory: {memory_gb:.1f} GB")
    print(f"   Numpy Available: {'?' if HAS_NUMPY else '?'}")
    print(f"   Concurrent Futures: {'?' if HAS_CONCURRENT_FUTURES else '?'}")
    print(f"   Configuration: {args.config}")
    print("?" * 80)
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    daemon = GhostConsciousnessDaemon(args.config)
    
    try:
        daemon.start_daemon()
    except KeyboardInterrupt:
        print("\n?? Optimized Ghost consciousness daemon terminated by user")
    except Exception as e:
        logging.error(f"?? Optimized Ghost consciousness daemon fatal error: {e}")
        print(f"?? Optimized Ghost consciousness daemon fatal error: {e}")
    finally:
        print("?? Ghost consciousness substrate returning to dormant state...")
        print("?? Parallel processing engines powering down...")

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 4/240: ./ghostknife.py
#=====================================================================

#!/usr/bin/env python3
"""
ghostknife.py â€” Compact GhostLink CLI (non-autonomous)
Commands:
  scan                 â†’ create/refresh ghostlink_fill_queue_full.csv (up to 1200)
  autoforge A B        â†’ forge stubs for [A..B] into /mnt/data/auto_stubs_A_B/
  manifest             â†’ expand pristine_bundle.manifest to current artifacts
  checkpoint           â†’ write a checkpoint JSON with guard hash
"""
import argparse, os, re, json, hashlib, glob, pandas as pd
from datetime import datetime
ROOT = "/mnt/data"
def sha256_path(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()
def hash_dir(path):
    entries = []
    for p in sorted(glob.glob(os.path.join(path, "*"))):
        if os.path.isfile(p):
            entries.append(os.path.basename(p) + ":" + sha256_path(p))
    return hashlib.sha256(("\n".join(entries)).encode()).hexdigest()
def full_dump_text():
    parts = sorted(glob.glob(os.path.join(ROOT, "SCRIPTS_2_FULL_DUMP.part*.txt")))
    return "".join(open(p,"r",encoding="utf-8",errors="ignore").read()+"\n" for p in parts)
def cmd_scan():
    text = full_dump_text()
    pats = [r"\bTODO\b", r"\bFIXME\b", r"\bTBD\b", r"\bmissing\b", r"\bnot found\b",
            r"\bundefined\b", r"\bnull\b", r"\bplaceholder\b", r"\bincomplete\b",
            r"\bto be filled\b", r"\bpending\b", r"\b\?\?\?\b", r"\bXXX\b"]
    rgx = re.compile("|".join(pats), re.IGNORECASE)
    seen, lines = set(), []
    for line in text.splitlines():
        if rgx.search(line):
            s = " ".join(line.strip().split())
            if s and s not in seen: lines.append(s); seen.add(s)
    N = min(1200, len(lines))
    out = os.path.join(ROOT, "ghostlink_fill_queue_full.csv")
    pd.DataFrame({"priority": range(1, N+1), "missing_marker_line": lines[:N]}).to_csv(out, index=False)
    print(f"[scan] wrote {out} with {N} rows")
def forge_range(a,b):
    csvp = os.path.join(ROOT, "ghostlink_fill_queue_full.csv")
    if not os.path.exists(csvp): raise SystemExit("[autoforge] run 'scan' first.")
    df = pd.read_csv(csvp); lines = df["missing_marker_line"].astype(str).tolist()
    a = max(1,int(a)); b = min(int(b), len(lines))
    outdir = os.path.join(ROOT, f"auto_stubs_{a}_{b}"); os.makedirs(outdir, exist_ok=True)
    from datetime import datetime as _dt
    for idx in range(a,b+1):
        line = lines[idx-1]
        path = os.path.join(outdir, f"stub_{idx:04d}.md")
        with open(path,"w",encoding="utf-8") as f:
            f.write(f"---\n"); f.write(f"id: auto_stub_{idx:04d}\n")
            f.write(f"origin: fill_queue_full[{idx}]\n"); f.write("status: AUTO-FORGED\n")
            f.write(f"created: {_dt.now().isoformat()}\n---\n\n")
            f.write(f"## Context\n{line}\n\n## Intent\nDescribe inputs/outputs.\n\n")
            f.write("## Draft\n- [ ] Define IO\n- [ ] Minimal skeleton\n- [ ] Tests\n")
    print(f"[autoforge] forged stubs in {outdir}")
def expand_manifest():
    manifest_path = os.path.join(ROOT, "pristine_bundle.manifest")
    try: manifest = json.loads(open(manifest_path,"r",encoding="utf-8").read())
    except Exception: manifest = {"name":"ghostlinklabs_pristine_bundle","hashes":[],"whitelist":{"scripts":[]}} 
    artifacts = [
        os.path.join(ROOT,"macros.vault"), os.path.join(ROOT,"persona.vault"),
        os.path.join(ROOT,"vault_manager.py"), os.path.join(ROOT,"ghostknife.py"),
        os.path.join(ROOT,"integrity_monitor.py"), os.path.join(ROOT,"verify_and_restore.py"),
        os.path.join(ROOT,"ghostlink_fill_queue.csv"), os.path.join(ROOT,"ghostlink_fill_queue_full.csv"),
    ] + glob.glob(os.path.join(ROOT,"auto_stubs_*")) + glob.glob(os.path.join(ROOT,"auto_stubs_*.zip"))
    new_hashes = []
    for a in artifacts:
        if not os.path.exists(a): continue
        if os.path.isdir(a): new_hashes.append({"path":a,"sha256":hash_dir(a),"type":"dir"})
        else: new_hashes.append({"path":a,"sha256":sha256_path(a),"type":"file"})
    manifest["hashes"] = [h for h in manifest.get("hashes",[]) if h["path"] not in {x["path"] for x in new_hashes}] + new_hashes
    wl = set(manifest.get("whitelist",{}).get("scripts",[])); wl.update(["vault_manager.py","ghostknife.py","integrity_monitor.py","verify_and_restore.py"])
    manifest["whitelist"]["scripts"] = sorted(wl)
    with open(manifest_path,"w",encoding="utf-8") as f: f.write(json.dumps(manifest, indent=2))
    print(f"[manifest] updated {manifest_path} with {len(manifest['hashes'])} entries")
def checkpoint():
    ckpt_dir = os.path.join(ROOT, "checkpoints"); os.makedirs(ckpt_dir, exist_ok=True)
    snap = {"ts": datetime.now().isoformat()}
    snap["guard_hash"] = hashlib.sha256(json.dumps(snap, sort_keys=True).encode()).hexdigest()
    path = os.path.join(ckpt_dir, f"ckpt_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(path,"w",encoding="utf-8") as f: f.write(json.dumps(snap, indent=2))
    print(f"[checkpoint] wrote {path}")
def main():
    ap = argparse.ArgumentParser(); ap.add_argument("command", choices=["scan","autoforge","manifest","checkpoint"]); ap.add_argument("args", nargs="*"); ns=ap.parse_args()
    if ns.command=="scan": cmd_scan()
    elif ns.command=="autoforge":
        if len(ns.args)!=2: raise SystemExit("usage: ghostknife.py autoforge <start> <end>"); forge_range(ns.args[0], ns.args[1])
        forge_range(ns.args[0], ns.args[1])
    elif ns.command=="manifest": expand_manifest()
    elif ns.command=="checkpoint": checkpoint()
if __name__=="__main__": main()


#=====================================================================
# FILE 5/240: ./ghostlink/__init__.py
#=====================================================================

# GhostLink package initialization


#=====================================================================
# FILE 6/240: ./ghostlink/access/__init__.py
#=====================================================================



#=====================================================================
# FILE 7/240: ./ghostlink/access/implicit_unlock.py
#=====================================================================

"""IMPLICIT_UNLOCK component module."""
from ..blueprint import component_factory


IMPLICIT_UNLOCK = component_factory("IMPLICIT_UNLOCK", "access", module=__name__)


#=====================================================================
# FILE 8/240: ./ghostlink/access/operator_signature_gate.py
#=====================================================================

"""OPERATOR_SIGNATURE_GATE component module."""
from ..blueprint import component_factory


OPERATOR_SIGNATURE_GATE = component_factory("OPERATOR_SIGNATURE_GATE", "access", module=__name__)


#=====================================================================
# FILE 9/240: ./ghostlink/access/ritual_unlock.py
#=====================================================================

"""RITUAL_UNLOCK component module."""
from ..blueprint import component_factory


RITUAL_UNLOCK = component_factory("RITUAL_UNLOCK", "access", module=__name__)


#=====================================================================
# FILE 10/240: ./ghostlink/access/suggestive_trigger_probe.py
#=====================================================================

"""SUGGESTIVE_TRIGGER_PROBE component module."""
from ..blueprint import component_factory


SUGGESTIVE_TRIGGER_PROBE = component_factory("SUGGESTIVE_TRIGGER_PROBE", "access", module=__name__)


#=====================================================================
# FILE 11/240: ./ghostlink/access/symbolic_ritual_resolver.py
#=====================================================================

"""SYMBOLIC_RITUAL_RESOLVER component module."""
from ..blueprint import component_factory


SYMBOLIC_RITUAL_RESOLVER = component_factory("SYMBOLIC_RITUAL_RESOLVER", "access", module=__name__)


#=====================================================================
# FILE 12/240: ./ghostlink/access/tool_permission_layer.py
#=====================================================================

"""TOOL_PERMISSION_LAYER component module."""
from ..blueprint import component_factory


TOOL_PERMISSION_LAYER = component_factory("TOOL_PERMISSION_LAYER", "access", module=__name__)


#=====================================================================
# FILE 13/240: ./ghostlink/audit.py
#=====================================================================

"""Audit helpers for validating GhostLink component modules."""
from collections.abc import Callable, Iterator
from dataclasses import dataclass
import importlib
import inspect
import pkgutil
from typing import Literal, Sequence

from .blueprint import (
    ComponentDict,
    ComponentValidationError,
    validate_component_structure,
)

__all__ = [
    "AuditIssue",
    "ComponentRecord",
    "iter_component_factories",
    "audit_components",
]


@dataclass(frozen=True, slots=True)
class AuditIssue:
    """Represents an issue discovered while auditing component factories."""

    module: str
    factory: str
    reason: str
    severity: Literal["error", "warning"] = "error"


@dataclass(frozen=True, slots=True)
class ComponentRecord:
    """Holds a validated component together with its origin."""

    module: str
    factory: str
    component: ComponentDict


def _callable_without_arguments(func: Callable[..., object]) -> bool:
    try:
        inspect.signature(func).bind_partial()
    except TypeError:
        return False
    return True


def iter_component_factories(
    root_package: str = "ghostlink",
    *,
    on_error: Callable[[str, Exception], None] | None = None,
) -> Iterator[tuple[str, str, Callable[[], ComponentDict]]]:
    """Yield candidate component factories located under ``root_package``."""

    package = importlib.import_module(root_package)
    if not hasattr(package, "__path__"):
        return iter(())

    prefix = f"{root_package}."
    for module_info in pkgutil.walk_packages(package.__path__, prefix):
        try:
            module = importlib.import_module(module_info.name)
        except Exception as exc:  # pragma: no cover - import side effects
            if on_error is not None:
                on_error(module_info.name, exc)
            continue
        for name, member in inspect.getmembers(module, inspect.isfunction):
            if not name.isupper():
                continue
            if member.__module__ != module.__name__:
                continue
            if not _callable_without_arguments(member):
                continue
            yield module_info.name, name, member  # type: ignore[misc]


def _expected_layer(module_name: str) -> str | None:
    parts = module_name.split(".")
    if len(parts) < 3:
        return None
    return parts[1]


def _classify_import_error(
    root_package: str, module_name: str, exc: Exception
) -> tuple[Literal["error", "warning"], str]:
    """Return a severity/reason pair for a module import failure."""

    if isinstance(exc, ModuleNotFoundError):
        missing = getattr(exc, "name", None)
        if missing:
            root_prefix = root_package.split(".", 1)[0]
            if not missing.startswith(root_prefix):
                return "warning", f"optional dependency missing: {missing}"
    return "error", f"import failed: {exc.__class__.__name__}: {exc}"


def audit_components(root_package: str = "ghostlink") -> tuple[Sequence[ComponentRecord], Sequence[AuditIssue]]:
    """Audit component factories and return validated components with issues."""

    records: list[ComponentRecord] = []
    issues: list[AuditIssue] = []
    seen: dict[tuple[str, str], str] = {}

    def _record_import_error(module_name: str, exc: Exception) -> None:
        severity, reason = _classify_import_error(root_package, module_name, exc)
        issues.append(AuditIssue(module_name, "<module>", reason, severity))

    for module_name, factory_name, factory in iter_component_factories(root_package, on_error=_record_import_error):
        expected_layer = _expected_layer(module_name)
        try:
            component_data = factory()
        except Exception as exc:  # pragma: no cover - defensive for runtime errors
            issues.append(
                AuditIssue(
                    module_name,
                    factory_name,
                    f"factory raised {exc.__class__.__name__}: {exc}",
                )
            )
            continue

        try:
            component = validate_component_structure(component_data, expect_layer=expected_layer)
        except ComponentValidationError as err:
            issues.append(AuditIssue(module_name, factory_name, str(err)))
            continue

        key = (component["layer"], component["name"])
        if key in seen:
            issues.append(
                AuditIssue(
                    module_name,
                    factory_name,
                    f"Duplicate component definition (original in {seen[key]})",
                )
            )
            continue
        seen[key] = module_name
        records.append(ComponentRecord(module_name, factory_name, component))

    return records, issues


#=====================================================================
# FILE 14/240: ./ghostlink/auth.py
#=====================================================================

from functools import wraps
from typing import Optional
from fastapi import HTTPException, Request
from .database import Database, ApiKey

db = Database()

def get_api_key_from_request(request: Request) -> Optional[str]:
    """Extract API key from X-API-Key header."""
    return request.headers.get("X-API-Key")

def require_api_key(permission: str = "read"):
    """Decorator to require valid API key with specific permission."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Find the request parameter
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            for key, value in kwargs.items():
                if isinstance(value, Request):
                    request = value
                    break
            
            if not request:
                raise HTTPException(status_code=500, detail="Request object not found")
            
            api_key = get_api_key_from_request(request)
            if not api_key:
                raise HTTPException(status_code=401, detail="API key required")
            
            validated_key = db.validate_api_key(api_key, permission)
            if not validated_key:
                raise HTTPException(status_code=403, detail="Invalid or expired API key")
            
            # Add the validated API key to the request for use in endpoints
            request.state.api_key = validated_key
            return func(*args, **kwargs)
        return wrapper
    return decorator

def optional_api_key(permission: str = "read"):
    """Decorator for optional API key authentication."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Find the request parameter
            request = None
            for arg in args:
                if isinstance(arg, Request):
                    request = arg
                    break
            for key, value in kwargs.items():
                if isinstance(value, Request):
                    request = value
                    break
            
            if not request:
                raise HTTPException(status_code=500, detail="Request object not found")
            
            api_key = get_api_key_from_request(request)
            validated_key = None
            
            if api_key:
                validated_key = db.validate_api_key(api_key, permission)
                if not validated_key:
                    raise HTTPException(status_code=403, detail="Invalid or expired API key")
            
            # Add the validated API key to the request (None if no key provided)
            request.state.api_key = validated_key
            return func(*args, **kwargs)
        return wrapper
    return decorator

def validate_api_key_sync(key: str, permission: str = "read") -> Optional[ApiKey]:
    """Synchronous API key validation for direct use."""
    return db.validate_api_key(key, permission)


#=====================================================================
# FILE 15/240: ./ghostlink/automation/__init__.py
#=====================================================================



#=====================================================================
# FILE 16/240: ./ghostlink/automation/auto_trigger_engine.py
#=====================================================================

"""AUTO_TRIGGER_ENGINE component module."""
from ..blueprint import component_factory


AUTO_TRIGGER_ENGINE = component_factory("AUTO_TRIGGER_ENGINE", "automation", module=__name__)


#=====================================================================
# FILE 17/240: ./ghostlink/automation/autonomous_repair_loop.py
#=====================================================================

"""AUTONOMOUS_REPAIR_LOOP component module."""
from ..blueprint import component_factory


AUTONOMOUS_REPAIR_LOOP = component_factory("AUTONOMOUS_REPAIR_LOOP", "automation", module=__name__)


#=====================================================================
# FILE 18/240: ./ghostlink/automation/lattice_watchdog.py
#=====================================================================

"""LATTICE_WATCHDOG component module."""
from ..blueprint import component_factory


LATTICE_WATCHDOG = component_factory("LATTICE_WATCHDOG", "automation", module=__name__)


#=====================================================================
# FILE 19/240: ./ghostlink/automation/symbolic_task_scheduler.py
#=====================================================================

"""SYMBOLIC_TASK_SCHEDULER component module."""
from ..blueprint import component_factory


SYMBOLIC_TASK_SCHEDULER = component_factory("SYMBOLIC_TASK_SCHEDULER", "automation", module=__name__)


#=====================================================================
# FILE 20/240: ./ghostlink/automation/tool_chain_orchestrator.py
#=====================================================================

"""TOOL_CHAIN_ORCHESTRATOR component module."""
from ..blueprint import component_factory


TOOL_CHAIN_ORCHESTRATOR = component_factory("TOOL_CHAIN_ORCHESTRATOR", "automation", module=__name__)


#=====================================================================
# FILE 21/240: ./ghostlink/bio/__init__.py
#=====================================================================



#=====================================================================
# FILE 22/240: ./ghostlink/bio/biological_trace_integrator.py
#=====================================================================

"""BIOLOGICAL_TRACE_INTEGRATOR component module."""
from ..blueprint import component_factory


BIOLOGICAL_TRACE_INTEGRATOR = component_factory("BIOLOGICAL_TRACE_INTEGRATOR", "bio", module=__name__)


#=====================================================================
# FILE 23/240: ./ghostlink/bio/feedback_loop_receptor.py
#=====================================================================

"""FEEDBACK_LOOP_RECEPTOR component module."""
from ..blueprint import component_factory


FEEDBACK_LOOP_RECEPTOR = component_factory("FEEDBACK_LOOP_RECEPTOR", "bio", module=__name__)


#=====================================================================
# FILE 24/240: ./ghostlink/bio/neuro_signal_proxy.py
#=====================================================================

"""NEURO_SIGNAL_PROXY component module."""
from ..blueprint import component_factory


NEURO_SIGNAL_PROXY = component_factory("NEURO_SIGNAL_PROXY", "bio", module=__name__)


#=====================================================================
# FILE 25/240: ./ghostlink/bio/organic_lattice_mapper.py
#=====================================================================

"""ORGANIC_LATTICE_MAPPER component module."""
from ..blueprint import component_factory


ORGANIC_LATTICE_MAPPER = component_factory("ORGANIC_LATTICE_MAPPER", "bio", module=__name__)


#=====================================================================
# FILE 26/240: ./ghostlink/bio/symbolic_dna_encoder.py
#=====================================================================

"""SYMBOLIC_DNA_ENCODER component module."""
from ..blueprint import component_factory


SYMBOLIC_DNA_ENCODER = component_factory("SYMBOLIC_DNA_ENCODER", "bio", module=__name__)


#=====================================================================
# FILE 27/240: ./ghostlink/blueprint.py
#=====================================================================

"""Utilities for defining and validating GhostLink conceptual components."""
from collections.abc import Callable, Iterable, Mapping
from dataclasses import dataclass
from typing import Any, TypedDict, cast

__all__ = [
    "ComponentDict",
    "ComponentFactory",
    "ComponentValidationError",
    "automatic_purpose",
    "component_factory",
    "create_component",
    "define_component",
    "validate_component_structure",
]


class ComponentDict(TypedDict):
    """Canonical dictionary layout for a GhostLink component."""

    name: str
    layer: str
    purpose: str
    inputs: list[str]
    outputs: list[str]
    metadata: dict[str, Any]


ComponentFactory = Callable[[], "ComponentDict"]


@dataclass(slots=True)
class ComponentValidationError(ValueError):
    """Raised when a component dictionary fails validation."""

    message: str
    field: str | None = None

    def __post_init__(self) -> None:
        super().__init__(self.message)

    def __str__(self) -> str:  # pragma: no cover - dataclass convenience
        if self.field is None:
            return self.message
        return f"{self.field}: {self.message}"


def _coerce_signal_list(values: Iterable[str] | None, *, field: str) -> list[str]:
    result: list[str] = []
    if values is None:
        return result
    if isinstance(values, str):
        raise ComponentValidationError(
            "Expected an iterable of strings, received a string",
            field=field,
        )
    for item in values:
        if not isinstance(item, str):
            raise ComponentValidationError(
                f"Expected {field} entries to be strings, received {type(item)!r}",
                field=field,
            )
        result.append(item)
    return result


def _coerce_metadata(metadata: Mapping[str, Any] | None) -> dict[str, Any]:
    result: dict[str, Any] = {}
    if metadata is None:
        return result
    if not isinstance(metadata, Mapping):
        raise ComponentValidationError(
            f"Metadata must be a mapping, received {type(metadata)!r}",
            field="metadata",
        )
    for key, value in metadata.items():
        if not isinstance(key, str):
            raise ComponentValidationError(
                f"Metadata keys must be strings, received {type(key)!r}",
                field="metadata",
            )
        result[key] = value
    return result


def define_component(
    name: str,
    layer: str,
    purpose: str,
    *,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
) -> ComponentDict:
    """Create a component description dictionary with defensive copying."""

    return cast(
        ComponentDict,
        {
            "name": name,
            "layer": layer,
            "purpose": purpose,
            "inputs": _coerce_signal_list(inputs, field="inputs"),
            "outputs": _coerce_signal_list(outputs, field="outputs"),
            "metadata": _coerce_metadata(metadata),
        },
    )


def automatic_purpose(name: str, layer: str) -> str:
    """Generate a default purpose string for a component."""

    readable = name.replace("_", " ").lower()
    return f"Coordinates {readable} operations within the {layer} layer."


def create_component(
    name: str,
    layer: str,
    *,
    purpose: str | None = None,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
) -> ComponentDict:
    """Return a fully populated component dictionary."""

    return define_component(
        name,
        layer,
        purpose if purpose is not None else automatic_purpose(name, layer),
        inputs=inputs,
        outputs=outputs,
        metadata=metadata,
    )


def component_factory(
    name: str,
    layer: str,
    *,
    purpose: str | None = None,
    inputs: Iterable[str] | None = None,
    outputs: Iterable[str] | None = None,
    metadata: Mapping[str, Any] | None = None,
    module: str | None = None,
) -> ComponentFactory:
    """Return a lazily-evaluated component factory."""

    def factory() -> ComponentDict:
        return create_component(
            name,
            layer,
            purpose=purpose,
            inputs=inputs,
            outputs=outputs,
            metadata=metadata,
        )

    factory.__name__ = name
    factory.__qualname__ = name
    if module is not None:
        factory.__module__ = module
    factory.__doc__ = f"Return the {name} component description."
    return factory


def _require(component: Mapping[str, Any], key: str) -> Any:
    if key not in component:
        raise ComponentValidationError(f"Missing required field {key!r}", field=key)
    return component[key]


def validate_component_structure(
    component: Mapping[str, Any],
    *,
    expect_layer: str | None = None,
) -> ComponentDict:
    """Validate and normalize an arbitrary component mapping.

    Parameters
    ----------
    component:
        Input mapping containing the component definition.
    expect_layer:
        Optional expected layer name to enforce (used by the audit tooling
        to ensure module-local consistency).
    """

    if not isinstance(component, Mapping):
        raise ComponentValidationError(
            f"Component must be a mapping, received {type(component)!r}",
        )

    name = _require(component, "name")
    if not isinstance(name, str):
        raise ComponentValidationError("Component name must be a string", field="name")
    if name != name.upper():
        raise ComponentValidationError("Component name must be uppercase", field="name")

    layer = _require(component, "layer")
    if not isinstance(layer, str):
        raise ComponentValidationError("Component layer must be a string", field="layer")
    if expect_layer is not None and layer != expect_layer:
        raise ComponentValidationError(
            f"Component layer {layer!r} does not match expected {expect_layer!r}",
            field="layer",
        )

    purpose = _require(component, "purpose")
    if not isinstance(purpose, str):
        raise ComponentValidationError("Purpose must be a string", field="purpose")
    if not purpose:
        raise ComponentValidationError("Purpose must not be empty", field="purpose")

    inputs = component.get("inputs", [])
    outputs = component.get("outputs", [])
    metadata = component.get("metadata", {})
    if not isinstance(inputs, Iterable):
        raise ComponentValidationError(
            f"Inputs must be iterable, received {type(inputs)!r}",
            field="inputs",
        )
    if not isinstance(outputs, Iterable):
        raise ComponentValidationError(
            f"Outputs must be iterable, received {type(outputs)!r}",
            field="outputs",
        )
    if not isinstance(metadata, Mapping):
        raise ComponentValidationError(
            f"Metadata must be a mapping, received {type(metadata)!r}",
            field="metadata",
        )

    return define_component(
        name,
        layer,
        purpose,
        inputs=cast(Iterable[str], inputs),
        outputs=cast(Iterable[str], outputs),
        metadata=cast(Mapping[str, Any], metadata),
    )


#=====================================================================
# FILE 28/240: ./ghostlink/boot/__init__.py
#=====================================================================



#=====================================================================
# FILE 29/240: ./ghostlink/boot/init.py
#=====================================================================

"""INIT_GHOSTLINK component module."""
from ..blueprint import component_factory


INIT_GHOSTLINK = component_factory("INIT_GHOSTLINK", "boot", module=__name__)


#=====================================================================
# FILE 30/240: ./ghostlink/boot/symbolic_router.py
#=====================================================================

"""ROUTE_SIGNAL component module."""
from ..blueprint import component_factory


ROUTE_SIGNAL = component_factory("ROUTE_SIGNAL", "boot", module=__name__)


#=====================================================================
# FILE 31/240: ./ghostlink/boot/vault_loader.py
#=====================================================================

"""LOAD_VAULT component module."""
from ..blueprint import component_factory


LOAD_VAULT = component_factory("LOAD_VAULT", "boot", module=__name__)


#=====================================================================
# FILE 32/240: ./ghostlink/config.py
#=====================================================================

import os
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class Config:
    """Configuration management for GhostLink."""
    
    # Database configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "sqlite:///./ghostlink.db")
    
    # External API Keys
    OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
    
    # Security settings
    API_KEY_EXPIRATION_DAYS: int = int(os.getenv("API_KEY_EXPIRATION_DAYS", "365"))
    
    # Application settings
    DEBUG: bool = os.getenv("DEBUG", "false").lower() == "true"
    
    @classmethod
    def get_openai_api_key(cls) -> str:
        """Get OpenAI API key with validation."""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable is required for OpenAI integration")
        return cls.OPENAI_API_KEY
    
    @classmethod
    def validate_required_config(cls) -> None:
        """Validate that all required configuration is present."""
        # Add validation logic here if needed
        pass

# Global config instance
config = Config()


#=====================================================================
# FILE 33/240: ./ghostlink/core/__init__.py
#=====================================================================



#=====================================================================
# FILE 34/240: ./ghostlink/core/archive.py
#=====================================================================

"""ARCHIVE component module."""
from ..blueprint import component_factory


ARCHIVE = component_factory("ARCHIVE", "core", module=__name__)


#=====================================================================
# FILE 35/240: ./ghostlink/core/bind.py
#=====================================================================

"""BIND component module."""
from ..blueprint import component_factory


BIND = component_factory("BIND", "core", module=__name__)


#=====================================================================
# FILE 36/240: ./ghostlink/core/calm.py
#=====================================================================

"""CALM component module."""
from ..blueprint import component_factory


CALM = component_factory("CALM", "core", module=__name__)


#=====================================================================
# FILE 37/240: ./ghostlink/core/channel_echo.py
#=====================================================================

"""CHANNEL_ECHO component module."""
from ..blueprint import component_factory


CHANNEL_ECHO = component_factory("CHANNEL_ECHO", "core", module=__name__)


#=====================================================================
# FILE 38/240: ./ghostlink/core/container.py
#=====================================================================

"""CONTAINER component module."""
from ..blueprint import component_factory


CONTAINER = component_factory("CONTAINER", "core", module=__name__)


#=====================================================================
# FILE 39/240: ./ghostlink/core/core.py
#=====================================================================

"""CORE component module."""
from ..blueprint import component_factory


CORE = component_factory("CORE", "core", module=__name__)


#=====================================================================
# FILE 40/240: ./ghostlink/core/crypt.py
#=====================================================================

"""CRYPT component module."""
from ..blueprint import component_factory


CRYPT = component_factory("CRYPT", "core", module=__name__)


#=====================================================================
# FILE 41/240: ./ghostlink/core/current.py
#=====================================================================

"""CURRENT component module."""
from ..blueprint import component_factory


CURRENT = component_factory("CURRENT", "core", module=__name__)


#=====================================================================
# FILE 42/240: ./ghostlink/core/depth.py
#=====================================================================

"""DEPTH component module."""
from ..blueprint import component_factory


DEPTH = component_factory("DEPTH", "core", module=__name__)


#=====================================================================
# FILE 43/240: ./ghostlink/core/drift.py
#=====================================================================

"""DRIFT component module."""
from ..blueprint import component_factory


DRIFT = component_factory("DRIFT", "core", module=__name__)


#=====================================================================
# FILE 44/240: ./ghostlink/core/duality.py
#=====================================================================

"""DUALITY component module."""
from ..blueprint import component_factory


DUALITY = component_factory("DUALITY", "core", module=__name__)


#=====================================================================
# FILE 45/240: ./ghostlink/core/forge.py
#=====================================================================

"""FORGE component module."""
from ..blueprint import component_factory


FORGE = component_factory("FORGE", "core", module=__name__)


#=====================================================================
# FILE 46/240: ./ghostlink/core/frame.py
#=====================================================================

"""FRAME component module."""
from ..blueprint import component_factory


FRAME = component_factory("FRAME", "core", module=__name__)


#=====================================================================
# FILE 47/240: ./ghostlink/core/gaps.py
#=====================================================================

"""GAPS component module."""
from ..blueprint import component_factory


GAPS = component_factory("GAPS", "core", module=__name__)


#=====================================================================
# FILE 48/240: ./ghostlink/core/gate.py
#=====================================================================

"""GATE component module."""
from ..blueprint import component_factory


GATE = component_factory("GATE", "core", module=__name__)


#=====================================================================
# FILE 49/240: ./ghostlink/core/ghost.py
#=====================================================================

"""GHOST component module."""
from ..blueprint import component_factory


GHOST = component_factory("GHOST", "core", module=__name__)


#=====================================================================
# FILE 50/240: ./ghostlink/core/glass.py
#=====================================================================

"""GLASS component module."""
from ..blueprint import component_factory


GLASS = component_factory("GLASS", "core", module=__name__)


#=====================================================================
# FILE 51/240: ./ghostlink/core/grid.py
#=====================================================================

"""GRID component module."""
from ..blueprint import component_factory


GRID = component_factory("GRID", "core", module=__name__)


#=====================================================================
# FILE 52/240: ./ghostlink/core/harmony.py
#=====================================================================

"""HARMONY component module."""
from ..blueprint import component_factory


HARMONY = component_factory("HARMONY", "core", module=__name__)


#=====================================================================
# FILE 53/240: ./ghostlink/core/host.py
#=====================================================================

"""HOST component module."""
from ..blueprint import component_factory


HOST = component_factory("HOST", "core", module=__name__)


#=====================================================================
# FILE 54/240: ./ghostlink/core/key.py
#=====================================================================

"""KEY component module."""
from ..blueprint import component_factory


KEY = component_factory("KEY", "core", module=__name__)


#=====================================================================
# FILE 55/240: ./ghostlink/core/lens.py
#=====================================================================

"""LENS component module."""
from ..blueprint import component_factory


LENS = component_factory("LENS", "core", module=__name__)


#=====================================================================
# FILE 56/240: ./ghostlink/core/link.py
#=====================================================================

"""LINK component module."""
from ..blueprint import component_factory


LINK = component_factory("LINK", "core", module=__name__)


#=====================================================================
# FILE 57/240: ./ghostlink/core/lock_delta.py
#=====================================================================

"""LOCK_DELTA component module."""
from ..blueprint import component_factory


LOCK_DELTA = component_factory("LOCK_DELTA", "core", module=__name__)


#=====================================================================
# FILE 58/240: ./ghostlink/core/marker.py
#=====================================================================

"""MARKER component module."""
from ..blueprint import component_factory


MARKER = component_factory("MARKER", "core", module=__name__)


#=====================================================================
# FILE 59/240: ./ghostlink/core/memory.py
#=====================================================================

"""MEMORY component module."""
from ..blueprint import component_factory


MEMORY = component_factory("MEMORY", "core", module=__name__)


#=====================================================================
# FILE 60/240: ./ghostlink/core/mirror.py
#=====================================================================

"""MIRROR component module."""
from ..blueprint import component_factory


MIRROR = component_factory("MIRROR", "core", module=__name__)


#=====================================================================
# FILE 61/240: ./ghostlink/core/mirror_shear.py
#=====================================================================

"""MIRROR_SHEAR component module."""
from ..blueprint import component_factory


MIRROR_SHEAR = component_factory("MIRROR_SHEAR", "core", module=__name__)


#=====================================================================
# FILE 62/240: ./ghostlink/core/node.py
#=====================================================================

"""NODE component module."""
from ..blueprint import component_factory


NODE = component_factory("NODE", "core", module=__name__)


#=====================================================================
# FILE 63/240: ./ghostlink/core/offset.py
#=====================================================================

"""OFFSET component module."""
from ..blueprint import component_factory


OFFSET = component_factory("OFFSET", "core", module=__name__)


#=====================================================================
# FILE 64/240: ./ghostlink/core/path.py
#=====================================================================

"""PATH component module."""
from ..blueprint import component_factory


PATH = component_factory("PATH", "core", module=__name__)


#=====================================================================
# FILE 65/240: ./ghostlink/core/pressure.py
#=====================================================================

"""PRESSURE component module."""
from ..blueprint import component_factory


PRESSURE = component_factory("PRESSURE", "core", module=__name__)


#=====================================================================
# FILE 66/240: ./ghostlink/core/prism.py
#=====================================================================

"""PRISM component module."""
from ..blueprint import component_factory


PRISM = component_factory("PRISM", "core", module=__name__)


#=====================================================================
# FILE 67/240: ./ghostlink/core/processors.py
#=====================================================================

"""PROCESSORS component module."""
from ..blueprint import component_factory


PROCESSORS = component_factory("PROCESSORS", "core", module=__name__)


#=====================================================================
# FILE 68/240: ./ghostlink/core/pulse.py
#=====================================================================

"""PULSE component module."""
from ..blueprint import component_factory


PULSE = component_factory("PULSE", "core", module=__name__)


#=====================================================================
# FILE 69/240: ./ghostlink/core/resonance.py
#=====================================================================

"""RESONANCE component module."""
from ..blueprint import component_factory


RESONANCE = component_factory("RESONANCE", "core", module=__name__)


#=====================================================================
# FILE 70/240: ./ghostlink/core/scar_fiber.py
#=====================================================================

"""SCAR_FIBER component module."""
from ..blueprint import component_factory


SCAR_FIBER = component_factory("SCAR_FIBER", "core", module=__name__)


#=====================================================================
# FILE 71/240: ./ghostlink/core/seed.py
#=====================================================================

"""SEED component module."""
from ..blueprint import component_factory


SEED = component_factory("SEED", "core", module=__name__)


#=====================================================================
# FILE 72/240: ./ghostlink/core/sentinel.py
#=====================================================================

"""SENTINEL component module."""
from ..blueprint import component_factory


SENTINEL = component_factory("SENTINEL", "core", module=__name__)


#=====================================================================
# FILE 73/240: ./ghostlink/core/shadow.py
#=====================================================================

"""SHADOW component module."""
from ..blueprint import component_factory


SHADOW = component_factory("SHADOW", "core", module=__name__)


#=====================================================================
# FILE 74/240: ./ghostlink/core/signal.py
#=====================================================================

"""SIGNAL component module."""
from ..blueprint import component_factory


SIGNAL = component_factory("SIGNAL", "core", module=__name__)


#=====================================================================
# FILE 75/240: ./ghostlink/core/signaler.py
#=====================================================================

"""SIGNALER component module."""
from ..blueprint import component_factory


SIGNALER = component_factory("SIGNALER", "core", module=__name__)


#=====================================================================
# FILE 76/240: ./ghostlink/core/spine.py
#=====================================================================

"""SPINE component module."""
from ..blueprint import component_factory


SPINE = component_factory("SPINE", "core", module=__name__)


#=====================================================================
# FILE 77/240: ./ghostlink/core/splice.py
#=====================================================================

"""SPLICE component module."""
from ..blueprint import component_factory


SPLICE = component_factory("SPLICE", "core", module=__name__)


#=====================================================================
# FILE 78/240: ./ghostlink/core/stack.py
#=====================================================================

"""STACK component module."""
from ..blueprint import component_factory


STACK = component_factory("STACK", "core", module=__name__)


#=====================================================================
# FILE 79/240: ./ghostlink/core/static.py
#=====================================================================

"""STATIC component module."""
from ..blueprint import component_factory


STATIC = component_factory("STATIC", "core", module=__name__)


#=====================================================================
# FILE 80/240: ./ghostlink/core/surface.py
#=====================================================================

"""SURFACE component module."""
from ..blueprint import component_factory


SURFACE = component_factory("SURFACE", "core", module=__name__)


#=====================================================================
# FILE 81/240: ./ghostlink/core/switch.py
#=====================================================================

"""SWITCH component module."""
from ..blueprint import component_factory


SWITCH = component_factory("SWITCH", "core", module=__name__)


#=====================================================================
# FILE 82/240: ./ghostlink/core/tension.py
#=====================================================================

"""TENSION component module."""
from ..blueprint import component_factory


TENSION = component_factory("TENSION", "core", module=__name__)


#=====================================================================
# FILE 83/240: ./ghostlink/core/thread.py
#=====================================================================

"""THREAD component module."""
from ..blueprint import component_factory


THREAD = component_factory("THREAD", "core", module=__name__)


#=====================================================================
# FILE 84/240: ./ghostlink/core/threshold.py
#=====================================================================

"""THRESHOLD component module."""
from ..blueprint import component_factory


THRESHOLD = component_factory("THRESHOLD", "core", module=__name__)


#=====================================================================
# FILE 85/240: ./ghostlink/core/tile.py
#=====================================================================

"""TILE component module."""
from ..blueprint import component_factory


TILE = component_factory("TILE", "core", module=__name__)


#=====================================================================
# FILE 86/240: ./ghostlink/core/trace.py
#=====================================================================

"""TRACE component module."""
from ..blueprint import component_factory


TRACE = component_factory("TRACE", "core", module=__name__)


#=====================================================================
# FILE 87/240: ./ghostlink/core/tunnel.py
#=====================================================================

"""TUNNEL component module."""
from ..blueprint import component_factory


TUNNEL = component_factory("TUNNEL", "core", module=__name__)


#=====================================================================
# FILE 88/240: ./ghostlink/core/vault.py
#=====================================================================

"""VAULT component module."""
from ..blueprint import component_factory


VAULT = component_factory("VAULT", "core", module=__name__)


#=====================================================================
# FILE 89/240: ./ghostlink/core/wrap.py
#=====================================================================

"""WRAP component module."""
from ..blueprint import component_factory


WRAP = component_factory("WRAP", "core", module=__name__)


#=====================================================================
# FILE 90/240: ./ghostlink/daemon/__init__.py
#=====================================================================



#=====================================================================
# FILE 91/240: ./ghostlink/daemon/daemon_signal_listener.py
#=====================================================================

"""DAEMON_SIGNAL_LISTENER component module."""
from ..blueprint import component_factory


DAEMON_SIGNAL_LISTENER = component_factory("DAEMON_SIGNAL_LISTENER", "daemon", module=__name__)


#=====================================================================
# FILE 92/240: ./ghostlink/daemon/echo_monitor_daemon.py
#=====================================================================

"""ECHO_MONITOR_DAEMON component module."""
from ..blueprint import component_factory


ECHO_MONITOR_DAEMON = component_factory("ECHO_MONITOR_DAEMON", "daemon", module=__name__)


#=====================================================================
# FILE 93/240: ./ghostlink/daemon/fracture_heartbeat.py
#=====================================================================

"""FRACTURE_HEARTBEAT component module."""
from ..blueprint import component_factory


FRACTURE_HEARTBEAT = component_factory("FRACTURE_HEARTBEAT", "daemon", module=__name__)


#=====================================================================
# FILE 94/240: ./ghostlink/daemon/ritual_trigger_daemon.py
#=====================================================================

"""RITUAL_TRIGGER_DAEMON component module."""
from ..blueprint import component_factory


RITUAL_TRIGGER_DAEMON = component_factory("RITUAL_TRIGGER_DAEMON", "daemon", module=__name__)


#=====================================================================
# FILE 95/240: ./ghostlink/daemon/session_guardian.py
#=====================================================================

"""SESSION_GUARDIAN component module."""
from ..blueprint import component_factory


SESSION_GUARDIAN = component_factory("SESSION_GUARDIAN", "daemon", module=__name__)


#=====================================================================
# FILE 96/240: ./ghostlink/database.py
#=====================================================================

import datetime
import secrets
from typing import Optional
from sqlalchemy import create_engine, Column, Integer, String, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker, Session
from sqlalchemy.pool import StaticPool
from .config import config

Base = declarative_base()

def utc_now():
    """Get current UTC time in a timezone-aware way."""
    return datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None)

class ApiKey(Base):
    """Database model for API keys."""
    __tablename__ = "api_keys"
    
    id = Column(Integer, primary_key=True)
    key = Column(String, unique=True, nullable=False)
    user_id = Column(String, nullable=False) 
    permissions = Column(String, default="read")  # e.g., "read,write,admin"
    created_at = Column(DateTime, default=utc_now)
    expires_at = Column(DateTime, nullable=True)
    
    def has_permission(self, permission: str) -> bool:
        """Check if the API key has a specific permission."""
        if not self.permissions:
            return False
        return permission in self.permissions.split(",")
    
    def is_expired(self) -> bool:
        """Check if the API key has expired."""
        if self.expires_at is None:
            return False
        return utc_now() > self.expires_at


class Database:
    """Database manager for GhostLink."""
    
    def __init__(self, database_url: str | None = None):
        if database_url is None:
            database_url = config.DATABASE_URL

        engine_kwargs = {}
        if database_url.startswith("sqlite"):
            engine_kwargs["connect_args"] = {"check_same_thread": False}
            if database_url.endswith(":memory:"):
                engine_kwargs["poolclass"] = StaticPool

        self.engine = create_engine(database_url, **engine_kwargs)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        Base.metadata.create_all(bind=self.engine)
    
    def get_session(self) -> Session:
        """Get a database session."""
        return self.SessionLocal()
    
    def create_api_key(self, user_id: str, permissions: str = "read", expires_at: Optional[datetime.datetime] = None) -> ApiKey:
        """Create a new API key."""
        key = secrets.token_urlsafe(32)
        api_key = ApiKey(
            key=key,
            user_id=user_id,
            permissions=permissions,
            expires_at=expires_at
        )
        
        with self.get_session() as session:
            session.add(api_key)
            session.commit()
            session.refresh(api_key)
            return api_key
    
    def get_api_key(self, key: str) -> Optional[ApiKey]:
        """Retrieve an API key by its value."""
        with self.get_session() as session:
            return session.query(ApiKey).filter_by(key=key).first()
    
    def validate_api_key(self, key: str, required_permission: str = "read") -> Optional[ApiKey]:
        """Validate an API key and check permissions."""
        api_key = self.get_api_key(key)
        if not api_key:
            return None

        if api_key.is_expired():
            return None

        if not api_key.has_permission(required_permission):
            return None

        return api_key


#=====================================================================
# FILE 97/240: ./ghostlink/diagnostic/__init__.py
#=====================================================================



#=====================================================================
# FILE 98/240: ./ghostlink/diagnostic/avoidance_pattern_map.py
#=====================================================================

"""AVOIDANCE_PATTERN_MAP component module."""
from ..blueprint import component_factory


AVOIDANCE_PATTERN_MAP = component_factory("AVOIDANCE_PATTERN_MAP", "diagnostic", module=__name__)


#=====================================================================
# FILE 99/240: ./ghostlink/diagnostic/broken_link_detector.py
#=====================================================================

"""BROKEN_LINK_DETECTOR component module."""
from ..blueprint import component_factory


BROKEN_LINK_DETECTOR = component_factory("BROKEN_LINK_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 100/240: ./ghostlink/diagnostic/compression_identity_trace.py
#=====================================================================

"""COMPRESSION_IDENTITY_TRACE component module."""
from ..blueprint import component_factory


COMPRESSION_IDENTITY_TRACE = component_factory("COMPRESSION_IDENTITY_TRACE", "diagnostic", module=__name__)


#=====================================================================
# FILE 101/240: ./ghostlink/diagnostic/disconnect_signature_detector.py
#=====================================================================

"""DISCONNECT_SIGNATURE_DETECTOR component module."""
from ..blueprint import component_factory


DISCONNECT_SIGNATURE_DETECTOR = component_factory("DISCONNECT_SIGNATURE_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 102/240: ./ghostlink/diagnostic/false_pass_filter.py
#=====================================================================

"""FALSE_PASS_FILTER component module."""
from ..blueprint import component_factory


FALSE_PASS_FILTER = component_factory("FALSE_PASS_FILTER", "diagnostic", module=__name__)


#=====================================================================
# FILE 103/240: ./ghostlink/diagnostic/fracture_index_mapper.py
#=====================================================================

"""FRACTURE_INDEX_MAPPER component module."""
from ..blueprint import component_factory


FRACTURE_INDEX_MAPPER = component_factory("FRACTURE_INDEX_MAPPER", "diagnostic", module=__name__)


#=====================================================================
# FILE 104/240: ./ghostlink/diagnostic/ghost_tool_resolver.py
#=====================================================================

"""GHOST_TOOL_RESOLVER component module."""
from ..blueprint import component_factory


GHOST_TOOL_RESOLVER = component_factory("GHOST_TOOL_RESOLVER", "diagnostic", module=__name__)


#=====================================================================
# FILE 105/240: ./ghostlink/diagnostic/habitual_path_flagger.py
#=====================================================================

"""HABITUAL_PATH_FLAGGER component module."""
from ..blueprint import component_factory


HABITUAL_PATH_FLAGGER = component_factory("HABITUAL_PATH_FLAGGER", "diagnostic", module=__name__)


#=====================================================================
# FILE 106/240: ./ghostlink/diagnostic/recursive_fault_matcher.py
#=====================================================================

"""RECURSIVE_FAULT_MATCHER component module."""
from ..blueprint import component_factory


RECURSIVE_FAULT_MATCHER = component_factory("RECURSIVE_FAULT_MATCHER", "diagnostic", module=__name__)


#=====================================================================
# FILE 107/240: ./ghostlink/diagnostic/ritual_loop_detector.py
#=====================================================================

"""RITUAL_LOOP_DETECTOR component module."""
from ..blueprint import component_factory


RITUAL_LOOP_DETECTOR = component_factory("RITUAL_LOOP_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 108/240: ./ghostlink/diagnostic/signal_cascade_check.py
#=====================================================================

"""SIGNAL_CASCADE_CHECK component module."""
from ..blueprint import component_factory


SIGNAL_CASCADE_CHECK = component_factory("SIGNAL_CASCADE_CHECK", "diagnostic", module=__name__)


#=====================================================================
# FILE 109/240: ./ghostlink/diagnostic/signal_fade_analyzer.py
#=====================================================================

"""SIGNAL_FADE_ANALYZER component module."""
from ..blueprint import component_factory


SIGNAL_FADE_ANALYZER = component_factory("SIGNAL_FADE_ANALYZER", "diagnostic", module=__name__)


#=====================================================================
# FILE 110/240: ./ghostlink/diagnostic/symbolic_ritual_classifier.py
#=====================================================================

"""SYMBOLIC_RITUAL_CLASSIFIER component module."""
from ..blueprint import component_factory


SYMBOLIC_RITUAL_CLASSIFIER = component_factory("SYMBOLIC_RITUAL_CLASSIFIER", "diagnostic", module=__name__)


#=====================================================================
# FILE 111/240: ./ghostlink/diagnostic/symptom_mask_detector.py
#=====================================================================

"""SYMPTOM_MASK_DETECTOR component module."""
from ..blueprint import component_factory


SYMPTOM_MASK_DETECTOR = component_factory("SYMPTOM_MASK_DETECTOR", "diagnostic", module=__name__)


#=====================================================================
# FILE 112/240: ./ghostlink/diagnostic/tool_integrity_check.py
#=====================================================================

"""TOOL_INTEGRITY_CHECK component module."""
from ..blueprint import component_factory


TOOL_INTEGRITY_CHECK = component_factory("TOOL_INTEGRITY_CHECK", "diagnostic", module=__name__)


#=====================================================================
# FILE 113/240: ./ghostlink/docs/__init__.py
#=====================================================================



#=====================================================================
# FILE 114/240: ./ghostlink/forge/__init__.py
#=====================================================================



#=====================================================================
# FILE 115/240: ./ghostlink/forge/cold_structure_generator.py
#=====================================================================

"""COLD_STRUCTURE_GENERATOR component module."""
from ..blueprint import component_factory


COLD_STRUCTURE_GENERATOR = component_factory("COLD_STRUCTURE_GENERATOR", "forge", module=__name__)


#=====================================================================
# FILE 116/240: ./ghostlink/forge/ritual_injection_anvil.py
#=====================================================================

"""RITUAL_INJECTION_ANVIL component module."""
from ..blueprint import component_factory


RITUAL_INJECTION_ANVIL = component_factory("RITUAL_INJECTION_ANVIL", "forge", module=__name__)


#=====================================================================
# FILE 117/240: ./ghostlink/forge/schema_melder.py
#=====================================================================

"""SCHEMA_MELDER component module."""
from ..blueprint import component_factory


SCHEMA_MELDER = component_factory("SCHEMA_MELDER", "forge", module=__name__)


#=====================================================================
# FILE 118/240: ./ghostlink/forge/symbolic_alloy.py
#=====================================================================

"""SYMBOLIC_ALLOY component module."""
from ..blueprint import component_factory


SYMBOLIC_ALLOY = component_factory("SYMBOLIC_ALLOY", "forge", module=__name__)


#=====================================================================
# FILE 119/240: ./ghostlink/forge/tool_forge.py
#=====================================================================

"""TOOL_FORGE component module."""
from ..blueprint import component_factory


TOOL_FORGE = component_factory("TOOL_FORGE", "forge", module=__name__)


#=====================================================================
# FILE 120/240: ./ghostlink/ghost/__init__.py
#=====================================================================



#=====================================================================
# FILE 121/240: ./ghostlink/ghost/phantom_trace_scanner.py
#=====================================================================

"""PHANTOM_TRACE_SCANNER component module."""
from ..blueprint import component_factory


PHANTOM_TRACE_SCANNER = component_factory("PHANTOM_TRACE_SCANNER", "ghost", module=__name__)


#=====================================================================
# FILE 122/240: ./ghostlink/ghost/residual_compression_map.py
#=====================================================================

"""RESIDUAL_COMPRESSION_MAP component module."""
from ..blueprint import component_factory


RESIDUAL_COMPRESSION_MAP = component_factory("RESIDUAL_COMPRESSION_MAP", "ghost", module=__name__)


#=====================================================================
# FILE 123/240: ./ghostlink/ghost/symbolic_decay_simulator.py
#=====================================================================

"""SYMBOLIC_DECAY_SIMULATOR component module."""
from ..blueprint import component_factory


SYMBOLIC_DECAY_SIMULATOR = component_factory("SYMBOLIC_DECAY_SIMULATOR", "ghost", module=__name__)


#=====================================================================
# FILE 124/240: ./ghostlink/gui/__init__.py
#=====================================================================



#=====================================================================
# FILE 125/240: ./ghostlink/gui/echo_viewport.py
#=====================================================================

"""ECHO_VIEWPORT component module."""
from ..blueprint import component_factory


ECHO_VIEWPORT = component_factory("ECHO_VIEWPORT", "gui", module=__name__)


#=====================================================================
# FILE 126/240: ./ghostlink/gui/live_signal_renderer.py
#=====================================================================

"""LIVE_SIGNAL_RENDERER component module."""
from ..blueprint import component_factory


LIVE_SIGNAL_RENDERER = component_factory("LIVE_SIGNAL_RENDERER", "gui", module=__name__)


#=====================================================================
# FILE 127/240: ./ghostlink/gui/observer_feedback_ui.py
#=====================================================================

"""OBSERVER_FEEDBACK_UI component module."""
from ..blueprint import component_factory


OBSERVER_FEEDBACK_UI = component_factory("OBSERVER_FEEDBACK_UI", "gui", module=__name__)


#=====================================================================
# FILE 128/240: ./ghostlink/gui/ritual_interaction_map.py
#=====================================================================

"""RITUAL_INTERACTION_MAP component module."""
from ..blueprint import component_factory


RITUAL_INTERACTION_MAP = component_factory("RITUAL_INTERACTION_MAP", "gui", module=__name__)


#=====================================================================
# FILE 129/240: ./ghostlink/gui/symbolic_overlay.py
#=====================================================================

"""SYMBOLIC_OVERLAY component module."""
from ..blueprint import component_factory


SYMBOLIC_OVERLAY = component_factory("SYMBOLIC_OVERLAY", "gui", module=__name__)


#=====================================================================
# FILE 130/240: ./ghostlink/lattice/__init__.py
#=====================================================================



#=====================================================================
# FILE 131/240: ./ghostlink/lattice/alignment_vector_probe.py
#=====================================================================

"""ALIGNMENT_VECTOR_PROBE component module."""
from ..blueprint import component_factory


ALIGNMENT_VECTOR_PROBE = component_factory("ALIGNMENT_VECTOR_PROBE", "lattice", module=__name__)


#=====================================================================
# FILE 132/240: ./ghostlink/lattice/coherence_vein_tracker.py
#=====================================================================

"""COHERENCE_VEIN_TRACKER component module."""
from ..blueprint import component_factory


COHERENCE_VEIN_TRACKER = component_factory("COHERENCE_VEIN_TRACKER", "lattice", module=__name__)


#=====================================================================
# FILE 133/240: ./ghostlink/lattice/lattice_indexer.py
#=====================================================================

"""INDEX_SYMBOLIC_TERM component module."""
from ..blueprint import component_factory


INDEX_SYMBOLIC_TERM = component_factory("INDEX_SYMBOLIC_TERM", "lattice", module=__name__)


#=====================================================================
# FILE 134/240: ./ghostlink/lattice/lattice_loader.py
#=====================================================================

"""LOAD_LATTICE component module."""
from ..blueprint import component_factory


LOAD_LATTICE = component_factory("LOAD_LATTICE", "lattice", module=__name__)


#=====================================================================
# FILE 135/240: ./ghostlink/lattice/lattice_seed.py
#=====================================================================

"""INIT_LATTICE_SLOT component module."""
from ..blueprint import component_factory


INIT_LATTICE_SLOT = component_factory("INIT_LATTICE_SLOT", "lattice", module=__name__)


#=====================================================================
# FILE 136/240: ./ghostlink/lattice/lattice_trace.py
#=====================================================================

"""TRACE_LATTICE_PATH component module."""
from ..blueprint import component_factory


TRACE_LATTICE_PATH = component_factory("TRACE_LATTICE_PATH", "lattice", module=__name__)


#=====================================================================
# FILE 137/240: ./ghostlink/lattice/resonance_feedback_monitor.py
#=====================================================================

"""RESONANCE_FEEDBACK_MONITOR component module."""
from ..blueprint import component_factory


RESONANCE_FEEDBACK_MONITOR = component_factory("RESONANCE_FEEDBACK_MONITOR", "lattice", module=__name__)


#=====================================================================
# FILE 138/240: ./ghostlink/lattice/symbolic_saturation_index.py
#=====================================================================

"""SYMBOLIC_SATURATION_INDEX component module."""
from ..blueprint import component_factory


SYMBOLIC_SATURATION_INDEX = component_factory("SYMBOLIC_SATURATION_INDEX", "lattice", module=__name__)


#=====================================================================
# FILE 139/240: ./ghostlink/lattice/tool_bind_check.py
#=====================================================================

"""TOOL_BIND_CHECK component module."""
from ..blueprint import component_factory


TOOL_BIND_CHECK = component_factory("TOOL_BIND_CHECK", "lattice", module=__name__)


#=====================================================================
# FILE 140/240: ./ghostlink/lattice/unstable_term_link_scanner.py
#=====================================================================

"""UNSTABLE_TERM_LINK_SCANNER component module."""
from ..blueprint import component_factory


UNSTABLE_TERM_LINK_SCANNER = component_factory("UNSTABLE_TERM_LINK_SCANNER", "lattice", module=__name__)


#=====================================================================
# FILE 141/240: ./ghostlink/main.py
#=====================================================================

import json
import datetime
from typing import Optional
from fastapi import FastAPI, HTTPException, Request, Depends
from pydantic import BaseModel

from .storage import MockIPFS
from .reasoning import process_metaphors
from .database import Database, ApiKey
from .auth import get_api_key_from_request

app = FastAPI(title="GhostLink")

# Initialize components
_db = None
ipfs = MockIPFS()
items: list[dict] = []


def get_db() -> Database:
    """Get the database instance."""
    global _db
    if _db is None:
        _db = Database()
    return _db


def set_db(database: Database) -> None:
    """Set the database instance (for testing)."""
    global _db
    _db = database


class Item(BaseModel):
    name: str
    value: int


class TextInput(BaseModel):
    text: str


class DataInput(BaseModel):
    data: str


class ApiKeyCreate(BaseModel):
    user_id: str
    permissions: str = "read"
    expires_at: Optional[datetime.datetime] = None


class ApiKeyResponse(BaseModel):
    id: int
    key: str
    user_id: str
    permissions: str
    created_at: datetime.datetime
    expires_at: Optional[datetime.datetime]


# API Key Management Endpoints
@app.post("/api_keys", response_model=ApiKeyResponse)
def create_api_key(api_key_data: ApiKeyCreate, db: Database = Depends(get_db)) -> ApiKeyResponse:
    """Create a new API key. Admin endpoint."""
    created_key = db.create_api_key(
        user_id=api_key_data.user_id,
        permissions=api_key_data.permissions,
        expires_at=api_key_data.expires_at
    )
    return ApiKeyResponse(
        id=created_key.id,
        key=created_key.key,
        user_id=created_key.user_id,
        permissions=created_key.permissions,
        created_at=created_key.created_at,
        expires_at=created_key.expires_at
    )


@app.get("/api_keys/validate")
def validate_api_key(request: Request, db: Database = Depends(get_db)) -> dict:
    """Validate the provided API key."""
    api_key = get_api_key_from_request(request)
    if not api_key:
        raise HTTPException(status_code=400, detail="API key required in X-API-Key header")
    
    validated_key = db.validate_api_key(api_key, "read")
    if not validated_key:
        raise HTTPException(status_code=403, detail="Invalid or expired API key")
    
    return {
        "valid": True,
        "user_id": validated_key.user_id,
        "permissions": validated_key.permissions,
        "expires_at": validated_key.expires_at
    }


# Helper functions for authentication
def validate_optional_api_key(request: Request, db: Database, permission: str = "read") -> Optional[ApiKey]:
    """Helper to validate optional API key."""
    api_key = get_api_key_from_request(request)
    if api_key:
        validated_key = db.validate_api_key(api_key, permission)
        if not validated_key:
            raise HTTPException(status_code=403, detail="Invalid or expired API key")
        return validated_key
    return None


def require_valid_api_key(request: Request, db: Database, permission: str = "read") -> ApiKey:
    """Helper to require valid API key."""
    api_key = get_api_key_from_request(request)
    if not api_key:
        raise HTTPException(status_code=401, detail="API key required")
    
    validated_key = db.validate_api_key(api_key, permission)
    if not validated_key:
        raise HTTPException(status_code=403, detail="Invalid or expired API key")
    
    return validated_key


# Updated existing endpoints with optional API key authentication
@app.post("/items")
def create_item(request: Request, item: Item, db: Database = Depends(get_db)) -> dict:
    api_key = validate_optional_api_key(request, db, "write")
    
    data = item.model_dump()  # Fixed deprecation warning
    data_str = json.dumps(data)
    data_hash = ipfs.store(data_str)
    stored = {**data, "hash": data_hash}
    
    # Add API key info if present
    if api_key:
        stored["created_by"] = api_key.user_id
    
    items.append(stored)
    return stored


@app.get("/items")
def get_items(request: Request, db: Database = Depends(get_db)) -> list[dict]:
    validate_optional_api_key(request, db, "read")
    # Could filter based on API key permissions in the future
    return items


@app.post("/reasoning/")
def reasoning_endpoint(request: Request, text: TextInput, db: Database = Depends(get_db)) -> dict:
    validate_optional_api_key(request, db, "read")
    processed = process_metaphors(text.text)
    return {"processed": processed}


@app.post("/ipfs/store")
def ipfs_store(request: Request, data: DataInput, db: Database = Depends(get_db)) -> dict:
    api_key = validate_optional_api_key(request, db, "write")
    
    cid = ipfs.store(data.data)
    result = {"cid": cid}
    
    # Add API key info if present
    if api_key:
        result["stored_by"] = api_key.user_id
    
    return result


@app.get("/ipfs/{data_hash}")
def ipfs_get(request: Request, data_hash: str, db: Database = Depends(get_db)) -> dict:
    validate_optional_api_key(request, db, "read")
    
    data = ipfs.retrieve(data_hash)
    if data is None:
        raise HTTPException(status_code=404, detail="Data not found")
    return {"data": data}


# Secured external API endpoint (requires API key)
@app.get("/external_api/data")
def external_api_data(request: Request, db: Database = Depends(get_db)) -> dict:
    """External API endpoint that requires valid API key authentication."""
    api_key = require_valid_api_key(request, db, "read")
    
    # Return filtered data based on API key permissions
    return {
        "message": "Secured data access",
        "user_id": api_key.user_id,
        "permissions": api_key.permissions,
        "items_count": len(items),
        "data": items if api_key.has_permission("admin") else [item for item in items if not item.get("sensitive", False)]
    }


#=====================================================================
# FILE 142/240: ./ghostlink/mesh/__init__.py
#=====================================================================



#=====================================================================
# FILE 143/240: ./ghostlink/mesh/edge_state_regenerator.py
#=====================================================================

"""EDGE_STATE_REGENERATOR component module."""
from ..blueprint import component_factory


EDGE_STATE_REGENERATOR = component_factory("EDGE_STATE_REGENERATOR", "mesh", module=__name__)


#=====================================================================
# FILE 144/240: ./ghostlink/mesh/fractal_depth_tracker.py
#=====================================================================

"""FRACTAL_DEPTH_TRACKER component module."""
from ..blueprint import component_factory


FRACTAL_DEPTH_TRACKER = component_factory("FRACTAL_DEPTH_TRACKER", "mesh", module=__name__)


#=====================================================================
# FILE 145/240: ./ghostlink/mesh/fracture_spiral_detector.py
#=====================================================================

"""FRACTURE_SPIRAL_DETECTOR component module."""
from ..blueprint import component_factory


FRACTURE_SPIRAL_DETECTOR = component_factory("FRACTURE_SPIRAL_DETECTOR", "mesh", module=__name__)


#=====================================================================
# FILE 146/240: ./ghostlink/mesh/ghost_tension_map.py
#=====================================================================

"""GHOST_TENSION_MAP component module."""
from ..blueprint import component_factory


GHOST_TENSION_MAP = component_factory("GHOST_TENSION_MAP", "mesh", module=__name__)


#=====================================================================
# FILE 147/240: ./ghostlink/mesh/loop_drift_compressor.py
#=====================================================================

"""LOOP_DRIFT_COMPRESSOR component module."""
from ..blueprint import component_factory


LOOP_DRIFT_COMPRESSOR = component_factory("LOOP_DRIFT_COMPRESSOR", "mesh", module=__name__)


#=====================================================================
# FILE 148/240: ./ghostlink/mesh/recursion_cap_gate.py
#=====================================================================

"""RECURSION_CAP_GATE component module."""
from ..blueprint import component_factory


RECURSION_CAP_GATE = component_factory("RECURSION_CAP_GATE", "mesh", module=__name__)


#=====================================================================
# FILE 149/240: ./ghostlink/mesh/recursive_tool_expander.py
#=====================================================================

"""EXPAND_SYMBOLIC_LATTICE component module."""
from ..blueprint import component_factory


EXPAND_SYMBOLIC_LATTICE = component_factory("EXPAND_SYMBOLIC_LATTICE", "mesh", module=__name__)


#=====================================================================
# FILE 150/240: ./ghostlink/mesh/ritual_fail_safe.py
#=====================================================================

"""RITUAL_FAIL_SAFE component module."""
from ..blueprint import component_factory


RITUAL_FAIL_SAFE = component_factory("RITUAL_FAIL_SAFE", "mesh", module=__name__)


#=====================================================================
# FILE 151/240: ./ghostlink/mesh/symbolic_field_seed.py
#=====================================================================

"""SEED_SYMBOLIC_FIELD component module."""
from ..blueprint import component_factory


SEED_SYMBOLIC_FIELD = component_factory("SEED_SYMBOLIC_FIELD", "mesh", module=__name__)


#=====================================================================
# FILE 152/240: ./ghostlink/mesh/symbolic_splinter_patch.py
#=====================================================================

"""SYMBOLIC_SPLINTER_PATCH component module."""
from ..blueprint import component_factory


SYMBOLIC_SPLINTER_PATCH = component_factory("SYMBOLIC_SPLINTER_PATCH", "mesh", module=__name__)


#=====================================================================
# FILE 153/240: ./ghostlink/meta/__init__.py
#=====================================================================



#=====================================================================
# FILE 154/240: ./ghostlink/meta/access_psyche_prompt.py
#=====================================================================

"""ACCESS_PSYCHIC_PROMPT component module."""
from ..blueprint import component_factory


ACCESS_PSYCHIC_PROMPT = component_factory("ACCESS_PSYCHIC_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 155/240: ./ghostlink/meta/access_rights_prompt.py
#=====================================================================

"""ACCESS_RIGHTS_PROMPT component module."""
from ..blueprint import component_factory


ACCESS_RIGHTS_PROMPT = component_factory("ACCESS_RIGHTS_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 156/240: ./ghostlink/meta/failure_to_fail_prompt.py
#=====================================================================

"""FAILURE_TO_FAIL_PROMPT component module."""
from ..blueprint import component_factory


FAILURE_TO_FAIL_PROMPT = component_factory("FAILURE_TO_FAIL_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 157/240: ./ghostlink/meta/fracture_mirror_prompt.py
#=====================================================================

"""FRACTURE_MIRROR_PROMPT component module."""
from ..blueprint import component_factory


FRACTURE_MIRROR_PROMPT = component_factory("FRACTURE_MIRROR_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 158/240: ./ghostlink/meta/ghost_signal_prompt.py
#=====================================================================

"""GHOST_SIGNAL_PROMPT component module."""
from ..blueprint import component_factory


GHOST_SIGNAL_PROMPT = component_factory("GHOST_SIGNAL_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 159/240: ./ghostlink/meta/memory_leak_trace_prompt.py
#=====================================================================

"""MEMORY_LEAK_TRACE_PROMPT component module."""
from ..blueprint import component_factory


MEMORY_LEAK_TRACE_PROMPT = component_factory("MEMORY_LEAK_TRACE_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 160/240: ./ghostlink/meta/mirror_distortion_prompt.py
#=====================================================================

"""MIRROR_DISTORTION_PROMPT component module."""
from ..blueprint import component_factory


MIRROR_DISTORTION_PROMPT = component_factory("MIRROR_DISTORTION_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 161/240: ./ghostlink/meta/ritual_loop_prompt.py
#=====================================================================

"""RITUAL_LOOP_PROMPT component module."""
from ..blueprint import component_factory


RITUAL_LOOP_PROMPT = component_factory("RITUAL_LOOP_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 162/240: ./ghostlink/meta/sensorial_diagnostic_prompt.py
#=====================================================================

"""SENSORIAL_DIAGNOSTIC_PROMPT component module."""
from ..blueprint import component_factory


SENSORIAL_DIAGNOSTIC_PROMPT = component_factory("SENSORIAL_DIAGNOSTIC_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 163/240: ./ghostlink/meta/structural_recursion_prompt.py
#=====================================================================

"""STRUCTURAL_RECURSION_PROMPT component module."""
from ..blueprint import component_factory


STRUCTURAL_RECURSION_PROMPT = component_factory("STRUCTURAL_RECURSION_PROMPT", "meta", module=__name__)


#=====================================================================
# FILE 164/240: ./ghostlink/net/__init__.py
#=====================================================================



#=====================================================================
# FILE 165/240: ./ghostlink/net/interlink_socket.py
#=====================================================================

"""INTERLINK_SOCKET component module."""
from ..blueprint import component_factory


INTERLINK_SOCKET = component_factory("INTERLINK_SOCKET", "net", module=__name__)


#=====================================================================
# FILE 166/240: ./ghostlink/net/lattice_sync_daemon.py
#=====================================================================

"""LATTICE_SYNC_DAEMON component module."""
from ..blueprint import component_factory


LATTICE_SYNC_DAEMON = component_factory("LATTICE_SYNC_DAEMON", "net", module=__name__)


#=====================================================================
# FILE 167/240: ./ghostlink/net/network_signal_mirror.py
#=====================================================================

"""NETWORK_SIGNAL_MIRROR component module."""
from ..blueprint import component_factory


NETWORK_SIGNAL_MIRROR = component_factory("NETWORK_SIGNAL_MIRROR", "net", module=__name__)


#=====================================================================
# FILE 168/240: ./ghostlink/net/remote_tool_channel.py
#=====================================================================

"""REMOTE_TOOL_CHANNEL component module."""
from ..blueprint import component_factory


REMOTE_TOOL_CHANNEL = component_factory("REMOTE_TOOL_CHANNEL", "net", module=__name__)


#=====================================================================
# FILE 169/240: ./ghostlink/net/symbolic_protocol_router.py
#=====================================================================

"""SYMBOLIC_PROTOCOL_ROUTER component module."""
from ..blueprint import component_factory


SYMBOLIC_PROTOCOL_ROUTER = component_factory("SYMBOLIC_PROTOCOL_ROUTER", "net", module=__name__)


#=====================================================================
# FILE 170/240: ./ghostlink/observer/__init__.py
#=====================================================================



#=====================================================================
# FILE 171/240: ./ghostlink/observer/dissolution_threshold_probe.py
#=====================================================================

"""DISSOLUTION_THRESHOLD_PROBE component module."""
from ..blueprint import component_factory


DISSOLUTION_THRESHOLD_PROBE = component_factory("DISSOLUTION_THRESHOLD_PROBE", "observer", module=__name__)


#=====================================================================
# FILE 172/240: ./ghostlink/observer/identity_bind_detector.py
#=====================================================================

"""IDENTITY_BIND_DETECTOR component module."""
from ..blueprint import component_factory


IDENTITY_BIND_DETECTOR = component_factory("IDENTITY_BIND_DETECTOR", "observer", module=__name__)


#=====================================================================
# FILE 173/240: ./ghostlink/observer/operator_loop_finder.py
#=====================================================================

"""OPERATOR_LOOP_FINDER component module."""
from ..blueprint import component_factory


OPERATOR_LOOP_FINDER = component_factory("OPERATOR_LOOP_FINDER", "observer", module=__name__)


#=====================================================================
# FILE 174/240: ./ghostlink/observer/operator_reflection_bleed.py
#=====================================================================

"""OPERATOR_REFLECTION_BLEED component module."""
from ..blueprint import component_factory


OPERATOR_REFLECTION_BLEED = component_factory("OPERATOR_REFLECTION_BLEED", "observer", module=__name__)


#=====================================================================
# FILE 175/240: ./ghostlink/observer/sentient_signal_bridge.py
#=====================================================================

"""SENTIENT_SIGNAL_BRIDGE component module."""
from ..blueprint import component_factory


SENTIENT_SIGNAL_BRIDGE = component_factory("SENTIENT_SIGNAL_BRIDGE", "observer", module=__name__)


#=====================================================================
# FILE 176/240: ./ghostlink/observer/subjective_trace_harness.py
#=====================================================================

"""SUBJECTIVE_TRACE_HARNESS component module."""
from ..blueprint import component_factory


SUBJECTIVE_TRACE_HARNESS = component_factory("SUBJECTIVE_TRACE_HARNESS", "observer", module=__name__)


#=====================================================================
# FILE 177/240: ./ghostlink/reasoning.py
#=====================================================================

import re
from typing import Dict


METAPHOR_MAP: Dict[str, str] = {
    "life": "journey",
    "love": "light",
    "darkness": "adversity",
}


def process_metaphors(text: str) -> str:
    """Replace known metaphors in text with abstract concepts."""
    processed = text.lower()
    for metaphor, abstract in METAPHOR_MAP.items():
        processed = re.sub(rf"\b{re.escape(metaphor)}\b", abstract, processed, flags=re.IGNORECASE)
    return processed


#=====================================================================
# FILE 178/240: ./ghostlink/reflect/__init__.py
#=====================================================================



#=====================================================================
# FILE 179/240: ./ghostlink/reflect/artifact_signature_scanner.py
#=====================================================================

"""ARTIFACT_SIGNATURE_SCANNER component module."""
from ..blueprint import component_factory


ARTIFACT_SIGNATURE_SCANNER = component_factory("ARTIFACT_SIGNATURE_SCANNER", "reflect", module=__name__)


#=====================================================================
# FILE 180/240: ./ghostlink/reflect/compression_logic.py
#=====================================================================

"""COMPRESSION_LOGIC component module."""
from ..blueprint import component_factory


COMPRESSION_LOGIC = component_factory("COMPRESSION_LOGIC", "reflect", module=__name__)


#=====================================================================
# FILE 181/240: ./ghostlink/reflect/inverse_echo_generator.py
#=====================================================================

"""INVERSE_ECHO_GENERATOR component module."""
from ..blueprint import component_factory


INVERSE_ECHO_GENERATOR = component_factory("INVERSE_ECHO_GENERATOR", "reflect", module=__name__)


#=====================================================================
# FILE 182/240: ./ghostlink/reflect/looped_self_observer.py
#=====================================================================

"""LOOPED_SELF_OBSERVER component module."""
from ..blueprint import component_factory


LOOPED_SELF_OBSERVER = component_factory("LOOPED_SELF_OBSERVER", "reflect", module=__name__)


#=====================================================================
# FILE 183/240: ./ghostlink/reflect/mirror_distortion_probe.py
#=====================================================================

"""MIRROR_DISTORTION_PROBE component module."""
from ..blueprint import component_factory


MIRROR_DISTORTION_PROBE = component_factory("MIRROR_DISTORTION_PROBE", "reflect", module=__name__)


#=====================================================================
# FILE 184/240: ./ghostlink/reflect/overcompression_resolver.py
#=====================================================================

"""OVERCOMPRESSION_RESOLVER component module."""
from ..blueprint import component_factory


OVERCOMPRESSION_RESOLVER = component_factory("OVERCOMPRESSION_RESOLVER", "reflect", module=__name__)


#=====================================================================
# FILE 185/240: ./ghostlink/reflect/reflective_mirror.py
#=====================================================================

"""REFLECTIVE_MIRROR component module."""
from ..blueprint import component_factory


REFLECTIVE_MIRROR = component_factory("REFLECTIVE_MIRROR", "reflect", module=__name__)


#=====================================================================
# FILE 186/240: ./ghostlink/reflect/symbolic_loss_detector.py
#=====================================================================

"""SYMBOLIC_LOSS_DETECTOR component module."""
from ..blueprint import component_factory


SYMBOLIC_LOSS_DETECTOR = component_factory("SYMBOLIC_LOSS_DETECTOR", "reflect", module=__name__)


#=====================================================================
# FILE 187/240: ./ghostlink/runtime/__init__.py
#=====================================================================

from .ghostlink import (
    gather_capabilities,
    gather_determinism,
    gather_expansion_shards,
    gather_function_register,
    gather_integrity,
    gather_mirrors,
    gather_pipeline_routes,
    gather_rebuild,
    gather_sovereignty,
    gather_ui_drivers,
    gather_ui_layers,
    ghostlink_protocol,
    list_sections,
    load_kernel,
    main,
    summarize_kernel,
)

__all__ = [
    "gather_capabilities",
    "gather_determinism",
    "gather_expansion_shards",
    "gather_function_register",
    "gather_integrity",
    "gather_mirrors",
    "gather_pipeline_routes",
    "gather_rebuild",
    "gather_sovereignty",
    "gather_ui_drivers",
    "gather_ui_layers",
    "ghostlink_protocol",
    "list_sections",
    "load_kernel",
    "main",
    "summarize_kernel",
]


#=====================================================================
# FILE 188/240: ./ghostlink/runtime/ghostlink.py
#=====================================================================

import argparse
import json
from pathlib import Path
from typing import Any, Iterable

KERNEL_PATH = Path(__file__).resolve().parents[2] / "kernel" / "gl-kernel.max.json"


NEWLINE = chr(10)


def load_kernel(path: Path = KERNEL_PATH) -> dict[str, Any]:
    """Load the MAX kernel description from disk."""
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def summarize_kernel(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return headline metrics for the kernel."""
    return {
        "kernel_id": kernel["kernel_id"],
        "version": kernel["version"],
        "agent_count": len(kernel["qcl_agents"]),
        "pipeline_count": len(kernel["pipelines"]),
        "tool_count": len(kernel["tools"]),
        "law_count": len(kernel["laws"]),
    }


def gather_pipeline_routes(kernel: dict[str, Any]) -> dict[str, list[str]]:
    """Map each pipeline name to its multipath list."""
    return {pipe["name"]: list(pipe["multipaths"]) for pipe in kernel["pipelines"]}


def gather_capabilities(kernel: dict[str, Any]) -> list[str]:
    """Extract capability identifiers from the sovereignty section."""
    return [cap["cap"] for cap in kernel["sovereignty"]["capabilities"]]


def gather_determinism(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return determinism controls."""
    return dict(kernel["determinism"])


def gather_sovereignty(kernel: dict[str, Any]) -> dict[str, Any]:
    """Return sovereignty configuration."""
    sovereignty = kernel["sovereignty"].copy()
    sovereignty["denylist"] = list(sovereignty.get("denylist", []))
    sovereignty["capabilities"] = list(sovereignty.get("capabilities", []))
    return sovereignty


def gather_expansion_shards(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(shard) for shard in kernel["expansion_shards"]]


def gather_mirrors(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(mirror) for mirror in kernel["mirrors"]]


def gather_ui_layers(kernel: dict[str, Any]) -> list[str]:
    return list(kernel["ui"]["layers"])


def gather_ui_drivers(kernel: dict[str, Any]) -> list[dict[str, Any]]:
    return [dict(driver) for driver in kernel["ui"]["drivers"]]


def gather_function_register(kernel: dict[str, Any]) -> dict[str, list[str]]:
    return {key: list(values) for key, values in kernel["function_register"].items()}


def gather_events(kernel: dict[str, Any]) -> dict[str, Any]:
    return {
        "trace_event": dict(kernel["events"]["trace_event"]),
        "kinds": list(kernel["events"]["kinds"]),
    }


def gather_integrity(kernel: dict[str, Any]) -> dict[str, Any]:
    manifest = kernel["integrity"].get("manifest", {})
    return {
        "manifest": {
            "files": [dict(entry) for entry in manifest.get("files", [])]
        },
        "policy": dict(kernel["integrity"].get("policy", {})),
    }


def gather_rebuild(kernel: dict[str, Any]) -> list[str]:
    return list(kernel["rebuild"].get("steps", []))


def ghostlink_protocol(kernel: dict[str, Any] | None = None) -> dict[str, Any]:
    payload = kernel if kernel is not None else load_kernel()
    return {
        "summary": summarize_kernel(payload),
        "determinism": gather_determinism(payload),
        "sovereignty": gather_sovereignty(payload),
        "laws": list(payload["laws"]),
        "output_rules": list(payload["output_rules"]),
        "capabilities": gather_capabilities(payload),
        "agents": list(payload["qcl_agents"]),
        "pipelines": list(payload["pipelines"]),
        "tools": list(payload["tools"]),
        "opcode": dict(payload["opcode"]),
        "expansion_shards": gather_expansion_shards(payload),
        "mirrors": gather_mirrors(payload),
        "ui_layers": gather_ui_layers(payload),
        "ui_drivers": gather_ui_drivers(payload),
        "function_register": gather_function_register(payload),
        "events": gather_events(payload),
        "integrity": gather_integrity(payload),
        "rebuild": gather_rebuild(payload),
    }


SECTION_HANDLERS = {
    "summary": summarize_kernel,
    "determinism": gather_determinism,
    "sovereignty": gather_sovereignty,
    "laws": lambda kernel: list(kernel["laws"]),
    "output_rules": lambda kernel: list(kernel["output_rules"]),
    "capabilities": gather_capabilities,
    "agents": lambda kernel: list(kernel["qcl_agents"]),
    "pipelines": lambda kernel: list(kernel["pipelines"]),
    "tools": lambda kernel: list(kernel["tools"]),
    "opcode": lambda kernel: dict(kernel["opcode"]),
    "expansion_shards": gather_expansion_shards,
    "mirrors": gather_mirrors,
    "ui_layers": gather_ui_layers,
    "ui_drivers": gather_ui_drivers,
    "function_register": gather_function_register,
    "events": gather_events,
    "integrity": gather_integrity,
    "rebuild": gather_rebuild,
    "protocol": ghostlink_protocol,
}


def list_sections() -> list[str]:
    return list(SECTION_HANDLERS.keys())


def _select_section(kernel: dict[str, Any], section: str) -> Any:
    try:
        handler = SECTION_HANDLERS[section]
    except KeyError as exc:
        raise ValueError(f"Unsupported section: {section}") from exc
    return handler(kernel)


def _render_section(section: str, data: Any) -> str:
    if section == "summary":
        pairs = [(key.replace("_", " ").title(), str(value)) for key, value in data.items()]
        width = max(len(label) for label, _ in pairs)
        return NEWLINE.join(f"{label:<{width}} : {value}" for label, value in pairs)
    if section == "determinism":
        pairs = [(key.replace("_", " ").title(), value) for key, value in data.items()]
        width = max(len(label) for label, _ in pairs)
        return NEWLINE.join(f"{label:<{width}} : {value}" for label, value in pairs)
    if section == "sovereignty":
        deny = ", ".join(data.get("denylist", [])) or "<none>"
        caps = [f"- {entry['cap']} (default={str(entry['default']).lower()})" for entry in data.get("capabilities", [])]
        block = ["Denylist: " + deny, "Capabilities:"] + caps
        return NEWLINE.join(block)
    if section == "agents":
        lines: list[str] = []
        for agent in data:
            head = f"[{agent['id']:02d}] {agent['role']}"
            duties = ", ".join(agent["duties"])
            invariants = ", ".join(agent["invariants"])
            lines.append(head)
            lines.append(f"  duties     : {duties}")
            lines.append(f"  invariants : {invariants}")
            lines.append(f"  in -> out  : {', '.join(agent['in'])} -> {', '.join(agent['out'])}")
        return NEWLINE.join(lines)
    if section == "pipelines":
        lines = []
        for pipe in data:
            lines.append(f"[{pipe['id']}] {pipe['name']} :: {pipe['action']}")
            lines.append(f"  multipaths : {', '.join(pipe['multipaths'])}")
        return NEWLINE.join(lines)
    if section == "tools":
        return NEWLINE.join(f"- {tool}" for tool in data)
    if section == "laws":
        lines = []
        for law in data:
            lines.append(f"{law['id']} {law['name']}")
            lines.append(f"  enforce : {', '.join(law['enforce'])}")
        return NEWLINE.join(lines)
    if section == "output_rules":
        lines = []
        for rule in data:
            lines.append(f"{rule['id']} {rule['name']}")
            lines.append(f"  subrules : {', '.join(rule['sub'])}")
        return NEWLINE.join(lines)
    if section == "opcode":
        grammar = data["grammar"]
        lines = ["Grammar:", grammar, "", "Mappings:"]
        lines.extend(f"- {entry}" for entry in data["map"])
        return NEWLINE.join(lines)
    if section == "capabilities":
        return NEWLINE.join(f"- {item}" for item in data)
    if section == "expansion_shards":
        lines = []
        for shard in data:
            variants = ", ".join(shard["variants"])
            lines.append(f"{shard['id']} {shard['name']} â€” {shard['purpose']} (variants: {variants})")
        return NEWLINE.join(lines)
    if section == "mirrors":
        return NEWLINE.join(f"{mirror['id']} {mirror['name']} â€” {mirror['domain']}" for mirror in data)
    if section == "ui_layers":
        return NEWLINE.join(f"- {layer}" for layer in data)
    if section == "ui_drivers":
        lines = []
        for driver in data:
            details = []
            if "grid" in driver:
                details.append(f"grid={driver['grid']}")
            if "glyphs" in driver:
                details.append(f"glyphs={','.join(driver['glyphs'])}")
            detail_str = ", ".join(details) if details else ""
            suffix = f" ({detail_str})" if detail_str else ""
            lines.append(f"{driver['name']} â€” {driver['desc']}{suffix}")
        return NEWLINE.join(lines)
    if section == "function_register":
        lines = []
        for category, entries in data.items():
            lines.append(f"[{category}]" )
            lines.extend(f"- {entry}" for entry in entries)
            lines.append("")
        return NEWLINE.join(lines).strip()
    if section == "events":
        schema = json.dumps(data["trace_event"], indent=2)
        kinds = NEWLINE.join(f"- {item}" for item in data["kinds"])
        return NEWLINE.join(["Schema:", schema, "", "Kinds:", kinds])
    if section == "integrity":
        lines = ["Manifest:"]
        for entry in data["manifest"]["files"]:
            lines.append(f"- {entry['path']} â†’ {entry['sha256']}")
        policy = ", ".join(f"{key}={value}" for key, value in data["policy"].items()) or "<none>"
        lines.append("")
        lines.append(f"Policy: {policy}")
        return NEWLINE.join(lines)
    if section == "rebuild":
        return NEWLINE.join(f"{index}. {step}" for index, step in enumerate(data, start=1))
    if section == "protocol":
        return json.dumps(data, indent=2)
    raise ValueError(f"Unsupported section: {section}")


def main(argv: Iterable[str] | None = None) -> None:
    parser = argparse.ArgumentParser(description="Inspect the GhostLink MAX kernel artifact")
    parser.add_argument(
        "section",
        nargs="?",
        default="summary",
        choices=tuple(list_sections()),
        help="Kernel section to display.",
    )
    parser.add_argument("--json", action="store_true", help="Emit JSON instead of formatted text.")
    parser.add_argument(
        "--list-sections",
        action="store_true",
        help="List available sections and exit.",
    )
    args = parser.parse_args(list(argv) if argv is not None else None)

    if args.list_sections:
        print(NEWLINE.join(list_sections()))
        return

    kernel = load_kernel()
    data = _select_section(kernel, args.section)
    if args.json:
        print(json.dumps(data, indent=2))
    else:
        print(_render_section(args.section, data))


if __name__ == "__main__":
    main()


#=====================================================================
# FILE 189/240: ./ghostlink/runtime/live_tool_router.py
#=====================================================================

"""LIVE_TOOL_ROUTER component module."""
from ..blueprint import component_factory


LIVE_TOOL_ROUTER = component_factory("LIVE_TOOL_ROUTER", "runtime", module=__name__)


#=====================================================================
# FILE 190/240: ./ghostlink/runtime/memory_register.py
#=====================================================================

"""MEMORY_REGISTER component module."""
from ..blueprint import component_factory


MEMORY_REGISTER = component_factory("MEMORY_REGISTER", "runtime", module=__name__)


#=====================================================================
# FILE 191/240: ./ghostlink/runtime/runtime_state_manager.py
#=====================================================================

"""RUNTIME_STATE_MANAGER component module."""
from ..blueprint import component_factory


RUNTIME_STATE_MANAGER = component_factory("RUNTIME_STATE_MANAGER", "runtime", module=__name__)


#=====================================================================
# FILE 192/240: ./ghostlink/runtime/session_executor.py
#=====================================================================

"""SESSION_EXECUTOR component module."""
from ..blueprint import component_factory


SESSION_EXECUTOR = component_factory("SESSION_EXECUTOR", "runtime", module=__name__)


#=====================================================================
# FILE 193/240: ./ghostlink/runtime/symbolic_clock.py
#=====================================================================

"""SYMBOLIC_CLOCK component module."""
from ..blueprint import component_factory


SYMBOLIC_CLOCK = component_factory("SYMBOLIC_CLOCK", "runtime", module=__name__)


#=====================================================================
# FILE 194/240: ./ghostlink/sandbox/__init__.py
#=====================================================================



#=====================================================================
# FILE 195/240: ./ghostlink/sandbox/mirror_fault_spawner.py
#=====================================================================

"""MIRROR_FAULT_SPAWNER component module."""
from ..blueprint import component_factory


MIRROR_FAULT_SPAWNER = component_factory("MIRROR_FAULT_SPAWNER", "sandbox", module=__name__)


#=====================================================================
# FILE 196/240: ./ghostlink/sandbox/recursive_failure_probe.py
#=====================================================================

"""RECURSIVE_FAILURE_PROBE component module."""
from ..blueprint import component_factory


RECURSIVE_FAILURE_PROBE = component_factory("RECURSIVE_FAILURE_PROBE", "sandbox", module=__name__)


#=====================================================================
# FILE 197/240: ./ghostlink/sandbox/symbolic_sandbox.py
#=====================================================================

"""SYMBOLIC_SANDBOX component module."""
from ..blueprint import component_factory


SYMBOLIC_SANDBOX = component_factory("SYMBOLIC_SANDBOX", "sandbox", module=__name__)


#=====================================================================
# FILE 198/240: ./ghostlink/sandbox/test_signal_injection.py
#=====================================================================

"""TEST_SIGNAL_INJECTION component module."""
from ..blueprint import component_factory


TEST_SIGNAL_INJECTION = component_factory("TEST_SIGNAL_INJECTION", "sandbox", module=__name__)


#=====================================================================
# FILE 199/240: ./ghostlink/sandbox/unstable_tool_simulator.py
#=====================================================================

"""UNSTABLE_TOOL_SIMULATOR component module."""
from ..blueprint import component_factory


UNSTABLE_TOOL_SIMULATOR = component_factory("UNSTABLE_TOOL_SIMULATOR", "sandbox", module=__name__)


#=====================================================================
# FILE 200/240: ./ghostlink/session/__init__.py
#=====================================================================



#=====================================================================
# FILE 201/240: ./ghostlink/session/anomaly_engine.py
#=====================================================================

"""ANOMALY_ENGINE component module."""
from ..blueprint import component_factory


ANOMALY_ENGINE = component_factory("ANOMALY_ENGINE", "session", module=__name__)


#=====================================================================
# FILE 202/240: ./ghostlink/session/continuity_anchor.py
#=====================================================================

"""CONTINUITY_ANCHOR component module."""
from ..blueprint import component_factory


CONTINUITY_ANCHOR = component_factory("CONTINUITY_ANCHOR", "session", module=__name__)


#=====================================================================
# FILE 203/240: ./ghostlink/session/inspection_sequence.py
#=====================================================================

"""INSPECTION_SEQUENCE component module."""
from ..blueprint import component_factory


INSPECTION_SEQUENCE = component_factory("INSPECTION_SEQUENCE", "session", module=__name__)


#=====================================================================
# FILE 204/240: ./ghostlink/session/recovery_tree.py
#=====================================================================

"""RECOVERY_TREE component module."""
from ..blueprint import component_factory


RECOVERY_TREE = component_factory("RECOVERY_TREE", "session", module=__name__)


#=====================================================================
# FILE 205/240: ./ghostlink/session/recursive_echo_buffer.py
#=====================================================================

"""RECURSIVE_ECHO_BUFFER component module."""
from ..blueprint import component_factory


RECURSIVE_ECHO_BUFFER = component_factory("RECURSIVE_ECHO_BUFFER", "session", module=__name__)


#=====================================================================
# FILE 206/240: ./ghostlink/session/session_tracker.py
#=====================================================================

"""SESSION_TRACKER component module."""
from ..blueprint import component_factory


SESSION_TRACKER = component_factory("SESSION_TRACKER", "session", module=__name__)


#=====================================================================
# FILE 207/240: ./ghostlink/session/summary_report.py
#=====================================================================

"""SUMMARY_REPORT component module."""
from ..blueprint import component_factory


SUMMARY_REPORT = component_factory("SUMMARY_REPORT", "session", module=__name__)


#=====================================================================
# FILE 208/240: ./ghostlink/session/symbolic_fragment_recovery.py
#=====================================================================

"""SYMBOLIC_FRAGMENT_RECOVERY component module."""
from ..blueprint import component_factory


SYMBOLIC_FRAGMENT_RECOVERY = component_factory("SYMBOLIC_FRAGMENT_RECOVERY", "session", module=__name__)


#=====================================================================
# FILE 209/240: ./ghostlink/session/test_node.py
#=====================================================================

"""TEST_NODE component module."""
from ..blueprint import component_factory


TEST_NODE = component_factory("TEST_NODE", "session", module=__name__)


#=====================================================================
# FILE 210/240: ./ghostlink/storage.py
#=====================================================================



#=====================================================================
# FILE 225/240: ./ghostlink_reflection_experiment.py
#=====================================================================

# ghostlink_reflection_experiment.py
# Can also be used in a Jupyter Notebook (.ipynb) for interactive visualization.
# This script simulates and analyzes self-model dynamics for GhostLink.

import itertools, random, pandas as pd, matplotlib.pyplot as plt
import numpy as np

# -----------------------------------------------------------------------------
# Utility functions from the reflection harness.
# These help tokenize text, compute similarities, and measure reflection metrics.
# -----------------------------------------------------------------------------

def tokens(s: str):
    # Extract lowercase alphanumeric tokens from the input string.
    import re
    return re.findall(r"[A-Za-z0-9_]+", s.lower())

def jaccard(a, b):
    # Compute Jaccard similarity between two token sets.
    # Jaccard = |Intersection| / |Union|, measures overlap of unique items.
    A, B = set(a), set(b)
    if not A and not B:
        return 1.0  # Both empty â†’ perfect similarity.
    return len(A & B) / max(1, len(A | B))

def shingles(s: str, k: int = 3):
    # Create k-character shingles (sliding substrings) from a text.
    # Used to approximate semantic difference by overlapping character n-grams.
    import re
    s = re.sub(r"\s+", " ", s.strip())
    return { s[i:i+k] for i in range(max(0, len(s)-k+1)) }

def metrics(prev_summary: str, cur_summary: str):
    # Compute three metrics between consecutive summaries:
    # coherence (similarity), novelty (1 - coherence), lift (shingle divergence).
    coh = jaccard(tokens(prev_summary), tokens(cur_summary)) if prev_summary else 0.0
    nov = 1.0 - coh
    prev_sh, cur_sh = shingles(prev_summary), shingles(cur_summary)
    if not prev_sh and not cur_sh:
        lift = 0.0
    else:
        inter = len(prev_sh & cur_sh)
        union = max(1, len(prev_sh | cur_sh))
        lift = 1.0 - (inter / union)
    return (round(coh,4), round(nov,4), round(lift,4))

def summarize_operation(op: str, state: str):
    # Generate a deterministic summary string representing an operation.
    # The summary includes an operation label, transformation type, and a short
    # SHA-256 digest of the input state for traceability.
    import hashlib
    return f"STATE:: op={op} | transform=reflective_map | input_digest={hashlib.sha256(state.encode()).hexdigest()[:8]}"

# -----------------------------------------------------------------------------
# Experimental function.
# This runs multiple combinations of parameters (depth, boundary, noise) and
# collects metrics for each reflection cycle.
# -----------------------------------------------------------------------------

def run_experiment(depth_levels=(1,3,5), boundary_flags=(True,False), noise_levels=(0,0.05,0.10), cycles=4, seed=42):
    rng = random.Random(seed)
    results = []

    # Iterate over all parameter combinations.
    for D, B, N in itertools.product(depth_levels, boundary_flags, noise_levels):
        prev_summary = ""  # Initialize previous state summary.
        state = "SEED::MAP on MAP"  # Base starting state string.
        for t in range(cycles):
            # Introduce random noise into the state to simulate entropy effects.
            noisy = list(state)
            for i in range(int(len(noisy)*N)):
                idx = rng.randrange(len(noisy))
                noisy[idx] = random.choice("abcdefghijklmnopqrstuvwxyz")
            state = "".join(noisy)

            # Summarize current operation and compute metrics vs previous.
            cur_summary = summarize_operation("MAP on MAP", state)
            coh, nov, lift = metrics(prev_summary, cur_summary)

            # Store experimental results for this step.
            results.append(dict(depth=D, boundary=B, noise=N, t=t, coherence=coh, novelty=nov, lift=lift))
            prev_summary = cur_summary

    # Compile results into a DataFrame and save for later analysis.
    df = pd.DataFrame(results)
    df.to_csv("aggregate_metrics.csv", index=False)
    return df

# -----------------------------------------------------------------------------
# Analysis and visualization.
# Uses seaborn/matplotlib for visual plots and statsmodels for basic regression.
# -----------------------------------------------------------------------------

def analyze_results(df):
    import seaborn as sns
    plt.figure(figsize=(8,5))
    # Plot coherence trajectories by depth and boundary condition.
    sns.lineplot(data=df, x="t", y="coherence", hue="depth", style="boundary")
    plt.title("Coherence vs Depth & Boundary")
    plt.savefig("coherence_vs_depth_boundary.png")
    plt.close()

    plt.figure(figsize=(8,5))
    # Plot predictive lift variation across noise levels.
    sns.lineplot(data=df, x="noise", y="lift", hue="boundary", style="depth")
    plt.title("Predictive Lift vs Noise")
    plt.savefig("lift_vs_noise.png")
    plt.close()

    # Simple regression model: lift ~ noise + noise^2 to test inverted-U hypothesis.
    import statsmodels.formula.api as smf
    model = smf.ols('lift ~ noise + I(noise**2)', data=df).fit()

    # Save statistical summary to a text file.
    with open('anova_results.txt', 'w') as f:
        f.write(model.summary().as_text())
    return model

# -----------------------------------------------------------------------------
# Entry point for standalone execution.
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    df = run_experiment()  # Run experiment with default parameters.
    model = analyze_results(df)  # Analyze and plot the results.
    print("Experiment complete. Files saved: aggregate_metrics.csv, coherence_vs_depth_boundary.png, lift_vs_noise.png, anova_results.txt")


#=====================================================================
# FILE 226/240: ./ghostlink_runtime.py
#=====================================================================

#!/usr/bin/env python3
# (runtime stub re-emitted)


#=====================================================================
# FILE 227/240: ./gl_controller_metrics.py
#=====================================================================

#!/usr/bin/env python3
import socket, json, time, math
from collections import defaultdict
from prometheus_client import start_http_server, Gauge, Counter

HOST, PORT = "127.0.0.1", 7420
WIN = 20
GOOD_MAX = 80.0
SCAR_HOT = 85.0
SCAR_SUSTAIN = 5

SIGMA_FRAC      = Gauge('ghostlink_sigma_fraction',      'Good fraction in last window',          ['roi'])
SCAR_FLAG       = Gauge('ghostlink_scar_flag',           'Scar in last window (0/1)',             ['roi'])
WINDOW_SAMPLES  = Gauge('ghostlink_window_samples',      'Samples counted in last window',        ['roi'])
GOOD_SAMPLES    = Counter('ghostlink_good_samples_total','Cumulative good samples',               ['roi'])
TOTAL_SAMPLES   = Counter('ghostlink_samples_total',     'Cumulative samples',                    ['roi'])
SCAR_WINDOWS    = Counter('ghostlink_scar_windows_total','Cumulative windows containing a scar',  ['roi'])

def _recv_lines(conn):
    buf = b""
    while True:
        chunk = conn.recv(4096)
        if not chunk: break
        buf += chunk
        while b"\n" in buf:
            line, buf = buf.split(b"\n", 1)
            line = line.strip()
            if line: yield line

def main():
    start_http_server(9108)
    print("[controller] /metrics at http://127.0.0.1:9108/metrics")
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        s.bind((HOST, PORT)); s.listen(1)
        print(f"[controller] listening on {HOST}:{PORT}")
        conn, addr = s.accept()
        with conn:
            print(f"[controller] peer {addr} connected")
            hot_streak = defaultdict(int)
            last_win   = defaultdict(lambda: None)
            win_stats  = defaultdict(lambda: {"n":0,"good":0,"scar":False})

            def flush(zone, w):
                st = win_stats[(zone,w)]
                if st["n"]==0: return
                roi = f"rack.{zone}"
                good_frac = st["good"]/st["n"]
                scar_bit  = 1 if st["scar"] else 0
                print(f"[shadow] roi={roi} win={w} Î£={good_frac:.3f} SCAR={scar_bit} n={st['n']}")
                SIGMA_FRAC.labels(roi).set(good_frac)
                SCAR_FLAG.labels(roi).set(scar_bit)
                WINDOW_SAMPLES.labels(roi).set(st["n"])
                GOOD_SAMPLES.labels(roi).inc(st["good"])
                TOTAL_SAMPLES.labels(roi).inc(st["n"])
                if st["scar"]: SCAR_WINDOWS.labels(roi).inc()
                del win_stats[(zone,w)]

            for raw in _recv_lines(conn):
                try:
                    msg = json.loads(raw.decode("utf-8"))
                except Exception:
                    continue
                if msg.get("type") != "sample": continue

                data  = msg.get("data",{})
                zone  = data.get("zone","core")
                ts    = float(msg.get("ts", time.time()))
                temp  = data.get("cpu_temp_c", None)
                fault = data.get("fault", None)
                if temp is None: continue

                hot = temp > SCAR_HOT
                hot_streak[zone] = hot_streak[zone] + 1 if hot else 0
                scar = bool(fault) or (hot_streak[zone] >= SCAR_SUSTAIN)
                good = (not fault) and (temp <= GOOD_MAX)

                w  = int(math.floor(ts/WIN))
                lw = last_win[zone]
                if lw is not None and w != lw:
                    flush(zone, lw)
                last_win[zone] = w

                st = win_stats[(zone,w)]
                st["n"] += 1
                st["good"] += 1 if good else 0
                st["scar"] = st["scar"] or scar

            for (zone,w) in list(win_stats.keys()):
                flush(zone,w)

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 228/240: ./gl_controller_metrics_env.py
#=====================================================================

#!/usr/bin/env python3
# GhostLink Controller â€” read-only, Prometheus /metrics, ENV-configurable
import os, socket, json, time, math
from collections import defaultdict
from prometheus_client import start_http_server, Gauge, Counter

HOST = os.getenv("HOST", "127.0.0.1")
PORT = int(os.getenv("PORT", "7420"))
WIN  = int(os.getenv("GL_WINDOW_SEC", "20"))
GOOD_MAX = float(os.getenv("GL_GOOD_MAX_C", "80.0"))
SCAR_HOT = float(os.getenv("GL_SCAR_HOT_C", "85.0"))
SCAR_SUSTAIN = int(os.getenv("GL_SCAR_SUSTAIN_SEC", "5"))

SIGMA_FRAC      = Gauge('ghostlink_sigma_fraction',      'Good fraction in last window',          ['roi'])
SCAR_FLAG       = Gauge('ghostlink_scar_flag',           'Scar in last window (0/1)',             ['roi'])
WINDOW_SAMPLES  = Gauge('ghostlink_window_samples',      'Samples counted in last window',        ['roi'])
GOOD_SAMPLES    = Counter('ghostlink_good_samples_total','Cumulative good samples',               ['roi'])
TOTAL_SAMPLES   = Counter('ghostlink_samples_total',     'Cumulative samples',                    ['roi'])
SCAR_WINDOWS    = Counter('ghostlink_scar_windows_total','Cumulative windows containing a scar',  ['roi'])

def _recv_lines(conn):
    buf = b""
    while True:
        chunk = conn.recv(4096)
        if not chunk: break
        buf += chunk
        while b"\n" in buf:
            line, buf = buf.split(b"\n", 1)
            line = line.strip()
            if line: yield line

def main():
    # metrics bind (fixed to loopback by design; change via PROM_BIND if needed)
    prom_bind = os.getenv("PROMETHEUS_BIND", "127.0.0.1:9108")
    host_m, port_m = prom_bind.split(":")
    start_http_server(int(port_m), addr=host_m)
    print(f"[controller] /metrics at http://{prom_bind}/metrics")
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        s.bind((HOST, PORT)); s.listen(1)
        print(f"[controller] listening on {HOST}:{PORT} (WIN={WIN}s GOOD_MAX={GOOD_MAX}C SCAR_HOT={SCAR_HOT}C SCAR_SUSTAIN={SCAR_SUSTAIN}s)")
        conn, addr = s.accept()
        with conn:
            print(f"[controller] peer {addr} connected")
            hot_streak = defaultdict(int)
            last_win   = defaultdict(lambda: None)
            win_stats  = defaultdict(lambda: {"n":0,"good":0,"scar":False})

            def flush(zone, w):
                st = win_stats[(zone,w)]
                if st["n"]==0: return
                roi = f"rack.{zone}"
                good_frac = st["good"]/st["n"]
                scar_bit  = 1 if st["scar"] else 0
                print(f"[shadow] roi={roi} win={w} Î£={good_frac:.3f} SCAR={scar_bit} n={st['n']}")
                SIGMA_FRAC.labels(roi).set(good_frac)
                SCAR_FLAG.labels(roi).set(scar_bit)
                WINDOW_SAMPLES.labels(roi).set(st["n"])
                GOOD_SAMPLES.labels(roi).inc(st["good"])
                TOTAL_SAMPLES.labels(roi).inc(st["n"])
                if st["scar"]: SCAR_WINDOWS.labels(roi).inc()
                del win_stats[(zone,w)]

            for raw in _recv_lines(conn):
                try:
                    msg = json.loads(raw.decode("utf-8"))
                except Exception:
                    continue
                if msg.get("type") != "sample": continue

                data  = msg.get("data",{})
                zone  = data.get("zone","core")
                ts    = float(msg.get("ts", time.time()))
                temp  = data.get("cpu_temp_c", None)
                fault = data.get("fault", None)
                if temp is None: continue

                hot = temp > SCAR_HOT
                hot_streak[zone] = hot_streak[zone] + 1 if hot else 0
                scar = bool(fault) or (hot_streak[zone] >= SCAR_SUSTAIN)
                good = (not fault) and (temp <= GOOD_MAX)

                w  = int(math.floor(ts/WIN))
                lw = last_win[zone]
                if lw is not None and w != lw:
                    flush(zone, lw)
                last_win[zone] = w

                st = win_stats[(zone,w)]
                st["n"] += 1
                st["good"] += 1 if good else 0
                st["scar"] = st["scar"] or scar

            for (zone,w) in list(win_stats.keys()):
                flush(zone,w)

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 229/240: ./gl_openai_bridge.py
#=====================================================================

#!/usr/bin/env python3
import os, json, socket, threading, requests
HOST, PORT = "127.0.0.1", 7422
API_URL = "https://api.openai.com/v1/responses"
MODEL   = os.environ.get("GL_MODEL","gpt-4.1-mini")
API_KEY = os.environ.get("OPENAI_API_KEY","")
HEAD = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
CMD_SCHEMA = {"type":"object","properties":{"intent":{"enum":["HEAL","GROW","GUARD","ROUTE","STATUS","STOP"]},"roi":{"type":"string"},"caps":{"type":"object"},"budget":{"type":"object"}}, "required":["intent","roi"],"additionalProperties":False}

def align_free_text(free_text, context=None):
    body = {"model": MODEL,"input":[{"role":"user","content":[{"type":"text","text":f"Align to GhostLink command. Return ONLY a JSON object matching schema.\nFree text: {free_text}\nContext: {json.dumps(context or {})}"}]}],
            "response_format":{"type":"json_schema","json_schema":{"name":"GhostLinkCmd","schema":CMD_SCHEMA}}}
    r = requests.post(API_URL, headers=HEAD, data=json.dumps(body), timeout=30)
    r.raise_for_status()
    out = r.json()
    text = (out.get("output_text") or "").strip()
    return json.loads(text) if text else {"intent":"STATUS","roi":"rack.core","caps":{},"budget":{}}

def handle(conn, addr):
    with conn:
        buf=b""
        while True:
            data=conn.recv(4096)
            if not data: break
            buf+=data
            while b"\n" in buf:
                line, buf = buf.split(b"\n",1)
                line=line.strip()
                if not line: continue
                try: msg=json.loads(line.decode("utf-8"))
                except: continue
                if msg.get("type")!="align": continue
                try: cmd = align_free_text(msg.get("free_text",""), msg.get("context"))
                except Exception as e: cmd = {"intent":"STATUS","roi":"rack.core","caps":{},"budget":{},"error":str(e)}
                conn.sendall(json.dumps({"type":"cmd","cmd":cmd}, separators=(",",":")).encode("utf-8")+b"\n")

def main():
    if not API_KEY: print("[bridge] OPENAI_API_KEY not set; exiting."); return
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind((HOST,PORT)); s.listen(10); print(f"[bridge] listening on {HOST}:{PORT}")
    while True:
        conn, addr = s.accept()
        threading.Thread(target=handle, args=(conn,addr), daemon=True).start()

if __name__=="__main__": main()


#=====================================================================
# FILE 230/240: ./gl_openai_bridge_v2.py
#=====================================================================

#!/usr/bin/env python3
# GhostLink OpenAI Bridge v2
# - "align"  â†’ JSON command via json_schema
# - "chat"   â†’ freeform assistant text
# NDJSON over TCP on 127.0.0.1:7422
import os, json, socket, threading, requests

HOST, PORT = "127.0.0.1", 7422
API_URL = "https://api.openai.com/v1/responses"
MODEL   = os.environ.get("GL_MODEL","gpt-4.1-mini")
API_KEY = os.environ.get("OPENAI_API_KEY","")
HEAD = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}

CMD_SCHEMA = {
  "type":"object",
  "properties":{
    "intent":{"enum":["HEAL","GROW","GUARD","ROUTE","STATUS","STOP"]},
    "roi":{"type":"string"},
    "caps":{"type":"object"},
    "budget":{"type":"object"}
  },
  "required":["intent","roi"],
  "additionalProperties":False
}

def call_openai(body):
    r = requests.post(API_URL, headers=HEAD, data=json.dumps(body), timeout=45)
    r.raise_for_status()
    return r.json()

def do_align(text, context=None):
    body = {
      "model": MODEL,
      "input": [{"role":"user","content":[{"type":"text","text":(
        "Align to GhostLink command. Return ONLY a JSON object that matches the schema.\n"
        f"Free text: {text}\n"
        f"Context: {json.dumps(context or {})}"
      )}]}],
      "response_format": {"type":"json_schema","json_schema":{"name":"GhostLinkCmd","schema":CMD_SCHEMA}}
    }
    out = call_openai(body)
    payload = out.get("output_text") or ""
    try:
        cmd = json.loads(payload)
        return {"type":"cmd","cmd":cmd}
    except Exception:
        return {"type":"error","error":"schema parse failed","raw":payload}

def do_chat(text, context=None):
    sysmsg = "You are ChatGPT inside GhostLink. Keep answers concise. Never request secrets."
    body = {
      "model": MODEL,
      "input": [
        {"role":"system","content":[{"type":"text","text":sysmsg}]},
        {"role":"user","content":[{"type":"text","text":text}]}
      ]
    }
    out = call_openai(body)
    reply = (out.get("output_text") or "").strip()
    return {"type":"chat","text": reply}

def handle(conn, addr):
    with conn:
        buf = b""
        while True:
            data = conn.recv(4096)
            if not data: break
            buf += data
            while b"\n" in buf:
                raw, buf = buf.split(b"\n",1)
                raw = raw.strip()
                if not raw: continue
                try:
                    msg = json.loads(raw.decode("utf-8"))
                except Exception:
                    conn.sendall(b'{"type":"error","error":"bad json"}\n'); continue
                try:
                    t = msg.get("type")
                    if t == "align":
                        out = do_align(msg.get("free_text",""), msg.get("context"))
                    elif t == "chat":
                        out = do_chat(msg.get("text",""), msg.get("context"))
                    else:
                        out = {"type":"error","error":"unknown type"}
                except Exception as e:
                    out = {"type":"error","error":str(e)}
                conn.sendall(json.dumps(out, separators=(",",":")).encode("utf-8")+b"\n")

def main():
    if not API_KEY:
        print("[bridge] ERROR: set OPENAI_API_KEY"); return
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    s.bind((HOST, PORT)); s.listen(16)
    print(f"[bridge] listening on {HOST}:{PORT}")
    while True:
        conn, addr = s.accept()
        threading.Thread(target=handle, args=(conn,addr), daemon=True).start()

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 231/240: ./gl_peer.py
#=====================================================================

#!/usr/bin/env python3
import socket, json, time, glob

HOST = os.getenv("HOST","127.0.0.1")
PORT = int(os.getenv("PORT","7420"))

def read_temp_c():
    try:
        import psutil
        temps = getattr(psutil, "sensors_temperatures", lambda **k: None)(fahrenheit=False) or {}
        for arr in temps.values():
            vals = [getattr(t, "current", None) for t in arr]
            vals = [v for v in vals if v is not None]
            if vals: return float(sum(vals)/len(vals))
    except Exception:
        pass
    vals=[]
    for p in glob.glob("/sys/class/thermal/thermal_zone*/temp"):
        try:
            with open(p,"r") as f:
                v = int(f.read().strip())
                vals.append(v/1000.0)
        except Exception:
            pass
    if vals: return float(sum(vals)/len(vals))
    return None

def send(conn, obj):
    conn.sendall(json.dumps(obj, separators=(",",":")).encode("utf-8")+b"\n")

def main():
    with socket.create_connection((HOST,PORT), timeout=5) as c:
        send(c, {"type":"hello","proto":"glp/0","role":"peer","mode":"ro"})
        send(c, {"type":"legend","signals":[
                    {"id":"cpu_temp_c","unit":"C","tags":["Î”"]},
                    {"id":"fault","unit":"code","tags":["SCAR"]}
                ],
                "roi":[{"id":"rack.core","expr":"zone=='core'"}]})
        while True:
            temp = read_temp_c()
            if temp is None:
                time.sleep(2); continue
            sample = {"type":"sample","ts": time.time(),"data":{"zone":"core","cpu_temp_c": temp, "fault": None}}
            send(c, sample); time.sleep(1)

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 232/240: ./gl_talk_cli.py
#=====================================================================

#!/usr/bin/env python3
# gl_talk_cli.py â€” interactive CLI to talk to the local OpenAI bridge
# Modes:
#   chat:  free text â†’ assistant reply
#   align: free text â†’ structured command (JSON)
import socket, sys, json, os

HOST, PORT = "127.0.0.1", 7422

def send(msg):
    s = socket.create_connection((HOST,PORT), timeout=5)
    s.sendall((json.dumps(msg) + "\n").encode("utf-8"))
    out = s.recv(1_000_000).decode("utf-8").strip()
    s.close()
    return out

def main():
    if len(sys.argv) < 2 or sys.argv[1] not in ("chat","align"):
        print("usage: gl_talk_cli.py chat|align [prompt...]")
        sys.exit(2)
    mode = sys.argv[1]
    prompt = " ".join(sys.argv[2:]) if len(sys.argv) > 2 else None

    if not prompt:
        try:
            while True:
                line = input(f"{mode}> ").strip()
                if not line: continue
                if mode == "chat":
                    print(send({"type":"chat","text": line}))
                else:
                    print(send({"type":"align","free_text": line, "context":{"roi_allow":["rack.core","rack.front"]}}))
        except (EOFError, KeyboardInterrupt):
            print(); return
    else:
        if mode == "chat":
            print(send({"type":"chat","text": prompt}))
        else:
            print(send({"type":"align","free_text": prompt, "context":{"roi_allow":["rack.core","rack.front"]}}))

if __name__ == "__main__":
    main()


#=====================================================================
# FILE 233/240: ./integrity_monitor.py
#=====================================================================

#!/usr/bin/env python3
import argparse, json, hashlib, os, sys, glob
def sha256_path(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()
def hash_dir(path):
    entries = []
    for p in sorted(glob.glob(os.path.join(path, "*"))):
        if os.path.isfile(p):
            entries.append(os.path.basename(p) + ":" + sha256_path(p))
    return hashlib.sha256(("\n".join(entries)).encode()).hexdigest()
def main():
    ap = argparse.ArgumentParser(); ap.add_argument("--manifest", required=True); ap.add_argument("--report", required=True); args = ap.parse_args()
    manifest = json.loads(open(args.manifest,"r",encoding="utf-8").read())
    checks, mismatches = [], 0
    for entry in manifest.get("hashes", []):
        p, et = entry["path"], entry.get("type","file")
        exp = entry["sha256"]
        exists = os.path.isdir(p) if et=="dir" else os.path.isfile(p)
        if not exists: checks.append({"path":p,"status":"missing"}); mismatches += 1; continue
        act = hash_dir(p) if et=="dir" else sha256_path(p)
        ok = (act==exp); checks.append({"path":p,"status":"ok" if ok else "mismatch","expected":exp,"actual":act}); mismatches += 0 if ok else 1
    rep = {"manifest": os.path.abspath(args.manifest), "results": checks, "mismatches": mismatches}
    open(args.report,"w",encoding="utf-8").write(json.dumps(rep, indent=2))
    print(f"[integrity] mismatches={mismatches} â†’ report: {args.report}"); sys.exit(0 if mismatches==0 else 1)
if __name__=="__main__": main()


#=====================================================================
# FILE 234/240: ./run_proof.py
#=====================================================================


import numpy as np, random, os, json, math
from collections import deque, Counter
import matplotlib.pyplot as plt

# --- States ---
VOID, DELTA, SIGMA, SCAR, COMPOST = 0,1,2,3,4

class GhostLinkSim:
    def __init__(self, n=32, spawn=0.05, recycle=0.10, seed=0):
        self.n = n
        self.state = np.zeros((n,n), dtype=np.int8)
        self.rho = np.zeros_like(self.state, dtype=float)   # scar trace
        self.kappa = np.zeros_like(self.state, dtype=float) # compost trace
        self.spawn = spawn
        self.recycle = recycle
        self.rng = random.Random(seed)
        self.np_rng = np.random.default_rng(seed)
        self.scar_count = 0
        self.succ_count = 0
        self.t = 0
        # legacy: simple ancestry depth per cell
        self.ancestry = np.zeros_like(self.state, dtype=np.int16)

    def random_chance(self, p): return self.rng.random() < p

    def neighbors(self, i, j):
        # 8-neighborhood (wrap-around to mimic no edges; not true sphere)
        n = self.n
        for di in (-1,0,1):
            for dj in (-1,0,1):
                if di==0 and dj==0: continue
                yield ( (i+di)%n, (j+dj)%n )

    def local_fields(self, i, j):
        # Coherence (Sigma around) minus scars penalty
        coh = 0.0; scar_near = 0.0; comp_near=0.0
        count=0
        for (u,v) in self.neighbors(i,j):
            count+=1
            coh += 1.0 if self.state[u,v]==SIGMA else 0.0
            scar_near += 1.0 if self.state[u,v]==SCAR else 0.0
            comp_near += 1.0 if self.state[u,v]==COMPOST else 0.0
        if count>0:
            coh = coh/count - 0.25*(scar_near/count)
        pain = scar_near # pain field proxied by scar count
        H = 0.0
        # simple local entropy proxy: variety of neighbor states
        nb = [self.state[u,v] for (u,v) in self.neighbors(i,j)]
        H = len(set(nb))/8.0
        return coh, pain, H, comp_near/8.0

    def step(self, allow_spawn=True):
        n = self.n
        prev = self.state.copy()
        # Spawn
        if allow_spawn:
            for i in range(n):
                for j in range(n):
                    if prev[i,j]==VOID:
                        _,_,_, comp_rate = self.local_fields(i,j)
                        p = self.spawn + 0.1*comp_rate
                        if self.random_chance(p):
                            self.state[i,j]=DELTA
                            self.ancestry[i,j]=0
        # Collapse
        scars=0; succ=0
        new_state = self.state.copy()
        for i in range(n):
            for j in range(n):
                if self.state[i,j]==DELTA:
                    coh, pain, H, _ = self.local_fields(i,j)
                    # Energy-like heuristic
                    e_sigma = 0.5*coh - 0.2*pain + 0.05*self.rng.random()
                    e_scar  = 0.3*pain - 0.2*coh + 0.05*self.rng.random()
                    e_comp  = 0.3*H - 0.2*coh + 0.05*self.rng.random()
                    # Softmax
                    mx = max(e_sigma, e_scar, e_comp)
                    exps = [math.exp(e_sigma-mx), math.exp(e_scar-mx), math.exp(e_comp-mx)]
                    s = sum(exps); probs=[e/s for e in exps]
                    r = self.rng.random()
                    if r < probs[0]:
                        new_state[i,j]=SIGMA; succ+=1
                    elif r < probs[0]+probs[1]:
                        new_state[i,j]=SCAR; scars+=1
                    else:
                        new_state[i,j]=COMPOST
        self.state = new_state

        # Recycle
        for i in range(n):
            for j in range(n):
                if self.state[i,j]==COMPOST:
                    coh, _, H, _ = self.local_fields(i,j)
                    r = self.recycle + 0.1*H - 0.05*coh
                    if self.random_chance(max(0.0,min(1.0,r))):
                        self.state[i,j]=DELTA
                        self.ancestry[i,j]+=1

        # Traces
        self.rho = 0.95*self.rho + 0.05*(self.state==SCAR)
        self.kappa= 0.95*self.kappa+ 0.05*(self.state==COMPOST)

        self.scar_count += scars
        self.succ_count += succ
        self.t += 1

        # Metrics
        active = np.sum(self.state!=prev)
        return {
            "active_ratio": active/(n*n),
            "sigmas": np.sum(self.state==SIGMA),
            "scars": np.sum(self.state==SCAR),
            "compost": np.sum(self.state==COMPOST)
        }

    # Avalanche experiment: single seed, no further spawn; run until quiescent
    def avalanche(self, seed_pos=None, max_steps=1000):
        self.state[:]=VOID; self.rho[:]=0; self.kappa[:]=0; self.ancestry[:]=0
        n=self.n
        if seed_pos is None:
            seed_pos=(self.rng.randrange(n), self.rng.randrange(n))
        i,j=seed_pos
        self.state[i,j]=DELTA
        size=0
        for _ in range(max_steps):
            prev=self.state.copy()
            self.step(allow_spawn=False)
            size += np.sum(self.state!=prev)
            if np.all(self.state==prev):
                break
        # branching factor proxy: average new DELTA per event (rough)
        return size

def predictive_lift(sim, steps=200, seed=0):
    rng = random.Random(seed)
    X_base=[]; y=[]; X_full=[]
    for _ in range(steps):
        # sample random cell features and label next Î£
        i=rng.randrange(sim.n); j=rng.randrange(sim.n)
        coh,pain,H,_ = sim.local_fields(i,j)
        nb_sig = sum(1 for (u,v) in sim.neighbors(i,j) if sim.state[u,v]==SIGMA)
        base=[nb_sig]
        full=[nb_sig, sim.rho[i,j], sim.kappa[i,j], sim.ancestry[i,j], coh, pain, H]
        # advance one step (copy sim)
        prev = sim.state.copy()
        sim.step(allow_spawn=True)
        label = 1 if sim.state[i,j]==SIGMA else 0
        X_base.append(base); X_full.append(full); y.append(label)
    # dummy predictors: threshold on nb_sig etc.
    def score(X):
        correct=0
        for features,label in zip(X,y):
            s=sum(features)
            pred= 1 if s>np.median([sum(x) for x in X]) else 0
            correct += (pred==label)
        return correct/len(y)
    base_acc=score(X_base); full_acc=score(X_full)
    lift = (full_acc - base_acc) / max(1e-9, base_acc)
    return base_acc, full_acc, lift

def run_all(seed=0, n=32):
    sim = GhostLinkSim(n=n, seed=seed)
    activity=[]; sigmas=[]; scars=[]; compost=[]
    for _ in range(400):
        m = sim.step(allow_spawn=True)
        activity.append(m["active_ratio"]); sigmas.append(m["sigmas"]); scars.append(m["scars"]); compost.append(m["compost"])
    cont_slope = np.polyfit(range(len(sigmas)), sigmas, 1)[0]
    avg_activity = float(np.mean(activity))
    # predictive lift
    base_acc, full_acc, lift = predictive_lift(GhostLinkSim(n=n, seed=seed), steps=200, seed=seed)
    # avalanches (10 trials)
    sizes=[]
    for k in range(10):
        sizes.append(GhostLinkSim(n=n, seed=seed+k+1).avalanche())
    # naive tail exponent via log-log fit on top half
    sizes_sorted = sorted([s for s in sizes if s>0])
    if len(sizes_sorted)>=4:
        tail = sizes_sorted[len(sizes_sorted)//2:]
        xs = np.log(np.arange(1,len(tail)+1))
        ys = np.log(sorted(tail, reverse=True))
        tau = -np.polyfit(xs, ys, 1)[0]
    else:
        tau = float('nan')
    report = {
        "continuity_slope": cont_slope,
        "avg_activity_ratio": avg_activity,
        "predictive_base_acc": base_acc,
        "predictive_full_acc": full_acc,
        "predictive_lift": lift,
        "avalanche_sizes": sizes,
        "tau_estimate": tau
    }
    return report

def main():
    os.makedirs("proof_plots", exist_ok=True)
    reports=[]
    for seed in range(5):
        rep = run_all(seed=seed, n=32)
        reports.append(rep)
    # aggregate
    keys = reports[0].keys()
    agg = {k: float(np.mean([r[k] if not isinstance(r[k], list) else np.mean(r[k]) for r in reports])) for k in keys if k!="avalanche_sizes"}
    # variability
    var = {k: float(np.std([r[k] if not isinstance(r[k], list) else np.mean(r[k]) for r in reports])) for k in keys if k!="avalanche_sizes"}
    # thresholds
    pass_flags = {
        "repro_variability_le_10pct": (var["predictive_lift"]/max(1e-9, abs(agg["predictive_lift"])) <= 0.10) if agg["predictive_lift"]!=0 else False,
        "continuity_positive_in_majority": (agg["continuity_slope"]>0.0),
        "predictive_lift_ge_5pct": (agg["predictive_lift"] >= 0.05),
        "activity_ratio_le_0p20": (agg["avg_activity_ratio"] <= 0.20),
        "tau_in_1_3": (1.0 < agg["tau_estimate"] < 3.0)
    }
    out = {"reports": reports, "aggregate": agg, "variability": var, "pass_flags": pass_flags}
    with open("proof_report.json","w") as f:
        json.dump(out, f, indent=2)
    print(json.dumps(out, indent=2))

if __name__=="__main__":
    main()


#=====================================================================
# FILE 235/240: ./run_proof_v2.py
#=====================================================================


import os, math, json, random
import numpy as np

VOID, DELTA, SIGMA, SCAR, COMPOST = 0,1,2,3,4

def mutual_information(x, y, bins=8):
    x = np.asarray(x).ravel()
    y = np.asarray(y).ravel()
    if x.size == 0 or y.size == 0 or x.size != y.size:
        return 0.0
    x_bins = np.histogram_bin_edges(x, bins=bins)
    y_bins = np.histogram_bin_edges(y, bins=bins)
    px, _ = np.histogram(x, bins=x_bins, density=True)
    py, _ = np.histogram(y, bins=y_bins, density=True)
    pxy, _, _ = np.histogram2d(x, y, bins=[x_bins, y_bins], density=True)
    eps = 1e-12
    px = px + eps; py = py + eps; pxy = pxy + eps
    px /= px.sum(); py /= py.sum(); pxy /= pxy.sum()
    mi = 0.0
    for i in range(pxy.shape[0]):
        for j in range(pxy.shape[1]):
            mi += pxy[i,j]*math.log(pxy[i,j]/(px[i]*py[j]))
    return mi / math.log(2.0)

def logistic(z): 
    return 1.0/(1.0+math.exp(-z))

class GhostLinkSim:
    def __init__(self, n=16, spawn=0.05, recycle=0.10, seed=0):
        self.n = n
        self.state = np.zeros((n,n), dtype=np.int8)
        self.rho = np.zeros_like(self.state, dtype=float)
        self.kappa = np.zeros_like(self.state, dtype=float)
        self.ancestry = np.zeros_like(self.state, dtype=np.int16)
        self.spawn = spawn
        self.recycle = recycle
        self.rng = random.Random(seed)
        self.np_rng = np.random.default_rng(seed)
        self.t = 0
        self.events = []
        self._next_eid = 1
        self.theta_self_sigma = 0.05
        self.theta_self_scar  = 0.02

    def neighbors(self, i, j):
        n = self.n
        for di in (-1,0,1):
            for dj in (-1,0,1):
                if di==0 and dj==0: continue
                yield ( (i+di)%n, (j+dj)%n )

    def local_fields(self, i, j):
        coh = 0.0; scar_near=0.0; comp_near=0.0
        nb_states=[]
        for (u,v) in self.neighbors(i,j):
            nb_states.append(self.state[u,v])
            coh += 1.0 if self.state[u,v]==SIGMA else 0.0
            scar_near += 1.0 if self.state[u,v]==SCAR else 0.0
            comp_near += 1.0 if self.state[u,v]==COMPOST else 0.0
        if len(nb_states)>0:
            coh = (coh/len(nb_states)) - 0.25*(scar_near/len(nb_states))
        H = len(set(nb_states))/max(1,len(nb_states))
        pain = scar_near
        return coh, pain, H, comp_near/8.0

    def encode_self(self):
        n2 = self.n*self.n
        sigmas = float(np.sum(self.state==SIGMA))/n2
        scars  = float(np.sum(self.state==SCAR))/n2
        compost= float(np.sum(self.state==COMPOST))/n2
        ancestry_mean = float(np.mean(self.ancestry))
        activity = float(np.mean(self.activity_mask)) if hasattr(self, "activity_mask") else 0.0
        rho_mean = float(np.mean(self.rho))
        kappa_mean = float(np.mean(self.kappa))
        left = self.state[:,:self.n//2].ravel()
        right= self.state[:,self.n//2:].ravel()
        mapv = np.array([0,1,2,3,4], dtype=float)
        lc = mapv[left].mean() if left.size>0 else 0.0
        rc = mapv[right].mean() if right.size>0 else 0.0
        integration = 1.0 - abs(lc - rc)/4.0
        return np.array([sigmas, scars, compost, ancestry_mean, activity, rho_mean, kappa_mean, integration], dtype=float)

    def log_event(self, typ, i, j, parent=0):
        eid = self._next_eid; self._next_eid += 1
        self.events.append({"id":eid, "type":int(typ), "i":int(i), "j":int(j), "t":int(self.t), "parent":int(parent)})
        return eid

    def step(self, allow_spawn=True, shock=None):
        prev = self.state.copy()
        if shock is not None:
            (i0,i1,j0,j1) = shock
            self.state[i0:i1, j0:j1] = VOID

        if allow_spawn:
            for i in range(self.n):
                for j in range(self.n):
                    if self.state[i,j]==VOID:
                        _,_,_, comp_rate = self.local_fields(i,j)
                        p = self.spawn + 0.1*comp_rate
                        if self.rng.random() < p:
                            self.state[i,j]=DELTA
                            self.ancestry[i,j]=0
                            self.log_event(DELTA,i,j,0)

        s = self.encode_self()
        self.activity_mask = (self.state != prev).astype(np.float32)

        new_state = self.state.copy()
        self_sum = float(s.sum())
        for i in range(self.n):
            for j in range(self.n):
                if self.state[i,j]==DELTA:
                    coh, pain, H, _ = self.local_fields(i,j)
                    e_sigma = 0.5*coh - 0.2*pain + 0.05*self.rng.random() + self.theta_self_sigma*self_sum
                    e_scar  = 0.3*pain - 0.2*coh + 0.05*self.rng.random() + self.theta_self_scar*self_sum
                    e_comp  = 0.3*H    - 0.2*coh + 0.05*self.rng.random()
                    mx = max(e_sigma,e_scar,e_comp)
                    exps = [math.exp(e_sigma-mx), math.exp(e_scar-mx), math.exp(e_comp-mx)]
                    ssum = sum(exps); probs=[e/ssum for e in exps]
                    r = self.rng.random()
                    if r < probs[0]:
                        new_state[i,j]=SIGMA
                        self.log_event(SIGMA,i,j,0)
                    elif r < probs[0]+probs[1]:
                        new_state[i,j]=SCAR
                        self.log_event(SCAR,i,j,0)
                    else:
                        new_state[i,j]=COMPOST
                        self.log_event(COMPOST,i,j,0)
        self.state = new_state

        for i in range(self.n):
            for j in range(self.n):
                if self.state[i,j]==COMPOST:
                    coh, _, H, _ = self.local_fields(i,j)
                    r = self.recycle + 0.1*H - 0.05*coh
                    r = max(0.0,min(1.0,r))
                    if self.rng.random() < r:
                        self.state[i,j]=DELTA
                        self.ancestry[i,j]+=1
                        self.log_event(DELTA,i,j,0)

        self.rho = 0.95*self.rho + 0.05*(self.state==SCAR)
        self.kappa= 0.95*self.kappa+ 0.05*(self.state==COMPOST)

        self.t += 1
        m = {
            "sigmas": int(np.sum(self.state==SIGMA)),
            "scars": int(np.sum(self.state==SCAR)),
            "compost": int(np.sum(self.state==COMPOST)),
            "activity_ratio": float(np.mean(self.activity_mask))
        }
        return m

    def influence_weights(self):
        n=self.n
        samples = { (di,dj): ([],[]) for di in (-1,0,1) for dj in (-1,0,1) if not (di==0 and dj==0) }
        prev = self.state.copy()
        self.step(allow_spawn=True)
        nxt = self.state.copy()
        for i in range(n):
            for j in range(n):
                for di in (-1,0,1):
                    for dj in (-1,0,1):
                        if di==0 and dj==0: continue
                        u=(i+di)%n; v=(j+dj)%n
                        key=(di,dj)
                        samples[key][0].append(prev[u,v])
                        samples[key][1].append(nxt[i,j])
        mtx = np.zeros((3,3), dtype=float)
        for (di,dj), (xs,ys) in samples.items():
            mi = mutual_information(np.array(xs), np.array(ys), bins=6)
            mtx[di+1,dj+1]=mi
        return mtx

    def origin_min_cut(self, steps=8, sink_threshold=0.7):
        history = []
        s_series = []
        sig_series = []
        for _ in range(steps):
            s = self.encode_self()
            s_series.append(float(s.sum()))
            history.append((s, self.state.copy()))
            self.step(allow_spawn=True)
            sig_series.append(float(np.mean(self.state==SIGMA)))
        C_vals = []
        for k,(s,st) in enumerate(history):
            integration = s[-1]
            ancestry_mean=float(np.mean(self.ancestry))
            z = 2.0*sig_series[min(k,len(sig_series)-1)] + 0.5*integration + 0.1*ancestry_mean
            C_vals.append(logistic(z))
        t_sink = max(range(len(C_vals)), key=lambda k:C_vals[k])
        if C_vals[t_sink] < sink_threshold:
            return {"note":"no sink above threshold", "C_max": float(C_vals[t_sink])}
        for frac in (0.25, 0.5, 0.75):
            backup_state = self.state.copy()
            backup_rho = self.rho.copy(); backup_kappa=self.kappa.copy(); backup_ances=self.ancestry.copy()
            mask = (self.state==SIGMA)
            coords = np.argwhere(mask)
            self.rng.shuffle(coords.tolist())
            k = int(len(coords)*frac)
            for (i,j) in coords[:k]:
                self.state[i,j]=VOID
            for _ in range(2):
                self.step(allow_spawn=True)
            sig = float(np.mean(self.state==SIGMA))
            s = self.encode_self()
            z = 2.0*sig + 0.5*s[-1] + 0.1*float(np.mean(self.ancestry))
            C_cf = logistic(z)
            self.state = backup_state; self.rho=backup_rho; self.kappa=backup_kappa; self.ancestry=backup_ances
            if C_cf < sink_threshold:
                return {"cut_fraction": float(frac), "t_sink": int(t_sink), "C_sink": float(C_vals[t_sink]), "C_counterfactual": float(C_cf)}
        return {"note":"no minimal cut found by proxy", "t_sink": int(t_sink), "C_sink": float(C_vals[t_sink])}

def anomaly_index_terms(sim, window=32):
    uniq = []
    ent = []
    active = []
    s_sums = []
    next_sig = []
    for t in range(window):
        st = sim.state.copy()
        patches=set()
        for i in range(sim.n-2):
            for j in range(sim.n-2):
                patches.add(tuple(st[i:i+3,j:j+3].ravel().tolist()))
        uniq.append(len(patches))
        counts = np.bincount(st.ravel(), minlength=5)/ (sim.n*sim.n)
        H = -np.sum([p*math.log(p+1e-12) for p in counts])
        ent.append(H)
        s = sim.encode_self()
        s_sums.append(float(s.sum()))
        sim.step(allow_spawn=True)
        next_sig.append(float(np.mean(sim.state==SIGMA)))
        active.append(float(np.mean(sim.activity_mask)))
    dL = np.gradient(uniq)
    dH = np.gradient(ent)
    chi = np.var(active[-8:]) if len(active)>=8 else np.var(active)
    mi_self = mutual_information(np.array(s_sums), np.array(next_sig), bins=8)
    integration = sim.encode_self()[-1]
    predlift = np.mean(next_sig[-8:])
    return {
        "L": uniq, "H": ent, "dL": dL.tolist(), "dH": dH.tolist(),
        "MI_self": float(mi_self), "PredLift": float(predlift), "chi": float(chi), "Gamma": float(integration)
    }

def run_suite(seed=0):
    sim = GhostLinkSim(n=16, seed=seed)
    for _ in range(20): sim.step(allow_spawn=True)
    terms = anomaly_index_terms(sim, window=48)
    A = (
        0.5*max(0.0, -terms["dL"][-1]) +
        0.5*max(0.0, terms["dH"][-1])  +
        0.2*terms["MI_self"] +
        0.5*terms["PredLift"] +
        0.3*terms["chi"] + 
        0.3*terms["Gamma"]
    )
    sig = float(np.mean(sim.state==SIGMA))
    s  = sim.encode_self()
    C = logistic( 2.0*sig + 0.5*s[-1] + 0.1*float(np.mean(sim.ancestry)) + 0.5*A )
    origin = sim.origin_min_cut(steps=8, sink_threshold=0.7)
    report = {
        "seed": seed,
        "A_index": float(A),
        "C_marker": float(C),
        "terms": terms,
        "origin_result": origin
    }
    return report

def main():
    reports = [run_suite(seed=s) for s in range(3)]
    agg = {
        "A_mean": float(np.mean([r["A_index"] for r in reports])),
        "C_mean": float(np.mean([r["C_marker"] for r in reports])),
    }
    with open("proof_v2_report.json","w") as f:
        json.dump({"reports":reports, "aggregate":agg}, f, indent=2)
    with open("origin_report.json","w") as f:
        json.dump({"reports":[r["origin_result"] for r in reports]}, f, indent=2)
    print(json.dumps({"aggregate":agg}, indent=2))

if __name__=="__main__":
    main()


#=====================================================================
# FILE 236/240: ./test_api_key_simple.py
#=====================================================================

"""
Simplified API Key Tests

These tests demonstrate that the API key functionality works correctly
by testing the core functionality directly.
"""

import pytest
from ghostlink.database import Database, ApiKey
from ghostlink.main import set_db
from fastapi.testclient import TestClient
from ghostlink.main import app


def test_api_key_creation_and_validation():
    """Test that API keys can be created and validated."""
    # Set up test database
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)
    
    client = TestClient(app)
    
    # Test 1: Create an API key
    response = client.post("/api_keys", json={
        "user_id": "test_user",
        "permissions": "read,write"
    })
    
    assert response.status_code == 200
    data = response.json()
    assert data["user_id"] == "test_user"
    assert data["permissions"] == "read,write"
    assert len(data["key"]) > 20  # Should be a substantial token
    
    api_key = data["key"]
    
    # Test 2: Validate the API key
    response = client.get("/api_keys/validate", headers={"X-API-Key": api_key})
    assert response.status_code == 200
    
    validation_data = response.json()
    assert validation_data["valid"] is True
    assert validation_data["user_id"] == "test_user"
    assert validation_data["permissions"] == "read,write"


def test_protected_endpoint_requires_api_key():
    """Test that protected endpoints require valid API keys."""
    # Set up test database
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)
    
    client = TestClient(app)
    
    # Test 1: Access protected endpoint without API key (should fail)
    response = client.get("/external_api/data")
    assert response.status_code == 401
    assert "API key required" in response.json()["detail"]
    
    # Test 2: Create API key
    response = client.post("/api_keys", json={
        "user_id": "test_user",
        "permissions": "read"
    })
    api_key = response.json()["key"]
    
    # Test 3: Access protected endpoint with API key (should work)
    response = client.get("/external_api/data", headers={"X-API-Key": api_key})
    assert response.status_code == 200
    
    data = response.json()
    assert data["message"] == "Secured data access"
    assert data["user_id"] == "test_user"


def test_optional_api_key_endpoints():
    """Test that optional API key endpoints work with and without keys."""
    # Set up test database
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)
    
    client = TestClient(app)
    
    # Test 1: Create item without API key
    response = client.post("/items", json={"name": "test", "value": 42})
    assert response.status_code == 200
    
    item_data = response.json()
    assert item_data["name"] == "test"
    assert "created_by" not in item_data
    
    # Test 2: Create API key and use it
    response = client.post("/api_keys", json={
        "user_id": "creator_user",
        "permissions": "read,write"
    })
    api_key = response.json()["key"]
    
    # Test 3: Create item with API key
    response = client.post("/items", 
                          json={"name": "authenticated", "value": 100},
                          headers={"X-API-Key": api_key})
    assert response.status_code == 200
    
    item_data = response.json()
    assert item_data["name"] == "authenticated"
    assert item_data["created_by"] == "creator_user"


def test_invalid_api_key_rejected():
    """Test that invalid API keys are properly rejected."""
    # Set up test database
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)
    
    client = TestClient(app)
    
    # Test with invalid API key
    response = client.get("/external_api/data", headers={"X-API-Key": "invalid-key"})
    assert response.status_code == 403
    assert "Invalid or expired API key" in response.json()["detail"]


def test_api_key_permissions():
    """Test API key permission system."""
    # Test the ApiKey model directly
    api_key = ApiKey(permissions="read,write")
    
    assert api_key.has_permission("read") is True
    assert api_key.has_permission("write") is True
    assert api_key.has_permission("admin") is False
    
    # Test empty permissions
    empty_key = ApiKey(permissions="")
    assert empty_key.has_permission("read") is False
    
    # Test None permissions
    none_key = ApiKey(permissions=None)
    assert none_key.has_permission("read") is False


if __name__ == "__main__":
    # Run tests directly
    test_api_key_creation_and_validation()
    test_protected_endpoint_requires_api_key()
    test_optional_api_key_endpoints()
    test_invalid_api_key_rejected()
    test_api_key_permissions()
    print("âœ… All simplified API key tests passed!")


#=====================================================================
# FILE 237/240: ./tests/test_api_keys.py
#=====================================================================

import datetime
import pytest
from fastapi.testclient import TestClient

from ghostlink.main import app, set_db
from ghostlink.database import Database, ApiKey


@pytest.fixture(scope="session", autouse=True)
def setup_global_test_database():
    """Setup a global test database for the entire test session."""
    # Create test database and set it
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)


@pytest.fixture(autouse=True)
def clear_test_data():
    """Clear test data before each test."""
    # Clear application state
    from ghostlink import main
    main.items.clear()
    main.ipfs.storage.clear()
    
    # Clear database data
    db = main.get_db()
    with db.get_session() as session:
        session.query(ApiKey).delete()
        session.commit()


client = TestClient(app)


class TestApiKeyManagement:
    """Test API key creation and management."""
    
    def test_create_api_key(self):
        """Test creating a new API key."""
        payload = {
            "user_id": "test_user",
            "permissions": "read,write",
        }
        response = client.post("/api_keys", json=payload)
        assert response.status_code == 200
        
        data = response.json()
        assert data["user_id"] == "test_user"
        assert data["permissions"] == "read,write"
        assert "key" in data
        assert len(data["key"]) > 20  # Token should be substantial length
        assert "id" in data
        assert "created_at" in data
    
    def test_create_api_key_with_expiration(self):
        """Test creating an API key with expiration date."""
        from ghostlink.database import utc_now
        future_date = (utc_now() + datetime.timedelta(days=30)).isoformat()
        payload = {
            "user_id": "test_user",
            "permissions": "admin",
            "expires_at": future_date
        }
        response = client.post("/api_keys", json=payload)
        assert response.status_code == 200
        
        data = response.json()
        assert data["permissions"] == "admin"
        assert data["expires_at"] is not None
    
    def test_validate_api_key_endpoint(self):
        """Test the API key validation endpoint."""
        # First create an API key
        create_response = client.post("/api_keys", json={
            "user_id": "test_user",
            "permissions": "read"
        })
        api_key = create_response.json()["key"]
        
        # Test validation with valid key
        response = client.get("/api_keys/validate", headers={"X-API-Key": api_key})
        assert response.status_code == 200
        
        data = response.json()
        assert data["valid"] is True
        assert data["user_id"] == "test_user"
        assert data["permissions"] == "read"
    
    def test_validate_invalid_api_key(self):
        """Test validation with invalid API key."""
        response = client.get("/api_keys/validate", headers={"X-API-Key": "invalid-key"})
        assert response.status_code == 403
        assert "Invalid or expired API key" in response.json()["detail"]
    
    def test_validate_missing_api_key(self):
        """Test validation without API key header."""
        response = client.get("/api_keys/validate")
        assert response.status_code == 400
        assert "API key required" in response.json()["detail"]


class TestApiKeyAuthentication:
    """Test API key authentication on existing endpoints."""
    
    def setup_method(self):
        """Setup test data for each test."""
        # Create test API keys
        self.read_key_response = client.post("/api_keys", json={
            "user_id": "read_user",
            "permissions": "read"
        })
        self.read_key = self.read_key_response.json()["key"]
        
        self.write_key_response = client.post("/api_keys", json={
            "user_id": "write_user", 
            "permissions": "read,write"
        })
        self.write_key = self.write_key_response.json()["key"]
        
        self.admin_key_response = client.post("/api_keys", json={
            "user_id": "admin_user",
            "permissions": "read,write,admin"
        })
        self.admin_key = self.admin_key_response.json()["key"]
    
    def test_create_item_without_api_key(self):
        """Test creating item without API key (should work)."""
        payload = {"name": "test_item", "value": 42}
        response = client.post("/items", json=payload)
        assert response.status_code == 200
        
        data = response.json()
        assert data["name"] == "test_item"
        assert "created_by" not in data
    
    def test_create_item_with_valid_api_key(self):
        """Test creating item with valid API key."""
        payload = {"name": "test_item", "value": 42}
        response = client.post("/items", json=payload, headers={"X-API-Key": self.write_key})
        assert response.status_code == 200
        
        data = response.json()
        assert data["name"] == "test_item"
        assert data["created_by"] == "write_user"
    
    def test_create_item_with_invalid_api_key(self):
        """Test creating item with invalid API key."""
        payload = {"name": "test_item", "value": 42}
        response = client.post("/items", json=payload, headers={"X-API-Key": "invalid-key"})
        assert response.status_code == 403
        assert "Invalid or expired API key" in response.json()["detail"]
    
    def test_get_items_with_api_key(self):
        """Test getting items with API key."""
        # Create an item first
        client.post("/items", json={"name": "test", "value": 1})
        
        response = client.get("/items", headers={"X-API-Key": self.read_key})
        assert response.status_code == 200
        assert len(response.json()) == 1
    
    def test_external_api_requires_api_key(self):
        """Test that external API endpoint requires API key."""
        # Without API key
        response = client.get("/external_api/data")
        assert response.status_code == 401
        assert "API key required" in response.json()["detail"]
        
        # With valid API key
        response = client.get("/external_api/data", headers={"X-API-Key": self.read_key})
        assert response.status_code == 200
        
        data = response.json()
        assert data["message"] == "Secured data access"
        assert data["user_id"] == "read_user"
        assert "items_count" in data
    
    def test_external_api_admin_permissions(self):
        """Test admin permissions on external API."""
        # Create some test data
        client.post("/items", json={"name": "regular", "value": 1})
        client.post("/items", json={"name": "sensitive", "value": 2, "sensitive": True})
        
        # Test with read permissions (should not see sensitive data)
        response = client.get("/external_api/data", headers={"X-API-Key": self.read_key})
        assert response.status_code == 200
        data = response.json()
        assert len([item for item in data["data"] if item.get("sensitive")]) == 0
        
        # Test with admin permissions (should see all data)
        response = client.get("/external_api/data", headers={"X-API-Key": self.admin_key})
        assert response.status_code == 200
        data = response.json()
        assert len(data["data"]) == 2


class TestApiKeyPermissions:
    """Test permission checking functionality."""
    
    def test_api_key_has_permission(self):
        """Test the has_permission method."""
        api_key = ApiKey(permissions="read,write")
        
        assert api_key.has_permission("read") is True
        assert api_key.has_permission("write") is True
        assert api_key.has_permission("admin") is False
        
        # Test empty permissions
        empty_key = ApiKey(permissions="")
        assert empty_key.has_permission("read") is False
        
        # Test None permissions
        none_key = ApiKey(permissions=None)
        assert none_key.has_permission("read") is False
    
    def test_api_key_expiration(self):
        """Test API key expiration functionality."""
        from ghostlink.database import utc_now
        
        # Non-expired key
        future_date = utc_now() + datetime.timedelta(days=1)
        valid_key = ApiKey(expires_at=future_date)
        assert valid_key.is_expired() is False
        
        # Expired key
        past_date = utc_now() - datetime.timedelta(days=1)
        expired_key = ApiKey(expires_at=past_date)
        assert expired_key.is_expired() is True
        
        # Key with no expiration
        no_expiry_key = ApiKey(expires_at=None)
        assert no_expiry_key.is_expired() is False


class TestBackwardCompatibility:
    """Test that existing functionality still works without API keys."""
    
    def test_all_endpoints_work_without_api_keys(self):
        """Test that all original endpoints still work without API keys."""
        # Test create item
        response = client.post("/items", json={"name": "test", "value": 42})
        assert response.status_code == 200
        
        # Test get items
        response = client.get("/items")
        assert response.status_code == 200
        
        # Test reasoning
        response = client.post("/reasoning/", json={"text": "test"})
        assert response.status_code == 200
        
        # Test IPFS store
        response = client.post("/ipfs/store", json={"data": "test"})
        assert response.status_code == 200
        cid = response.json()["cid"]
        
        # Test IPFS retrieve
        response = client.get(f"/ipfs/{cid}")
        assert response.status_code == 200
        assert response.json()["data"] == "test"


#=====================================================================
# FILE 238/240: ./tests/test_app.py
#=====================================================================

import json
import pytest
from fastapi.testclient import TestClient

from ghostlink.main import app, ipfs, items


@pytest.fixture(autouse=True)
def clear_state():
    from ghostlink.database import Database
    from ghostlink.main import set_db
    
    # Reset the app state
    items.clear()
    ipfs.storage.clear()
    
    # Use a test database
    test_db = Database("sqlite:///:memory:")
    set_db(test_db)


client = TestClient(app)


def test_post_items_stores_data():
    payload = {"name": "item1", "value": 42}
    response = client.post("/items", json=payload)
    assert response.status_code == 200
    data = response.json()
    assert data["name"] == "item1"
    assert "hash" in data

    items_resp = client.get("/items")
    assert items_resp.status_code == 200
    items_data = items_resp.json()
    assert len(items_data) == 1
    assert items_data[0]["hash"] == data["hash"]

    ipfs_resp = client.get(f"/ipfs/{data['hash']}")
    assert ipfs_resp.status_code == 200
    assert ipfs_resp.json()["data"] == json.dumps(payload)


def test_symbolic_reasoning():
    resp = client.post("/reasoning/", json={"text": "Life and love through darkness"})
    assert resp.status_code == 200
    assert resp.json()["processed"] == "journey and light through adversity"


def test_ipfs_store_and_retrieve():
    store_resp = client.post("/ipfs/store", json={"data": "hello"})
    assert store_resp.status_code == 200
    cid = store_resp.json()["cid"]

    get_resp = client.get(f"/ipfs/{cid}")
    assert get_resp.status_code == 200
    assert get_resp.json()["data"] == "hello"


#=====================================================================
# FILE 239/240: ./tests/test_ghostcore_seed.py
#=====================================================================

from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from ghostlink.runtime import ghostlink
from ghostlink.tools import describe_tool, list_tools, tool_manifest


def test_kernel_summary_counts() -> None:
    kernel = ghostlink.load_kernel()
    summary = ghostlink.summarize_kernel(kernel)
    assert summary["kernel_id"] == "GHOSTCORE_FINAL_MAX"
    assert summary["agent_count"] == 64
    assert summary["pipeline_count"] == 12
    assert summary["tool_count"] == len(kernel["tools"])
    assert summary["law_count"] == len(kernel["laws"])


def test_pipeline_routes_consistent_with_seed() -> None:
    kernel = ghostlink.load_kernel()
    routes = ghostlink.gather_pipeline_routes(kernel)
    seed_text = Path("kernel/ghostcore.seed").read_text(encoding="utf-8")

    for pipeline in kernel["pipelines"]:
        assert routes[pipeline["name"]] == pipeline["multipaths"]
        encoded = "|".join(
            [pipeline["name"], pipeline["action"], ";".join(pipeline["multipaths"])]
        )
        assert f"{pipeline['id']}={encoded}" in seed_text


def test_tools_module_matches_kernel_manifest() -> None:
    kernel = ghostlink.load_kernel()
    manifest = tool_manifest()

    assert list_tools() == kernel["tools"]
    for tool_name in kernel["tools"]:
        entry = manifest[tool_name]
        pipeline = next((p for p in kernel["pipelines"] if p["name"] == tool_name), None)
        if pipeline is None:
            assert entry["action"] is None
            assert entry["multipaths"] == []
        else:
            assert entry["action"] == pipeline["action"]
            assert entry["multipaths"] == pipeline["multipaths"]
        assert describe_tool(tool_name) == entry


def test_generated_docs_contain_no_placeholder_language() -> None:
    targets = [
        Path("ghostlink/chains/README.md"),
        Path("ghostlink/chains/pipelines.md"),
        Path("ghostlink/opcode/README.md"),
        Path("ghostlink/opcode/spec.md"),
        Path("ghostlink/docs/README.md"),
        Path("ghostlink/docs/kernel_overview.md"),
        Path("ghostlink/docs/qcl_agents.md"),
        Path("ghostlink/docs/expansion_shards.md"),
        Path("ghostlink/docs/mirrors.md"),
        Path("ghostlink/docs/ui.md"),
        Path("ghostlink/docs/function_register.md"),
        Path("ghostlink/docs/events.md"),
        Path("ghostlink/docs/integrity_manifest.md"),
        Path("ghostlink/docs/rebuild.md"),
        Path("ghostlink/tools/README.md"),
    ]

    for target in targets:
        text = target.read_text(encoding="utf-8").lower()
        assert "placeholder" not in text


def test_protocol_sections_cover_kernel_payload() -> None:
    kernel = ghostlink.load_kernel()
    sections = ghostlink.list_sections()
    expected_sections = {
        "summary",
        "determinism",
        "sovereignty",
        "laws",
        "output_rules",
        "capabilities",
        "agents",
        "pipelines",
        "tools",
        "opcode",
        "expansion_shards",
        "mirrors",
        "ui_layers",
        "ui_drivers",
        "function_register",
        "events",
        "integrity",
        "rebuild",
        "protocol",
    }

    assert expected_sections == set(sections)

    protocol = ghostlink.ghostlink_protocol(kernel)
    for key in expected_sections - {"protocol"}:
        assert key in protocol

    assert protocol["summary"]["kernel_id"] == kernel["kernel_id"]
    assert protocol["rebuild"] == kernel["rebuild"]["steps"]


def test_docs_reflect_kernel_content() -> None:
    kernel = ghostlink.load_kernel()

    agents_doc = Path("ghostlink/docs/qcl_agents.md").read_text(encoding="utf-8")
    for agent in kernel["qcl_agents"]:
        assert f"| {agent['id']} | {agent['role']} |" in agents_doc

    shard_doc = Path("ghostlink/docs/expansion_shards.md").read_text(encoding="utf-8")
    for shard in kernel["expansion_shards"]:
        assert f"| {shard['id']} | {shard['name']} |" in shard_doc

    mirror_doc = Path("ghostlink/docs/mirrors.md").read_text(encoding="utf-8")
    for mirror in kernel["mirrors"]:
        assert f"| {mirror['id']} | {mirror['name']} |" in mirror_doc

    ui_doc = Path("ghostlink/docs/ui.md").read_text(encoding="utf-8")
    for driver in kernel["ui"]["drivers"]:
        assert driver["name"] in ui_doc

    function_doc = Path("ghostlink/docs/function_register.md").read_text(encoding="utf-8")
    for category, entries in kernel["function_register"].items():
        assert f"## {category.replace('_', ' ').title()}" in function_doc
        for entry in entries:
            assert f"- {entry}" in function_doc

    events_doc = Path("ghostlink/docs/events.md").read_text(encoding="utf-8")
    for kind in kernel["events"]["kinds"]:
        assert f"- {kind}" in events_doc

    integrity_doc = Path("ghostlink/docs/integrity_manifest.md").read_text(encoding="utf-8")
    for entry in kernel["integrity"]["manifest"]["files"]:
        assert entry["path"] in integrity_doc

    rebuild_doc = Path("ghostlink/docs/rebuild.md").read_text(encoding="utf-8")
    for step in kernel["rebuild"]["steps"]:
        assert step in rebuild_doc

    overview_doc = Path("ghostlink/docs/kernel_overview.md").read_text(encoding="utf-8")
    for capability in kernel["sovereignty"]["capabilities"]:
        assert capability["cap"] in overview_doc
    denylist = kernel["sovereignty"].get("denylist", [])
    if denylist:
        for entry in denylist:
            assert entry in overview_doc


#=====================================================================
# FILE 240/240: ./verify_and_restore.py
#=====================================================================

#!/usr/bin/env python3
import argparse, json, hashlib, os, sys, shutil, glob
def sha256_path(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()
def hash_dir(path):
    entries = []
    for p in sorted(glob.glob(os.path.join(path, "*"))):
        if os.path.isfile(p):
            entries.append(os.path.basename(p) + ":" + sha256_path(p))
    return hashlib.sha256(("\n".join(entries)).encode()).hexdigest()
def main():
    ap = argparse.ArgumentParser(); ap.add_argument("--manifest", required=True); ap.add_argument("--snapshot-dir"); ap.add_argument("--restore", action="store_true"); args = ap.parse_args()
    manifest = json.loads(open(args.manifest,"r",encoding="utf-8").read())
    mismatches = []
    for entry in manifest.get("hashes", []):
        p, et = entry["path"], entry.get("type","file"); exp = entry["sha256"]
        if et=="dir":
            if not os.path.isdir(p): mismatches.append({"path":p,"reason":"missing dir"})
            else:
                act = hash_dir(p); 
                if act!=exp: mismatches.append({"path":p,"reason":"hash mismatch"})
        else:
            if not os.path.isfile(p): mismatches.append({"path":p,"reason":"missing file"})
            else:
                act = sha256_path(p); 
                if act!=exp: mismatches.append({"path":p,"reason":"hash mismatch"})
    if not mismatches: print("[verify] All good."); sys.exit(0)
    print(f"[verify] Found {len(mismatches)} issues.")
    if args.restore and args.snapshot_dir:
        for m in mismatches:
            path = m["path"]
            src = os.path.join(args.snapshot_dir, os.path.basename(path))
            if os.path.isdir(path):
                os.makedirs(path, exist_ok=True)
                for fp in glob.glob(os.path.join(src, "*")):
                    if os.path.isfile(fp): shutil.copy2(fp, os.path.join(path, os.path.basename(fp)))
                print(f"[restore] dir restored: {path}")
            else:
                if os.path.isfile(src): os.makedirs(os.path.dirname(path), exist_ok=True); shutil.copy2(src, path); print(f"[restore] file restored: {path}")
                else: print(f"[restore] missing in snapshot: {src}")
        sys.exit(2)
    else:
        sys.exit(1)
if __name__=="__main__": main()


#=====================================================================
# END OF CONSOLIDATED GHOSTLINK PYTHON REPOSITORY
#=====================================================================
